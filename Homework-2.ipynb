{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CIS 545"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "d025e16d26692caef428df4e261636a8",
     "grade": false,
     "grade_id": "cell-5c070717a34bed66",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "Before you turn this problem in, make sure everything runs as expected. First, **restart the kernel** (in the menubar, select Kernel$\\rightarrow$Restart) and then **run all cells** (in the menubar, select Cell$\\rightarrow$Run All).\n",
    "\n",
    "---- **ENTER YOUR NAME AND PENN KEY BELOW**----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "Name = 'Ruo Jia'\n",
    "PennKey = 'rj16'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "e62f5d24b382e659425a18cfade66a44",
     "grade": false,
     "grade_id": "cell-9ccad7757ff52383",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "124de9440319ecd286129de0a57a2b70",
     "grade": false,
     "grade_id": "cell-3fb8b47a9039f3da",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "# Homework 2 Basic: Big Data and Graph Data\n",
    "## Due February 25, 2018 by 10pm\n",
    "### Worth 100 points in total\n",
    "\n",
    "For this assignment, we will focus on graph data.  You saw an instance of this with Homework 1 -- the airline flight network is actually a graph -- but we only did limited kinds of computation over the graph.  However, many real-world datasets are, or can be modeled by, graphs (or trees which are special cases of graphs).  Examples include:\n",
    "\n",
    "* Networks (social networks, the Web, the connectome, the Internet, traffic networks, …)\n",
    "* Sets of data in which some of the data is more closely connected than other parts of the data (edges may represent weighted similarity or affinity)\n",
    "* Phylogenetic trees, grammars, etc.\n",
    "\n",
    "For this assignment, we will be doing a few common operations on graphs.  In the next assignment, when we have the power of matrices, we will do some further computation over the same graph data.  (It’s very common to encode graph connectivity through an adjacency matrix that we’ll discuss in lecture.)\n",
    "\n",
    "\n",
    "*Submission*. See the external document for submission information.  Remember to also do **Homework 2-Advanced**.\n",
    "\n",
    "## 1. Getting Started with Apache Spark (within Docker)\n",
    "\n",
    "Apache Spark, which has become the de facto successor to Apache Hadoop, is a complex, cluster-based data processing system that was written in Scala.  It leverages a wide variety of distributed tools and components used for big data processing.  It interfaces “smoothly” to Python, but be forewarned that there are some rough edges.  For those interested in why, there are a few reasons:\n",
    "\n",
    "* Scala has slightly different notions of types (especially things like Rows) and handles missing values (nulls) differently from Python.\n",
    "* The Scala-based Spark “engine” can’t just run Python functions as it’s doing data processing.  This means that you want to be careful to use Spark’s library of functions, or the special mechanisms for inserting “user defined functions.”\n",
    "* DataFrames on Spark are “sharded,” so there is no single object corresponding to the DataFrame!\n",
    "\n",
    "While Spark DataFrames try to emulate the same programming style as Pandas DataFrames, there are some differences in how you express things.  Please refer to the Lecture Slides for our take on the differences.  You may also find the following Web pages to be useful resources for understanding Spark vs Pandas DataFrames:\n",
    "\n",
    "https://lab.getbase.com/pandarize-spark-dataframes/\n",
    "\n",
    "https://ogirardot.wordpress.com/2015/07/31/from-pandas-to-apache-sparks-dataframe/ \n",
    "\n",
    "For this assignment, we are going to get familiar with Spark without worrying too much about sharding and distribution.  We are going to run Spark on your Docker container.  This isn’t really using it to its strengths -- and in fact you might find Spark to be unexpectedly slow -- but it will get you comfortable with programming in Spark without worrying about distributed nodes, clusters, and spending real dollars on the cloud.  Your code, if written properly, will “naturally scale” to clusters running on the Cloud. In the next assigment we’ll connect your Jupyter instance to Spark running on the cloud -- to handle “truly big data.”\n",
    "\n",
    "Useful tutorial:  http://spark.apache.org/docs/latest/sql-programming-guide.html\n",
    "\n",
    "### Step 1.1 Initializing a Connection to Spark\n",
    "\n",
    "We'll open a connection to Spark as follows. Note that Spark has multiple interfaces, as you will see if you look at sample code elsewhere. `SparkSession` is the “most modern” one and we’ll be using it for this course.  From `SparkSession`, you can load data into Spark DataFrames as well as `RDD`s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import time\n",
    "import networkx as nx\n",
    "import warnings\n",
    "\n",
    "# If you want to run from an environment outside of the Docker container you'll need to uncomment \n",
    "# and run this.  Otherwise you can skip through.\n",
    "#\n",
    "# ! pip install pyspark --user\n",
    "# ! pip install seaborn --user\n",
    "# ! pip install plotly --user\n",
    "# ! pip install imageio --user\n",
    "# ! pip install folium --user\n",
    "# ! pip install heapq\n",
    "# \n",
    "# # Misc\n",
    "# import gc\n",
    "# \n",
    "# # Visualization\n",
    "# import matplotlib.pyplot as plt\n",
    "# import seaborn as sns \n",
    "# import matplotlib.gridspec as gridspec \n",
    "# import matplotlib.gridspec as gridspec \n",
    "# \n",
    "# # Graph Visualization\n",
    "# import plotly.offline as pyo\n",
    "# from plotly.graph_objs import *\n",
    "# import plotly.graph_objs as go\n",
    "# \n",
    "# # Map Section\n",
    "# import imageio\n",
    "# import folium\n",
    "# import folium.plugins as plugins\n",
    "# from mpl_toolkits.basemap import Basemap\n",
    "# \n",
    "# # Graph Section\n",
    "# import heapq  # for getting top n number of things from list,dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "import os\n",
    "\n",
    "# Tells pyspark tells workers to use python3 not 2 if both are installed, you can leave\n",
    "# this commented out if you are running this notebook on the Docker image we provided\n",
    "#os.environ['PYSPARK_PYTHON'] = '/usr/bin/python3'\n",
    "#os.environ['PYSPARK_DRIVER_PYTHON'] = '/usr/bin/ipython3'\n",
    "\n",
    "spark = SparkSession.builder.appName('Graphs-HW2').getOrCreate()\n",
    "#spark.config(\"spark.driver.memory\", \"4g\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('spark.master', 'local[*]'),\n",
       " ('spark.submit.deployMode', 'client'),\n",
       " ('spark.executor.memory', '5g'),\n",
       " ('spark.app.name', 'Graphs-HW2'),\n",
       " ('spark.driver.memory', '5g'),\n",
       " ('spark.ui.showConsoleProgress', 'true')]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark import SparkConf\n",
    "from pyspark import SparkContext\n",
    "SparkContext.setSystemProperty('spark.executor.memory', '5g')\n",
    "SparkContext.setSystemProperty('spark.driver.memory', '5g')\n",
    "#spark = (SparkSession.config(\"spark.driver.memory\", \"4g\"))\n",
    "SparkConf().getAll()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "8543cbfeeb057543ab66d68245adc0ef",
     "grade": false,
     "grade_id": "cell-e1af7c5816c066d7",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### Step 1.2 Download data\n",
    "\n",
    "The following code retrieves the Yelp dataset in a zipfile and decompresses it.  It will take quite a while (20mins or so, don't let your computer go to sleep) - you may want to take a break while it runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Based on https://stackoverflow.com/questions/9419162/download-returned-zip-file-from-url\n",
    "\n",
    "import zipfile\n",
    "import urllib.request\n",
    "import shutil\n",
    "import os\n",
    "\n",
    "def get_and_unzip(url,file_name):\n",
    "    urllib.request.urlretrieve(url, file_name)\n",
    "    zip_ref = zipfile.ZipFile(file_name,'r')\n",
    "    zip_ref.extractall()\n",
    "    zip_ref.close()\n",
    "\n",
    "if os.path.exists(\"data\"): \n",
    "    os.system('rm -rf data/')\n",
    "os.mkdir(\"data\")\n",
    "os.chdir(\"data\")\n",
    "get_and_unzip(\"http://www.cis.upenn.edu/~cis545/yelp-dataset.zip\",\"yelp-dataset.zip\")\n",
    "os.chdir(\"..\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "1b3cb52b5e462fae8e02352c20c93c43",
     "grade": false,
     "grade_id": "cell-5fa22a39d9d3bdb8",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### Step 1.3 Load Our Graph Datasets.\n",
    "\n",
    "For this assignment, we’ll be looking at graph data (reviews, reviewers, businesses) downloaded from Yelp.\n",
    "\n",
    "**Review of graph theory**\n",
    "\n",
    "Recall that a graph $G$ is composed of a set of vertices $V$ (also called nodes) and edges $E$ (sometimes called links).  Each vertex $v \\in V$ has an identity (often represented in the real world as a string or numeric “node ID”).  Each edge $e \\in E$ is a tuple $(v_i,...,v_j)$ where $v_i$ represents the source or origin of the edge, and $v_j$ represents the target or destination.  In the simplest case, the edge tuple above is simply the pair $(v_i,v_j)$ but in many cases we may have additional fields such as a label or a distance.  Recall also that graphs may be undirected or directed; in undirected graphs, all edges are symmetric whereas in directed graphs, they are not.  For instance, airline flights are directed, whereas Facebook friend relationships are undirected. \n",
    "\n",
    "**Load Dataset Into Spark**\n",
    "\n",
    "Now, let's read our social graph datasets from Yelp you just downloaded into Spark to form a directed graph. There should be a _/data_ folder in your working directory if the above cell completed succesfully. As a hint, if you were to load a file called `input.txt` into a Spark DataFrame, you can use lines like the following:\n",
    "\n",
    "```\n",
    "# Read lines from the text file\n",
    "input_sdf = spark.read.load('input.txt', format=\"text\")\n",
    "```\n",
    "_Note1_: The above example is loading a .txt file, the files you downloaded are .csv, thus require a slightly different syntax and additional options parameters, so make sure to read the appropriate documentation.\n",
    "\n",
    "We’ll use the suffix `_sdf` to represent “Spark DataFrame,” just like we used `_df` to denote a Pandas DataFrame in Homework 1.  \n",
    "\n",
    "Now, load the various files you downloaded into a Spark Dataframe. Your datasets should be named:\n",
    "`yelp_business_sdf`, `yelp_business_attributes_sdf`, `yelp_business_horus_sdf`, `yelp_check_in_sdf`, `yelp_reviews_sdf`, and `yelp_users_sdf`. \n",
    "\n",
    "_Note2_: Make sure DataFrame's schema makes logical sense (and don't worry about NaN's, null's for now and datetime types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "265c4b649f7129f3c4d99a617c8b768e",
     "grade": false,
     "grade_id": "cell-5c9280411a8d7af8",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# TODO: Load Yelp datasets\n",
    "\n",
    "yelp_business_attributes_sdf = spark.read.load('data/yelp_business_attributes.csv', format=\"csv\", header=True)\n",
    "yelp_business_hours_sdf = spark.read.load('data/yelp_business_hours.csv',format=\"csv\", header=True)\n",
    "yelp_business_sdf = spark.read.load('data/yelp_business.csv',format=\"csv\", header=True)\n",
    "yelp_check_in_sdf = spark.read.load('data/yelp_checkin.csv',format=\"csv\", header=True)\n",
    "yelp_reviews_sdf = spark.read.load('data/yelp_review2.csv',format=\"csv\", header=True)\n",
    "yelp_users_sdf = spark.read.load('data/yelp_user.csv',format=\"csv\", header=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yelp_business_hours_sdf.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yelp_check_in_sdf.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yelp_business_sdf.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yelp_business_sdf.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yelp_reviews_sdf.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yelp_users_sdf.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yelp_business_attributes_sdf.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yelp_reviews_sdf.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "12fd4775d748b43fa179a09112551f55",
     "grade": true,
     "grade_id": "cell-49ba9885946af920",
     "locked": true,
     "points": 5,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CIS 545 Test Cases] (5 pts)\n"
     ]
    }
   ],
   "source": [
    "# [CIS 545 Test Cases] (5 pts)\n",
    "\n",
    "if yelp_reviews_sdf.dtypes[0][1] != 'string':\n",
    "    raise ValueError('Unexpected datatype on ' + yelp_reviews.dtypes[0][0])\n",
    "\n",
    "\n",
    "print('[CIS 545 Test Cases] (5 pts)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "c0472b6e3b5115f240ba7870d3ba2e3f",
     "grade": false,
     "grade_id": "cell-bf008484ff74d458",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### Step 1.4 Simple Wrangling in Spark DataFrames\n",
    "\n",
    "Currently, some of the data from the Yelp dataset is a bit ugly.\n",
    "\n",
    "In this section you will need to:\n",
    "* Create SQL tables for each Spark DataFrames you created in the previous step (**Note**: The table names should be the same as the Spark DataFrames, except remove the `_sdf` suffix i.e. yelp_business_sdf --> yelp_business ).\n",
    "* Clean `yelp_business_hours` by replacing `\"None\"` with a Spark `null`.\n",
    "* Clean `yelp_users` by replacing `\"None\"` with a Spark `null`.\n",
    "\n",
    "_Hint_: You might find this documentation helpful... http://spark.apache.org/docs/2.1.0/api/python/pyspark.sql.html#pyspark.sql.SparkSession.sql"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "080adff7927441f182fc61cb45b454f1",
     "grade": false,
     "grade_id": "cell-5116e54bc3b2ddff",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### Step 1.4.1 Spark DataFrames SQL Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "c327dc4fda29c1859d9c0e0a35a99be4",
     "grade": false,
     "grade_id": "cell-9a3a748f4b0da7d4",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# TODO: Create SQL tables with names such as yelp_business, yelp_users\n",
    "\n",
    "yelp_business_attributes_sdf.createOrReplaceTempView(\"yelp_business_attributes\")\n",
    "yelp_business_attributes = spark.sql(\"SELECT * FROM yelp_business_attributes\")\n",
    "yelp_business_hours_sdf.createOrReplaceTempView(\"yelp_business_hours\")\n",
    "yelp_business_hours = spark.sql(\"SELECT * FROM yelp_business_hours\")\n",
    "yelp_business_sdf.createOrReplaceTempView('yelp_business')\n",
    "#yelp_business = spark.sql(\"SELECT * FROM yelp_business\")\n",
    "yelp_check_in_sdf.createOrReplaceTempView('yelp_check_in')\n",
    "yelp_check_in = spark.sql(\"SELECT * FROM yelp_check_in\")\n",
    "yelp_reviews_sdf.createOrReplaceTempView('yelp_reviews')\n",
    "yelp_reviews = spark.sql(\"SELECT * FROM yelp_reviews\")\n",
    "yelp_users_sdf.createOrReplaceTempView('yelp_users')\n",
    "yelp_users = spark.sql(\"SELECT * FROM yelp_users\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "d01a4a79c947346018dc0d921672a962",
     "grade": false,
     "grade_id": "cell-c77f2e12c852a699",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### Step 1.4.2 Cleaning None's\n",
    "To now convert the string `\"None\"` to Spark `null`, you'll need to define a function `replace_none_with_null` to apply to then apply to your DataFrames.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "c363ece79cf372f660c1e84cbd22d57f",
     "grade": false,
     "grade_id": "cell-b9897aabcb6e4350",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# TODO: Create replace_none_with_null function\n",
    "import pyspark.sql.types as T\n",
    "def replace_none_with_null(x):\n",
    "    # This function should take a string parameter and compare it with \n",
    "    # 'None' and 'Na'. If there is a match to either, it should return\n",
    "    # the Python None value otherwise it should return the passed value.\n",
    "\n",
    "    if x == 'None' or x=='Na':\n",
    "        return None\n",
    "    else:\n",
    "        return x\n",
    "\n",
    "#udf = F.UserDefinedFunction(replace_none_with_null, T.StringType())\n",
    "#yelp_business_attributes = yelp_business_attributes.select(*[udf(column) for column in yelp_business_attributes.columns])\n",
    "#yelp_business_hours = yelp_business_hours.select(*[udf(column) for column in yelp_business_hours.columns])\n",
    "#yelp_business = yelp_business.select(*[udf(column) for column in yelp_business.columns])\n",
    "#yelp_check_in = yelp_check_in.select(*[udf(column) for column in yelp_check_in.columns])\n",
    "#yelp_reviews = yelp_reviews.select(*[udf(column) for column in yelp_reviews.columns])\n",
    "#yelp_users = yelp_users.select(*[udf(column) for column in yelp_users.columns])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "993ac2f5421dcb36ad8dc87568508d4c",
     "grade": true,
     "grade_id": "cell-ecb54071b1fb030a",
     "locked": true,
     "points": 5,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CIS 545 Test Cases] (5 pts)\n"
     ]
    }
   ],
   "source": [
    "# [CIS 545 Test Cases] (5 pts)\n",
    "\n",
    "if replace_none_with_null('None'):\n",
    "    raise ValueError('Your function does not work')\n",
    "\n",
    "\n",
    "print('[CIS 545 Test Cases] (5 pts)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following code wraps the Python code in a Spark UDF \n",
    "# (User Defined Function) such that you can use this function \n",
    "# in your spark.sql('...') commands. \n",
    "# Run this cell it to create the Spark UDF\n",
    "\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StringType, NullType\n",
    "\n",
    "spark.udf.register('replace_none_with_null', replace_none_with_null)\n",
    "spark_replace_none_with_null = udf(replace_none_with_null)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "34df92fc7ab476e29bd734efbb44cf6f",
     "grade": false,
     "grade_id": "cell-6fbf57649df0c37d",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "Now use the above created UDF in SQL, or `spark_replace_none_with_null` if you prefer Pandas-style Spark statements, to replace the 'None's and 'Na's from `yelp_business_hours_sdf` and `yelp_users`. (Note: If you are getting a weird Java error, try re-starting your kernel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "4db653c38874f44e07e8e1aa1abbb849",
     "grade": false,
     "grade_id": "cell-e2d3471775b15155",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# TODO: Clean yelp_business_hours_sdf\n",
    "# Hint: yelp_business_hours_sdf schema: business_id, monday, tuesday, wednesday, thursday, friday, saturday, sunday\n",
    "\n",
    "# *** YOUR CODE HERE ***\n",
    "#udf = F.UserDefinedFunction(replace_none_with_null, T.StringType())\n",
    "yelp_business_hours_sdf= yelp_business_hours_sdf.select(spark_replace_none_with_null('business_id').alias('business_id'),\n",
    "                                                       spark_replace_none_with_null('monday').alias('monday') ,\n",
    "                                                       spark_replace_none_with_null('tuesday').alias('tuesday'),\n",
    "                                                       spark_replace_none_with_null('wednesday').alias('wednesday'),\n",
    "                                                       spark_replace_none_with_null('thursday').alias('thursday'),\n",
    "                                                       spark_replace_none_with_null('friday').alias('friday'),\n",
    "                                                       spark_replace_none_with_null('saturday').alias('saturday'),\n",
    "                                                       spark_replace_none_with_null('sunday').alias('sunday'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(business_id='FYWN1wneV18bWNgQjJ2GNg', monday='7:30-17:0', tuesday='7:30-17:0', wednesday='7:30-17:0', thursday='7:30-17:0', friday='7:30-17:0', saturday=None, sunday=None),\n",
       " Row(business_id='He-G7vWjzVUysIKrfNbPUQ', monday='9:0-20:0', tuesday='9:0-20:0', wednesday='9:0-20:0', thursday='9:0-20:0', friday='9:0-16:0', saturday='8:0-16:0', sunday=None),\n",
       " Row(business_id='KQPW8lFf1y5BT2MxiSZ3QA', monday=None, tuesday=None, wednesday=None, thursday=None, friday=None, saturday=None, sunday=None),\n",
       " Row(business_id='8DShNS-LuFqpEWIp0HxijA', monday='10:0-21:0', tuesday='10:0-21:0', wednesday='10:0-21:0', thursday='10:0-21:0', friday='10:0-21:0', saturday='10:0-21:0', sunday='11:0-19:0'),\n",
       " Row(business_id='PfOCPjBrlQAnz__NXj9h_w', monday='11:0-1:0', tuesday='11:0-1:0', wednesday='11:0-1:0', thursday='11:0-1:0', friday='11:0-1:0', saturday='11:0-2:0', sunday='11:0-0:0')]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yelp_business_hours_sdf.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "yelp_business_hours_sdf.createOrReplaceTempView('yelp_business_hours')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "yelp_business_hours = spark.sql(\"SELECT * FROM yelp_business_hours\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(business_id='FYWN1wneV18bWNgQjJ2GNg', monday='7:30-17:0', tuesday='7:30-17:0', wednesday='7:30-17:0', thursday='7:30-17:0', friday='7:30-17:0', saturday=None, sunday=None),\n",
       " Row(business_id='He-G7vWjzVUysIKrfNbPUQ', monday='9:0-20:0', tuesday='9:0-20:0', wednesday='9:0-20:0', thursday='9:0-20:0', friday='9:0-16:0', saturday='8:0-16:0', sunday=None),\n",
       " Row(business_id='KQPW8lFf1y5BT2MxiSZ3QA', monday=None, tuesday=None, wednesday=None, thursday=None, friday=None, saturday=None, sunday=None),\n",
       " Row(business_id='8DShNS-LuFqpEWIp0HxijA', monday='10:0-21:0', tuesday='10:0-21:0', wednesday='10:0-21:0', thursday='10:0-21:0', friday='10:0-21:0', saturday='10:0-21:0', sunday='11:0-19:0'),\n",
       " Row(business_id='PfOCPjBrlQAnz__NXj9h_w', monday='11:0-1:0', tuesday='11:0-1:0', wednesday='11:0-1:0', thursday='11:0-1:0', friday='11:0-1:0', saturday='11:0-2:0', sunday='11:0-0:0')]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yelp_business_hours.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "879d0d1f5bb02c18557c0b8543394e9f",
     "grade": false,
     "grade_id": "cell-f3d2ccd3a41852c9",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# TODO: Clean yelp_users\n",
    "# Hint: yelp_business_hours_sdf schema: user_id, name, review_count, yelping_since, friends, useful, funny, cool, fans, elite, average_stars, compliment_hot, compliment_more, compliment_profile, compliment_cute, compliment_list, compliment_note, compliment_plain, compliment_cool, compliment_funny, compliment_writer, compliment_photos\n",
    "#compliment_profile, compliment_cute, compliment_list, compliment_note, compliment_plain, compliment_cool, compliment_funny, compliment_writer, compliment_photos\n",
    "\n",
    "# *** YOUR CODE HERE ***\n",
    "yelp_users_sdf = yelp_users_sdf.select(spark_replace_none_with_null('user_id').alias('user_id'),\n",
    "                                      spark_replace_none_with_null('name').alias('name'),\n",
    "                                      spark_replace_none_with_null('review_count').alias('review_count'),\n",
    "                                      spark_replace_none_with_null('yelping_since').alias('yelping_since'),\n",
    "                                      spark_replace_none_with_null('friends').alias('friends'),\n",
    "                                      spark_replace_none_with_null('useful').alias('useful'),\n",
    "                                      spark_replace_none_with_null('funny').alias('funny'),\n",
    "                                      spark_replace_none_with_null('cool').alias('cool'),\n",
    "                                      spark_replace_none_with_null('fans').alias('fans'),\n",
    "                                      spark_replace_none_with_null('elite').alias('elite'),\n",
    "                                      spark_replace_none_with_null('average_stars').alias('average_stars'),\n",
    "                                      spark_replace_none_with_null('compliment_hot').alias('compliment_hot'),\n",
    "                                      spark_replace_none_with_null('compliment_more').alias('compliment_more'),\n",
    "                                      spark_replace_none_with_null('compliment_profile').alias('compliment_profile'),\n",
    "                                      spark_replace_none_with_null('compliment_cute').alias('compliment_cute'),\n",
    "                                      spark_replace_none_with_null('compliment_list').alias('compliment_list'),\n",
    "                                      spark_replace_none_with_null('compliment_note').alias('compliment_note'),\n",
    "                                      spark_replace_none_with_null('compliment_plain').alias('compliment_plain'),\n",
    "                                      spark_replace_none_with_null('compliment_cool').alias('compliment_cool'),\n",
    "                                      spark_replace_none_with_null('compliment_funny').alias('compliment_funny'),\n",
    "                                      spark_replace_none_with_null('compliment_writer').alias('compliment_writer'),\n",
    "                                      spark_replace_none_with_null('compliment_photos').alias('compliment_photos'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "yelp_users_sdf.createOrReplaceTempView(\"yelp_users\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure the SDF's were cleaned correctly\n",
    "#yelp_business_hours_sdf.show(10)\n",
    "yelp_users = spark.sql(\"SELECT * FROM yelp_users\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "502ab09fc83fdb3eaa204b3321d6e542",
     "grade": true,
     "grade_id": "cell-bb78025dd4ec0cbf",
     "locked": true,
     "points": 5,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CIS 545 Test Cases] (5 pts)\n"
     ]
    }
   ],
   "source": [
    "# [CIS 545 Test Cases] (5 pts)\n",
    "\n",
    "try:\n",
    "    if spark.sql('select count(*) as count from yelp_business_hours where wednesday=\\'None\\'').take(1)[0]['count'] > 0:\n",
    "        raise ValueError('Did not successfully clean business hours')\n",
    "except TypeError as te:\n",
    "    pass\n",
    "    \n",
    "print('[CIS 545 Test Cases] (5 pts)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "14126919aec7a71857aa458ad03fb127",
     "grade": false,
     "grade_id": "cell-e9f7f7a61e269781",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### Step 1.5 Simple Analytics on the Data\n",
    "\n",
    "In this section, we will be executing Spark operations on the data given. Beyond simply executing the queries, you can use `.explain()` to see more about the query execution. You should also look at the schemas of the SDFs using `.dtypes` before attempting the questions to understand the data.\n",
    "\n",
    "#### 1.5.1 Most reviewed business in PA \n",
    "\n",
    "Use `yelp_business_sdf` to create a new SDF named `PA_most_reviewed_sdf`.  This should be a table of businesses that: <br>\n",
    "(1) are in the `state` of Pennsylvania (PA) <br>\n",
    "(2) with a rating (`stars`) greater than or equal to 4.0  <br>\n",
    "(3) finally, sort this by the number of reviews (`review_count`) in descending order, and if they have the same `review_count` then sort alphabetically by their names in ascending order.\n",
    "\n",
    "The columns of `PA_most_reviewed_sdf` should be: `name`, `stars`, `review_count`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "4befffb482da80c2f7817026a0b84c5c",
     "grade": false,
     "grade_id": "cell-7a42b0372c015890",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# TODO: Create PA_most_reviewed_sdf\n",
    "\n",
    "PA_most_reviewed_sdf = yelp_business_sdf[(yelp_business_sdf['state']=='PA') & (yelp_business_sdf['stars']>=4.0)]['name','stars','review_count']\n",
    "PA_most_reviewed_sdf = PA_most_reviewed_sdf.orderBy(['review_count', 'name'], ascending=[False,True] )\n",
    "#PA_most_reviewed_sdf = PA_most_reviewed_sdf.orderBy('name')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+------------+\n",
      "|                name|stars|review_count|\n",
      "+--------------------+-----+------------+\n",
      "|Tootie's Famous I...|  4.0|          99|\n",
      "|Vincent's Pizza Park|  4.0|          99|\n",
      "|      Brew Gentlemen|  4.5|          98|\n",
      "|              Lot 17|  4.0|          98|\n",
      "|             Peppi's|  4.5|          98|\n",
      "|       Totin's Diner|  4.0|          98|\n",
      "| East End Food Co-Op|  4.5|          97|\n",
      "|       Hidden Harbor|  4.5|          97|\n",
      "|   Kavsar Restaurant|  4.5|          97|\n",
      "|     Pizza Perfectta|  4.0|          97|\n",
      "+--------------------+-----+------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "PA_most_reviewed_sdf.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "13b3c0da3c7e0dc625b5a0f354345443",
     "grade": true,
     "grade_id": "cell-159fc8b8e14b3f34",
     "locked": true,
     "points": 5,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CIS 545 Test Cases] (5 pts)\n"
     ]
    }
   ],
   "source": [
    "# [CIS 545 Test Cases] (5 pts)\n",
    "\n",
    "print('[CIS 545 Test Cases] (5 pts)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "28a04e433ce89337e22dc56bba1436d3",
     "grade": false,
     "grade_id": "cell-873523cd739d46bb",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "#### 1.5.2 Businesses with the highest average review\n",
    "\n",
    "Use `yelp_reviews_sdf` and `yelp_business_sdf` to create a new SDF named `best_businesses_sdf`. This should be a table of business names <br>\n",
    "(1) sorted by the average review score (in descending order) and if they have the same score they should then be sorted alphabetically by their name (ascending order)\n",
    "\n",
    "The columns of `best_businesses_sdf` should be: `name`, `avg_rating` \n",
    "\n",
    "**Note**: You should use `yelp_business_sdf` ONLY to get the name of the businesses given the business_id from `yelp_reviews_sdf`. Both SDFs share a `stars` field, but we will only be interested in the ones in  `yelp_reviews_sdf ` i.e. think groupby on yelp_review's business_id to get an average star rating for each business. If you were to use only `yelp_business_sdf` you would get a slightly different that would not match our answer key!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yelp_reviews_sdf.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yelp_business_sdf.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "1e7d5e5b949ddeebbd90b163d18cac23",
     "grade": false,
     "grade_id": "cell-8f80aa1585dfce0a",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# TO DO: Create best_businesses_sdf\n",
    "yelp_reviews_sdf = spark.sql('''\n",
    "SELECT business_id, AVG(stars) as avg_rating FROM yelp_reviews\n",
    "GROUP BY business_id\n",
    "''')\n",
    "best_businesses_sdf = yelp_business_sdf.join(yelp_reviews_sdf, yelp_business_sdf['business_id'] == yelp_reviews_sdf['business_id'])['name','avg_rating']\n",
    "best_businesses_sdf = best_businesses_sdf.orderBy(['avg_rating', 'name'], ascending=[False,True])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('business_id', 'string'),\n",
       " ('name', 'string'),\n",
       " ('neighborhood', 'string'),\n",
       " ('address', 'string'),\n",
       " ('city', 'string'),\n",
       " ('state', 'string'),\n",
       " ('postal_code', 'string'),\n",
       " ('latitude', 'string'),\n",
       " ('longitude', 'string'),\n",
       " ('stars', 'string'),\n",
       " ('review_count', 'string'),\n",
       " ('is_open', 'string'),\n",
       " ('categories', 'string')]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yelp_business_sdf.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------+\n",
      "|                name|avg_rating|\n",
      "+--------------------+----------+\n",
      "|\"\"\"T\"\"s Hair Affair\"|       5.0|\n",
      "|\"Davis \"\"N\"\" Sons...|       5.0|\n",
      "|\"Manantial De Sal...|       5.0|\n",
      "| \"Scotty\"\"s Kitchen\"|       5.0|\n",
      "|\"Wallbeds \"\"n\"\" M...|       5.0|\n",
      "|   $40 Backflow Test|       5.0|\n",
      "|  'Round Table Tours|       5.0|\n",
      "|           007 Nails|       5.0|\n",
      "|         1 One Armor|       5.0|\n",
      "|              1 Reef|       5.0|\n",
      "+--------------------+----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "best_businesses_sdf.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "26153e8fb9264690ca4c8172def597e6",
     "grade": true,
     "grade_id": "cell-b97f763205ba95d8",
     "locked": true,
     "points": 5,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CIS 545 Test Cases] (5 pts)\n"
     ]
    }
   ],
   "source": [
    "# [CIS 545 Test Cases] (5 pts)\n",
    "\n",
    "print('[CIS 545 Test Cases] (5 pts)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "34028379f387f4948436b4f8f5d655d7",
     "grade": false,
     "grade_id": "cell-16084b723a4f2a8f",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "#### 1.5.3 Difference from average in PA\n",
    "Sometimes it is useful to feature engineer new features to bring expose information your ML model can then leverage. This is especially true when you have small datasets.  In this problem, you will do this with `yelp_business_sdf` for businesses in PA.\n",
    "\n",
    "Use `yelp_business_sdf` to create a new SDF named `new_yelp_business_sdf`. This should be a table of businesses <br>\n",
    "(1) that are in the state of PA <br>\n",
    "(2) containing the columns `business_id`, `name` and `stars` <br>\n",
    "(3) as well as a new column called `avg_stars_diff`.  This column should contain the difference between the business's `stars` and the average `stars` rating across\n",
    "all businesses in _PA_.\n",
    "\n",
    "The columns of `new_yelp_business_sdf` should be: `business_id`, `name`, `stars` and `avg_stars_diff`.\n",
    "\n",
    "**Hint**: Calculate `average_stars` separately and then use it within your query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "4c1272e8303de15c34df06a9b4a59ceb",
     "grade": false,
     "grade_id": "cell-813cb1a1b049c607",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# TODO: Create new_yelp_business_sdf\n",
    "from pyspark.sql.functions import mean\n",
    "new_yelp_business_sdf = yelp_business_sdf[(yelp_business_sdf['state']=='PA')]['business_id', 'name', 'stars']\n",
    "#new_yelp_business_sdf.createOrReplaceTempView('new_yelp_business')\n",
    "#new_yelp_business = spark.sql('select * from new_yelp_business')\n",
    "#avg = spark.sql('SELECT AVG(stars) as avg_rating FROM new_yelp_business')\n",
    "average = new_yelp_business_sdf.agg(mean(new_yelp_business_sdf['stars'].cast('double')).alias(\"mean\")).collect()[0][\"mean\"]\n",
    "#new_yelp_business_sdf= new_yelp_business_sdf.join(avg, new_yelp_business_sdf['business_id']==avg['business_id'])\n",
    "new_yelp_business_sdf = new_yelp_business_sdf.withColumn('avg_stars_diff', new_yelp_business_sdf['stars'].cast('double') - average)\n",
    "\n",
    "new_yelp_business_sdf = new_yelp_business_sdf.orderBy('avg_stars_diff')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.6071817192600655"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-2.6071817192600655"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_yelp_business_sdf.first().avg_stars_diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+-----+-------------------+\n",
      "|         business_id|                name|stars|     avg_stars_diff|\n",
      "+--------------------+--------------------+-----+-------------------+\n",
      "|YS1XX9AONq2hdBbU3...|          McDonald's|  1.0|-2.6071817192600655|\n",
      "|d1VkwosIRj65-FYDf...|        CVS Pharmacy|  1.0|-2.6071817192600655|\n",
      "|AJhf56_l_NilDEwjc...| Monroeville Massage|  1.0|-2.6071817192600655|\n",
      "|AypqU-0z1PYkT5O4L...|Crystal Springs W...|  1.0|-2.6071817192600655|\n",
      "|lLhK23e7sX4cTqvzI...|Empire Beauty School|  1.0|-2.6071817192600655|\n",
      "|5z1tzT8C7H1l7onTt...|              Richey|  1.0|-2.6071817192600655|\n",
      "|F0xDG6OkLSpXAm9RV...|Dependable Remode...|  1.0|-2.6071817192600655|\n",
      "|LBJXtzJQbSElsKZHS...|        Pitney Bowes|  1.0|-2.6071817192600655|\n",
      "|kDXMDfHYbhkPTdSqQ...|      Quality Suites|  1.0|-2.6071817192600655|\n",
      "|cPNyy7N02xQEqyd0o...|Novak Berkowitz &...|  1.0|-2.6071817192600655|\n",
      "+--------------------+--------------------+-----+-------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "new_yelp_business_sdf.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "10865e205ee1a31a4ce1d06405cba432",
     "grade": true,
     "grade_id": "cell-c6740f643f11f0b4",
     "locked": true,
     "points": 5,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CIS 545 Test Cases] (5 pts)\n"
     ]
    }
   ],
   "source": [
    "# [CIS 545 Test Cases] (5 pts)\n",
    "\n",
    "if new_yelp_business_sdf.first().avg_stars_diff < -2.7 or new_yelp_business_sdf.first().avg_stars_diff > -2.6:\n",
    "    raise ValueError('Unexpected avg_stars_diff for top best business')\n",
    "\n",
    "print('[CIS 545 Test Cases] (5 pts)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "ca659f526be2506614fe1d8b2dbe39bb",
     "grade": false,
     "grade_id": "cell-3794359bdb2cf3bb",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "#### 1.5.4 Most common check-in days/times times in PA\n",
    "\n",
    "You will now find the weekday/hour pairs with the most checkins for all businesses in PA, sorted in descending order of checkins. To do this you will use `yelp_check_in_sdf` and `yelp_business_sdf` to create a new SDF named `common_PA_checkin_sdf`.\n",
    "\n",
    "`common_PA_checkin_sdf` should be a table of businesses with <br>\n",
    "(1) columns `weekday`, `hour` and <br>\n",
    "(2) a new column called `num_checkins` containing the total number of checkins that occurred on the given weekday/hour pair in **any business in PA**. Make sure that the `hour` column contains only the hour number, i.e. 8:00 --> 8  \n",
    "\n",
    "The columns of `common_PA_checkin_sdf` should be: `weekday`, `hour` and `num_checkins`\n",
    "\n",
    "**Note**: The hour column in yelp_business is a string of format hh:mm, to get just the hour use `SPLIT()`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('business_id', 'string'),\n",
       " ('weekday', 'string'),\n",
       " ('hour', 'string'),\n",
       " ('checkins', 'string')]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yelp_check_in_sdf.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "get_col = pyspark.sql.functions.split(yelp_check_in_sdf['hour'], ':')\n",
    "yelp_check_in_sdf = yelp_check_in_sdf.withColumn('hour', get_col.getItem(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "business_idx = yelp_business_sdf[(yelp_business_sdf['state']=='PA')]['business_id', 'name']\n",
    "business_idx = business_idx.withColumnRenamed('business_id', 'id')\n",
    "common_PA_checkins_sdf = yelp_check_in_sdf.join(business_idx, business_idx['id'] == yelp_check_in_sdf['business_id'], how='inner')\n",
    "common_PA_checkins_sdf.createOrReplaceTempView('common_PA_checkins')\n",
    "common_PA_checkins_sdf = spark.sql('''\n",
    "SELECT weekday, hour, sum(checkins) as num_checkins FROM common_PA_checkins\n",
    "GROUP BY weekday, hour\n",
    "ORDER BY num_checkins DESC\n",
    "''')\n",
    "#common_PA_checkins_sdf = common_PA_checkins_sdf.join(summ, common_PA_checkins_sdf['business_id']==summ['idx'])['weekday', 'hour', 'num_checkins']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "4d3c81fb744a496faaafba1d57f4dc2f",
     "grade": false,
     "grade_id": "cell-0c2629d1dd76ba0e",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# TODO: Create common_PA_checkins_sdf\n",
    "\n",
    "common_PA_checkin_sdf = common_PA_checkins_sdf\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----+------------+\n",
      "|weekday|hour|num_checkins|\n",
      "+-------+----+------------+\n",
      "|    Fri|  23|     11082.0|\n",
      "|    Sat|  23|     10644.0|\n",
      "|    Sat|  17|     10347.0|\n",
      "|    Sat|  18|      9951.0|\n",
      "|    Fri|  22|      9901.0|\n",
      "|    Sat|  22|      9748.0|\n",
      "|    Sat|   0|      9640.0|\n",
      "|    Sat|  16|      9604.0|\n",
      "|    Sun|   0|      9305.0|\n",
      "|    Sun|  17|      9175.0|\n",
      "+-------+----+------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "common_PA_checkin_sdf.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "441131b8032606951ee0a92e2d136953",
     "grade": true,
     "grade_id": "cell-f24b8f262a886f26",
     "locked": true,
     "points": 5,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CIS 545 Test Cases] (5 pts)\n"
     ]
    }
   ],
   "source": [
    "# [CIS 545 Test Cases] (5 pts)\n",
    "\n",
    "print('[CIS 545 Test Cases] (5 pts)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "86ab5cf678b61b1c8f0e888ca3418af3",
     "grade": true,
     "grade_id": "cell-280220ad65a12768",
     "locked": true,
     "points": 5,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CIS 545 Test Cases] (5 pts)\n"
     ]
    }
   ],
   "source": [
    "# [CIS 545 Test Cases] (5 pts)\n",
    "\n",
    "print('[CIS 545 Test Cases] (5 pts)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "1ec9d760b7e5dc5207708c45b6d4f739",
     "grade": false,
     "grade_id": "cell-14d714b839487bbc",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "# Step 2. Simple Graph Algorithms\n",
    "\n",
    "## 2.1 Generate user-business graph\n",
    "\n",
    "For this step, you will construct a *directed* graph with edges from users to businesses indicating reviews. To do this, you should extract from `yelp_reviews_sdf` the `user_id` as the `from_node`, the `business_id` as the `to_node`, and the `stars` field as the `score`.  Put this into a dataframe called `review_graph_sdf`, and make it available as a table in SQL called `review_graph`.\n",
    "\n",
    "Some of the values may be null; remove these for `user_id` (`from_node`) or `business_id` (`to_node`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "67e193f74c2d01d0962b2e95a3dd5b86",
     "grade": false,
     "grade_id": "cell-bc86854105e0a8e5",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# TODO: Create review_graph\n",
    "\n",
    "# ** YOUR CODE HERE **\n",
    "\n",
    "review_graph_sdf = spark.sql('select user_id as from_node, business_id as to_node, stars as score from yelp_reviews')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+-----+\n",
      "|           from_node|             to_node|score|\n",
      "+--------------------+--------------------+-----+\n",
      "|bv2nCi5Qv5vroFiqK...|AEx2SYEUJmTxVVB18...|    5|\n",
      "|bv2nCi5Qv5vroFiqK...|VR6GpWIda3SfvPC-l...|    5|\n",
      "|bv2nCi5Qv5vroFiqK...|CKC0-MOWMqoeWf6s-...|    5|\n",
      "|bv2nCi5Qv5vroFiqK...|ACFtxLv8pGrrxMm6E...|    4|\n",
      "|bv2nCi5Qv5vroFiqK...|s2I_Ni76bjJNK9yG6...|    4|\n",
      "|_4iMDXbXZ1p1ONG29...|8QWPlVQ6D-OExqXoa...|    5|\n",
      "|u0LXt3Uea_GidxRW1...|9_CGhHMz8698M9-Pk...|    4|\n",
      "|u0LXt3Uea_GidxRW1...|gkCorLgPyQLsptTHa...|    4|\n",
      "|u0LXt3Uea_GidxRW1...|5r6-G9C4YLbC7Ziz5...|    3|\n",
      "|u0LXt3Uea_GidxRW1...|fDF_o2JPU8BR1Gya-...|    5|\n",
      "+--------------------+--------------------+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "review_graph_sdf.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "review_graph_sdf = review_graph_sdf.filter(review_graph_sdf['from_node'].isNotNull()) \n",
    "review_graph_sdf = review_graph_sdf.filter(review_graph_sdf['to_node'].isNotNull()) \n",
    "#review_graph_sdf = review_graph_sdf.filter(review_graph_sdf['score'].isNotNull()) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "eb4b806a505500305cdd26e024335cef",
     "grade": true,
     "grade_id": "cell-241e3c2832cd96fd",
     "locked": true,
     "points": 5,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CIS 545 Test Cases] (5 pts)\n"
     ]
    }
   ],
   "source": [
    "# [CIS 545 Test Cases] (5 pts)\n",
    "\n",
    "if review_graph_sdf.count() != 5273700:\n",
    "    raise ValueError('Unexpected graph size')\n",
    "\n",
    "print('[CIS 545 Test Cases] (5 pts)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "489ef54b6d0631a233cc1825d5d48ec4",
     "grade": false,
     "grade_id": "cell-d06e0fcc4d8c467f",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "## 2.2 Distributed Breadth-First Search\n",
    "A search algorithm typically starts at a node or set of nodes, and “explores” or “walks” for some number of steps to find a match or a set of matches.\n",
    "\n",
    "Let’s implement a distributed version of a popular algorithm, breadth-first-search (BFS).  This algorithm is given a graph `G`, a set of origin nodes `N`, and a depth `d`.  In each iteration or round up to depth `d`, it explores the set of all new nodes directly connected to the nodes it already has seen, before going on to the nodes another “hop” away.  If we do this correctly, we will explore the graph in a way that (1) avoids getting caught in cycles or loops, and (2) visits each node in the fewest number of “hops” from the origin.  BFS is commonly used in tasks such as friend recommendation in social networks.\n",
    "\n",
    "![ChessUrl](https://upload.wikimedia.org/wikipedia/commons/5/5d/Breadth-First-Search-Algorithm.gif \"BFS\")\n",
    "\n",
    "**How does distributed BFS in Spark work**?  Let’s start with a brief sketch of standard BFS.  During exploration “rounds”, we can divide the graph into three categories:\n",
    "\n",
    "1. *unexplored nodes*.  These are nodes we have not yet visited.  You don’t necessarily need to track these separately from the graph.\n",
    "2. *visited nodes*.  We have already reached these nodes in a previous “round”.\n",
    "3. *frontier nodes*.  These are nodes we have visited in this round.  We have not yet checked whether they have out-edges connecting to unexplored nodes.\n",
    "\n",
    "We can illustrate these with a figure and an example.\n",
    "\n",
    "\n",
    "![Graph traversal](https://drive.google.com/uc?export=view&id=1I2Kc3uQcDlp7RsDqRQAfQAvS3F_VcJpA)\n",
    "\n",
    "Let’s look at the figure, which shows a digraph.  The green node A represents the origin.\n",
    "\n",
    "* In the first round, the origin A is the sole frontier node.  We find all nodes reachable directly from A, namely B-F; then we remove all nodes we have already visited (there are none) or that are in the frontier (the node A itself).  This leaves the blue nodes B-F, which are all reachable in (at most) 1 hop from A.\n",
    "* In the second round, we move A to the visited set and B-F to the frontier.  Now we explore all nodes connected directly to frontier nodes, namely A (from B), F (from E), and the red nodes G-L.  We eliminate the nodes already contained in the frontier and visited sets from the next round’s frontier set, leaving the red nodes only.\n",
    "* In the third round, we will move B-F to the visited set, G-L to the frontier set, and explore the next round of neighbors N-V.  This process continues up to some maximum depth (or until there are no more unexplored nodes).\n",
    "\n",
    "Assume we create data structures (we can make them DataFrames) for the visited and frontier nodes.  Consider (1) how to initialize the different sets at the start of computation (note: unexplored nodes are already in the graph), and (2) how to use the graph edges and the existing data structures to update state for the next iteration “round”.\n",
    "\n",
    "You might possibly have seen how to create a breadth-first-search algorithm in a single-CPU programming language, using a queue to capture the frontier nodes. With Spark we don’t need a queue -- we just need the three sets above.\n",
    "\n",
    "### 2.2.1 Breadth-First Search Algorithm\n",
    "\n",
    "Create a function `spark_bfs(G, origins, max_depth)` that takes a Spark DataFrame with a graph G (following the schema for `review_graph_sdf` described above, but to be treated as an **undirected graph**), a Python list-of-dictionaries `origins` of the form \n",
    "\n",
    "```\n",
    "[{‘node’: nid1}, \n",
    " {‘node’: nid2}, \n",
    " …]\n",
    "```\n",
    "\n",
    "and a nonnegative integer “exploration depth” `max_depth` (to only run BFS on a tractable portion of the graph).  The `max_depth` will be the maximum number of edge traversals (e.g., the origin is at `max_depth=0`, one hop from the origin is `max_depth=1`, etc.  The function should return a DataFrame containing pairs of the form (node, distance), where the distance is depth at which $n$ was *first* encountered (i.e., the shortest-path distance from the origin nodes).  Note that the origin nodes should also be returned in this Spark DataFrame (with depth 0)!  \n",
    "\n",
    "You can create a new Spark DataFrame with an integer `node` column from the above list of maps `origins`, as follows. This will give you a DataFrame of the nodes to start the BFS at\n",
    "\n",
    "```\n",
    "schema = StructType([\n",
    "            StructField(\"node\", StringType(), True)\n",
    "        ])\n",
    "\n",
    "    my_sdf = spark.createDataFrame(my_list_of_maps, schema)\n",
    "```\n",
    "\n",
    "In this algorithm, be careful in each iteration to keep only the nodes with their shortest distances (you may need to do aggregation or item removal).  You should accumulate all nodes at distances 0, 1, ..., `max_depth`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              total        used        free      shared  buff/cache   available\r\n",
      "Mem:        8029748     3135544      194864      399696     4699340     4223116\r\n",
      "Swap:       1344768      626432      718336\r\n"
     ]
    }
   ],
   "source": [
    "!free"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "19ad592a2bb6386ea2cf5ac05d3eef2d",
     "grade": false,
     "grade_id": "cell-e97e2f9ec6bf695b",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# TODO: Write Spark BFS Functions\n",
    "\n",
    "def spark_bfs(G, origins, max_depth):\n",
    "    front_sdf = spark.createDataFrame(origins, schema)\n",
    "    visited_sdf = spark.createDataFrame(origins, schema).withColumn('depth',F.lit(0))\n",
    "    for depth in range(max_depth):\n",
    "        visited_sdf.show(5)\n",
    "        edge_sdf = front_sdf.join(G, (front_sdf['node'] == G['from_node']) | (front_sdf['node'] == G['to_node']),'inner').cache()\n",
    "        front_sdf = edge_sdf.select(edge_sdf['to_node'].alias('node'))\n",
    "        from_node = edge_sdf.select(edge_sdf['from_node'].alias('node'))\n",
    "        front_sdf = front_sdf.union(from_node).drop_duplicates(['node']).cache()\n",
    "        front_sdf = front_sdf.join(visited_sdf, visited_sdf['node'] == front_sdf['node'], 'leftanti')\n",
    "        front_sdf.show(10)\n",
    "        visited_sdf = visited_sdf.union(front_sdf.withColumn('depth',F.lit(depth+1))).drop_duplicates().cache()\n",
    "    return visited_sdf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = StructType([StructField(\"node\",StringType(),True)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "4fa49536e5c3a0264ca0675f056a564c",
     "grade": true,
     "grade_id": "cell-ce2fd419f78b8322",
     "locked": true,
     "points": 10,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|                node|depth|\n",
      "+--------------------+-----+\n",
      "|bv2nCi5Qv5vroFiqK...|    0|\n",
      "+--------------------+-----+\n",
      "\n",
      "+--------------------+\n",
      "|                node|\n",
      "+--------------------+\n",
      "|VR6GpWIda3SfvPC-l...|\n",
      "|s2I_Ni76bjJNK9yG6...|\n",
      "|AEx2SYEUJmTxVVB18...|\n",
      "|ACFtxLv8pGrrxMm6E...|\n",
      "|CKC0-MOWMqoeWf6s-...|\n",
      "+--------------------+\n",
      "\n",
      "+--------------------+-----+\n",
      "|                node|depth|\n",
      "+--------------------+-----+\n",
      "|bv2nCi5Qv5vroFiqK...|    0|\n",
      "|s2I_Ni76bjJNK9yG6...|    1|\n",
      "|AEx2SYEUJmTxVVB18...|    1|\n",
      "|VR6GpWIda3SfvPC-l...|    1|\n",
      "|CKC0-MOWMqoeWf6s-...|    1|\n",
      "+--------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n",
      "+--------------------+\n",
      "|                node|\n",
      "+--------------------+\n",
      "|qZcP5lo3KW5SvVXbe...|\n",
      "|QejrB5aH2IUmr4rXA...|\n",
      "|-bgszoDnhaUEuVydd...|\n",
      "|vvk0SrRa9no3CrLEL...|\n",
      "|hAiso8IQfKW88uWUG...|\n",
      "|wmayxaF9C8wAkEllD...|\n",
      "|dT1jqOZrFUmY4m4o3...|\n",
      "|GlEhxPlaofxOOvcf-...|\n",
      "|YVaONBBT9lSJQwBPW...|\n",
      "|bYcOiNUIKLOn2Aral...|\n",
      "+--------------------+\n",
      "only showing top 10 rows\n",
      "\n",
      "+--------------------+-----+\n",
      "|                node|depth|\n",
      "+--------------------+-----+\n",
      "|-bgszoDnhaUEuVydd...|    2|\n",
      "|qZfMILmXeiTGiZQXJ...|    2|\n",
      "|XYjB4jCthoDf1aeFG...|    2|\n",
      "|7eFy-LxZuvS2K6PAQ...|    2|\n",
      "|TOJJE4oWFTet2t8xU...|    2|\n",
      "+--------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n",
      "+--------------------+\n",
      "|                node|\n",
      "+--------------------+\n",
      "|N3J76CRP2H52NUo4V...|\n",
      "|4iY_gyKX2ogbem7ra...|\n",
      "|RtUvSWO_UZ8V3Wpj0...|\n",
      "|q1EeYFpt_E9tVo8_o...|\n",
      "|mI17MpWybqUw2_6WZ...|\n",
      "|JLbgvGM4FXh9zNP4O...|\n",
      "|0owIRP_z5RcYKKmh5...|\n",
      "|--9e1ONYQuAa-CB_R...|\n",
      "|Fmtg_NZ1Ib1eBgbtM...|\n",
      "|eKznX8VTfcQrjCqXp...|\n",
      "+--------------------+\n",
      "only showing top 10 rows\n",
      "\n",
      "13603\n",
      "[CIS 545 Test Cases] (10 pts)\n"
     ]
    }
   ],
   "source": [
    "# [CIS 545 Test Cases] (10 pts)\n",
    "\n",
    "orig  = [{'node': 'bv2nCi5Qv5vroFiqKGopiw'}] \n",
    "count = spark_bfs(review_graph_sdf, orig, 3).count()\n",
    "print(count)\n",
    "\n",
    "print('[CIS 545 Test Cases] (10 pts)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2.2.2 Restaurant Recommendation\n",
    "\n",
    "Now create a function `friend_rec` that takes in two arguments: the graph_sdf and the ID of a user, `user`.  Using the `spark_bfs()` as a help function it should return restaurants with a rating greater than 4.0 that were reviewed by users who reviewed similar restaurants as `user` reviewed.  To do this, first find all users who have reviewed some restaurant that `user` reviewed, and then find all restaurants that they reviewed (excluding the ones `user` already reviewed) with a rating greater or equal to 4.0. \n",
    "\n",
    "**Note**: Your resulting spark dataframe should contain the following columns: `name`, `score`, and `count`, where count is the number of times that restaurant was reviewed). It should be sorted primarily by count (in descending order), and then secondarily by name (in ascending order).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "05ae7ce8ecaae85a489d911f8dd11903",
     "grade": false,
     "grade_id": "cell-f8acaf9993a924f6",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# TO DO Implement friend_rec using spark_bfs() \n",
    "\n",
    "                        \n",
    "def friend_rec(review_graph_sdf, user):\n",
    "    \n",
    "    # ** YOUR CODE HERE **\n",
    "    orig  = [{'node': user}] \n",
    "    review_graph_sdf.createOrReplaceTempView('review_graph')\n",
    "    friends = spark_bfs(review_graph_sdf, orig, 3).cache()\n",
    "    friends1 = friends[friends['depth']==3]\n",
    "    friends1.createOrReplaceTempView('friends1')\n",
    "    friends2  = friends[friends['depth']==2]\n",
    "    friends2.createOrReplaceTempView('friends2')\n",
    "    friend_graph_sdf = spark.sql('select to_node, count(to_node) as count from review_graph where from_node in (select node from friends2) and to_node in (select node from friends1) GROUP BY to_node')\n",
    "    friend_graph_sdf = friend_graph_sdf.join(yelp_business_sdf, friend_graph_sdf['to_node']==yelp_business_sdf['business_id'], 'inner')['name', 'count', 'stars']\n",
    "    friend_graph_sdf.show(10)\n",
    "    friend_graph_sdf.createOrReplaceTempView('friend_graph_sdf')\n",
    "    print('done')\n",
    "    friend_graph_sdf = spark.sql('select name,  stars as score, count from friend_graph_sdf where stars>=4').orderBy(['count', 'name'], ascending=[False,True]).cache()\n",
    "    return friend_graph_sdf\n",
    "    \n",
    "# recomm = friend_rec(review_graph_sdf, 'bv2nCi5Qv5vroFiqKGopiw')\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|                node|depth|\n",
      "+--------------------+-----+\n",
      "|bv2nCi5Qv5vroFiqK...|    0|\n",
      "+--------------------+-----+\n",
      "\n",
      "+--------------------+\n",
      "|                node|\n",
      "+--------------------+\n",
      "|VR6GpWIda3SfvPC-l...|\n",
      "|s2I_Ni76bjJNK9yG6...|\n",
      "|AEx2SYEUJmTxVVB18...|\n",
      "|ACFtxLv8pGrrxMm6E...|\n",
      "|CKC0-MOWMqoeWf6s-...|\n",
      "+--------------------+\n",
      "\n",
      "+--------------------+-----+\n",
      "|                node|depth|\n",
      "+--------------------+-----+\n",
      "|bv2nCi5Qv5vroFiqK...|    0|\n",
      "|s2I_Ni76bjJNK9yG6...|    1|\n",
      "|AEx2SYEUJmTxVVB18...|    1|\n",
      "|VR6GpWIda3SfvPC-l...|    1|\n",
      "|CKC0-MOWMqoeWf6s-...|    1|\n",
      "+--------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n",
      "+--------------------+\n",
      "|                node|\n",
      "+--------------------+\n",
      "|qZcP5lo3KW5SvVXbe...|\n",
      "|QejrB5aH2IUmr4rXA...|\n",
      "|-bgszoDnhaUEuVydd...|\n",
      "|vvk0SrRa9no3CrLEL...|\n",
      "|hAiso8IQfKW88uWUG...|\n",
      "|wmayxaF9C8wAkEllD...|\n",
      "|dT1jqOZrFUmY4m4o3...|\n",
      "|GlEhxPlaofxOOvcf-...|\n",
      "|YVaONBBT9lSJQwBPW...|\n",
      "|bYcOiNUIKLOn2Aral...|\n",
      "+--------------------+\n",
      "only showing top 10 rows\n",
      "\n",
      "+--------------------+-----+\n",
      "|                node|depth|\n",
      "+--------------------+-----+\n",
      "|-bgszoDnhaUEuVydd...|    2|\n",
      "|qZfMILmXeiTGiZQXJ...|    2|\n",
      "|XYjB4jCthoDf1aeFG...|    2|\n",
      "|7eFy-LxZuvS2K6PAQ...|    2|\n",
      "|TOJJE4oWFTet2t8xU...|    2|\n",
      "+--------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n",
      "+--------------------+\n",
      "|                node|\n",
      "+--------------------+\n",
      "|N3J76CRP2H52NUo4V...|\n",
      "|4iY_gyKX2ogbem7ra...|\n",
      "|RtUvSWO_UZ8V3Wpj0...|\n",
      "|q1EeYFpt_E9tVo8_o...|\n",
      "|mI17MpWybqUw2_6WZ...|\n",
      "|JLbgvGM4FXh9zNP4O...|\n",
      "|0owIRP_z5RcYKKmh5...|\n",
      "|--9e1ONYQuAa-CB_R...|\n",
      "|Fmtg_NZ1Ib1eBgbtM...|\n",
      "|eKznX8VTfcQrjCqXp...|\n",
      "+--------------------+\n",
      "only showing top 10 rows\n",
      "\n",
      "+--------------------+-----+-----+\n",
      "|                name|count|stars|\n",
      "+--------------------+-----+-----+\n",
      "|Delmonico Steakhouse|    4|  4.0|\n",
      "|           Starbucks|    1|  3.5|\n",
      "|          McDonald's|    1|  2.0|\n",
      "|     Cabane Hot-Dogs|    1|  4.0|\n",
      "|Downtown Cocktail...|    1|  4.5|\n",
      "|             O'Bagel|    3|  4.0|\n",
      "|  Divine Chocolatier|    2|  4.5|\n",
      "|  Brault & Martineau|    1|  1.0|\n",
      "|          Parc Jarry|    3|  4.5|\n",
      "|           Starbucks|    1|  3.5|\n",
      "+--------------------+-----+-----+\n",
      "only showing top 10 rows\n",
      "\n",
      "done\n",
      "+-----------------+-----+-----+\n",
      "|             name|score|count|\n",
      "+-----------------+-----+-----+\n",
      "|       Schwartz's|  4.0|  112|\n",
      "|      La Banquise|  4.0|   77|\n",
      "|Olive & Gourmando|  4.5|   77|\n",
      "|             Kazu|  4.5|   51|\n",
      "|Au Pied de Cochon|  4.0|   48|\n",
      "|          Romados|  4.0|   47|\n",
      "|  Fairmount Bagel|  4.0|   45|\n",
      "|         Kem CoBa|  4.5|   43|\n",
      "|         Joe Beef|  4.0|   42|\n",
      "|         L'Avenue|  4.5|   42|\n",
      "+-----------------+-----+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "review_graph_sdf = review_graph_sdf.cache()\n",
    "yelp_business = yelp_business.cache()\n",
    "recommended_sdf = friend_rec(review_graph_sdf, 'bv2nCi5Qv5vroFiqKGopiw')\n",
    "recommended_sdf=recommended_sdf.cache()\n",
    "recommended_sdf.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "a51a87d81c3e57b2c303f4bd62d9a479",
     "grade": true,
     "grade_id": "cell-d67f9968d67a90ff",
     "locked": true,
     "points": 10,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CIS 545 Test Cases] (10 pts)\n"
     ]
    }
   ],
   "source": [
    "# [CIS 545 Test Cases] (10 pts)\n",
    "\n",
    "\n",
    "print('[CIS 545 Test Cases] (10 pts)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "3f393f64e7845b67d3485a1b98efe241",
     "grade": false,
     "grade_id": "cell-8af0db362938ecf8",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "dbe5a1fcb123cea9b9f9f4721520a50f",
     "grade": false,
     "grade_id": "cell-9aaabb69fcff8991",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### Step 2.2.3 Shortest Path\n",
    "\n",
    "Now create a function `shortest_path(from_user, to_user)` that takes in two arguemnts: `from_user` being the `user_id` of the start node and `to_user` being the `user_id` of the end node. Your function should modify `spark_bfs` to find the smallest number of restaurants between the two users.\n",
    "\n",
    "_Note_: We are looking for the number of restaurants between users, not the total number of BFS hops\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "086233845af532c785b26e51175f0167",
     "grade": false,
     "grade_id": "cell-6dd50dfa84b5daeb",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# TODO: Implement friend_rec using spark_bfs() \n",
    "from pyspark.sql.functions import array_contains                      \n",
    "def shortest_path(from_user, to_user, review_graph_sdf):\n",
    "    \n",
    "    origins  = [{'node': from_user}]\n",
    "    front_sdf = spark.createDataFrame(origins, schema)\n",
    "    visited_sdf = spark.createDataFrame(origins, schema).withColumn('depth',F.lit(0))\n",
    "    for depth in range(10):\n",
    "        #visited_sdf.show(10)\n",
    "        edge_sdf = review_graph_sdf.join(front_sdf, (front_sdf['node'] == review_graph_sdf['from_node']) | (front_sdf['node'] == review_graph_sdf['to_node']),'inner').cache()\n",
    "        front_sdf = edge_sdf.select(edge_sdf['to_node'].alias('node'))\n",
    "        from_node = edge_sdf.select(edge_sdf['from_node'].alias('node'))\n",
    "        front_sdf = front_sdf.union(from_node).drop_duplicates(['node'])\n",
    "        front_sdf = front_sdf.join(visited_sdf, visited_sdf['node'] == front_sdf['node'], 'leftanti').cache()\n",
    "\n",
    "        visited_sdf = visited_sdf.union(front_sdf.withColumn('depth',F.lit(depth+1))).drop_duplicates()\n",
    "        front_sdf = front_sdf.drop_duplicates(['node']).cache()\n",
    "        node_list = visited_sdf.select('node').distinct().collect()\n",
    "        nodes = [i.node for i in node_list]\n",
    "        if to_user in nodes:\n",
    "            return depth\n",
    "    return None\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "distance = shortest_path('bv2nCi5Qv5vroFiqKGopiw','-bgszoDnhaUEuVydd4CRPw',review_graph_sdf)\n",
    "print(distance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "bfa94c11b8e6ed7ab59d202c27b5d642",
     "grade": true,
     "grade_id": "cell-d05dd1f756220a61",
     "locked": true,
     "points": 5,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CIS 545 Test Cases] (5 pts)\n"
     ]
    }
   ],
   "source": [
    "# [CIS 545 Test Cases] (5 pts)\n",
    "\n",
    "print('[CIS 545 Test Cases] (5 pts)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "a878f78ec0703a4d08d2823d8afa0238",
     "grade": false,
     "grade_id": "cell-a1bc92be6337c324",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "# Step 3. Friend Visualization\n",
    "\n",
    "\n",
    "#### 3.3.1: Loading data subsets\n",
    "A closer look at the `yelp_user` dataframe tells us that there is an attribute called `friends` that we can use in order to construct an undirected friend graph.  For this part of the assignment we'll go back to Pandas -- not Spark -- DataFrames.\n",
    "\n",
    "We will work with the first 200 entries from the `yelp_user` data file and visualize these users' friends.\n",
    "\n",
    "Read the first 200 entries of the `yelp_user.csv` file into a Pandas dataframe called `user_200` (Remember: You can pass `nrows` as an option to the `pd.read_csv()`)\n",
    "\n",
    "We’ll subsequently make use of the `networkx` graph visualization tool, which lets us see what the graph actually looks like.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "7d7af828b67e1d2fd4cb8b957e128129",
     "grade": false,
     "grade_id": "cell-8d7ab1dea23b7983",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# TODO: read first 200 entries of yelp_user\n",
    "\n",
    "user_200 = pd.read_csv('/home/jovyan/work/hw2/data/yelp_user.csv', header = 0, nrows = 200)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>name</th>\n",
       "      <th>review_count</th>\n",
       "      <th>yelping_since</th>\n",
       "      <th>friends</th>\n",
       "      <th>useful</th>\n",
       "      <th>funny</th>\n",
       "      <th>cool</th>\n",
       "      <th>fans</th>\n",
       "      <th>elite</th>\n",
       "      <th>...</th>\n",
       "      <th>compliment_more</th>\n",
       "      <th>compliment_profile</th>\n",
       "      <th>compliment_cute</th>\n",
       "      <th>compliment_list</th>\n",
       "      <th>compliment_note</th>\n",
       "      <th>compliment_plain</th>\n",
       "      <th>compliment_cool</th>\n",
       "      <th>compliment_funny</th>\n",
       "      <th>compliment_writer</th>\n",
       "      <th>compliment_photos</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>JJ-aSuM4pCFPdkfoZ34q0Q</td>\n",
       "      <td>Chris</td>\n",
       "      <td>10</td>\n",
       "      <td>2013-09-24</td>\n",
       "      <td>0njfJmB-7n84DlIgUByCNw, rFn3Xe3RqHxRSxWOU19Gpg...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>uUzsFQn_6cXDh6rPNGbIFA</td>\n",
       "      <td>Tiffy</td>\n",
       "      <td>1</td>\n",
       "      <td>2017-03-02</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>mBneaEEH5EMyxaVyqS-72A</td>\n",
       "      <td>Mark</td>\n",
       "      <td>6</td>\n",
       "      <td>2015-03-13</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>W5mJGs-dcDWRGEhAzUYtoA</td>\n",
       "      <td>Evelyn</td>\n",
       "      <td>3</td>\n",
       "      <td>2016-09-08</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4E8--zUZO1Rr1IBK4_83fg</td>\n",
       "      <td>Lisa</td>\n",
       "      <td>11</td>\n",
       "      <td>2012-07-16</td>\n",
       "      <td>None</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Ob-2oGBQ7rwwYwUvhmnf7g</td>\n",
       "      <td>B</td>\n",
       "      <td>9</td>\n",
       "      <td>2012-05-01</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>JaTVvKsBl0bHHJEpESn4pQ</td>\n",
       "      <td>Peter</td>\n",
       "      <td>2</td>\n",
       "      <td>2013-03-23</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Ykj0DVsz0c6rX9ghjd0hDg</td>\n",
       "      <td>Colleen</td>\n",
       "      <td>1</td>\n",
       "      <td>2010-10-10</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>kmyEPfKnHQJdTceCdoyMQg</td>\n",
       "      <td>A</td>\n",
       "      <td>7</td>\n",
       "      <td>2012-10-16</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>H54pA7YHfjl8IjhHAfdXJA</td>\n",
       "      <td>Chad</td>\n",
       "      <td>3</td>\n",
       "      <td>2010-06-25</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  user_id     name  review_count yelping_since  \\\n",
       "0  JJ-aSuM4pCFPdkfoZ34q0Q    Chris            10    2013-09-24   \n",
       "1  uUzsFQn_6cXDh6rPNGbIFA    Tiffy             1    2017-03-02   \n",
       "2  mBneaEEH5EMyxaVyqS-72A     Mark             6    2015-03-13   \n",
       "3  W5mJGs-dcDWRGEhAzUYtoA   Evelyn             3    2016-09-08   \n",
       "4  4E8--zUZO1Rr1IBK4_83fg     Lisa            11    2012-07-16   \n",
       "5  Ob-2oGBQ7rwwYwUvhmnf7g        B             9    2012-05-01   \n",
       "6  JaTVvKsBl0bHHJEpESn4pQ    Peter             2    2013-03-23   \n",
       "7  Ykj0DVsz0c6rX9ghjd0hDg  Colleen             1    2010-10-10   \n",
       "8  kmyEPfKnHQJdTceCdoyMQg        A             7    2012-10-16   \n",
       "9  H54pA7YHfjl8IjhHAfdXJA     Chad             3    2010-06-25   \n",
       "\n",
       "                                             friends  useful  funny  cool  \\\n",
       "0  0njfJmB-7n84DlIgUByCNw, rFn3Xe3RqHxRSxWOU19Gpg...       0      0     0   \n",
       "1                                               None       0      0     0   \n",
       "2                                               None       0      0     0   \n",
       "3                                               None       0      0     0   \n",
       "4                                               None       4      0     0   \n",
       "5                                               None       0      0     0   \n",
       "6                                               None       0      0     0   \n",
       "7                                               None       0      0     0   \n",
       "8                                               None       0      0     0   \n",
       "9                                               None       0      0     0   \n",
       "\n",
       "   fans elite        ...          compliment_more  compliment_profile  \\\n",
       "0     0  None        ...                        0                   0   \n",
       "1     0  None        ...                        0                   0   \n",
       "2     0  None        ...                        0                   0   \n",
       "3     0  None        ...                        0                   0   \n",
       "4     0  None        ...                        0                   0   \n",
       "5     0  None        ...                        0                   0   \n",
       "6     0  None        ...                        0                   0   \n",
       "7     0  None        ...                        0                   0   \n",
       "8     0  None        ...                        0                   0   \n",
       "9     0  None        ...                        0                   0   \n",
       "\n",
       "   compliment_cute  compliment_list  compliment_note  compliment_plain  \\\n",
       "0                0                0                0                 0   \n",
       "1                0                0                0                 0   \n",
       "2                0                0                0                 0   \n",
       "3                0                0                0                 0   \n",
       "4                0                0                0                 0   \n",
       "5                0                0                0                 0   \n",
       "6                0                0                0                 0   \n",
       "7                0                0                0                 0   \n",
       "8                0                0                0                 0   \n",
       "9                0                0                0                 0   \n",
       "\n",
       "   compliment_cool  compliment_funny  compliment_writer  compliment_photos  \n",
       "0                0                 0                  0                  0  \n",
       "1                0                 0                  0                  0  \n",
       "2                0                 0                  0                  0  \n",
       "3                0                 0                  0                  0  \n",
       "4                0                 0                  1                  0  \n",
       "5                0                 0                  0                  0  \n",
       "6                0                 0                  0                  0  \n",
       "7                0                 0                  0                  0  \n",
       "8                0                 0                  0                  0  \n",
       "9                0                 0                  0                  0  \n",
       "\n",
       "[10 rows x 22 columns]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_200[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "8b2669f5f692fd3a861c6293fc0e43af",
     "grade": true,
     "grade_id": "cell-9fd4512fdb38a0e5",
     "locked": true,
     "points": 5,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CIS 545 Test Cases] (5 pts)\n"
     ]
    }
   ],
   "source": [
    "# [CIS 545 Test Cases] (5 pts)\n",
    "\n",
    "print('[CIS 545 Test Cases] (5 pts)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "0e0529751c508985ef4f5477bdfa2f16",
     "grade": false,
     "grade_id": "cell-a3e39045cff6784a",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "#### Step 3.3.2: Select users with at least one friend\n",
    "\n",
    "In this part, select the friends from `user_200` who have **at least one friend**. That is, rows in which the `friends` column does not have the value \"None\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "edc409c2ef26d69bc6a97e8128990010",
     "grade": false,
     "grade_id": "cell-403382c597db2839",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# TODO: find users with friends\n",
    "\n",
    "user_200 = user_200[user_200['friends']!='None']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "f609d9cc4399d3a4c742c05a55f8e492",
     "grade": true,
     "grade_id": "cell-75b46010adf9753f",
     "locked": true,
     "points": 5,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CIS 545 Test Cases] (5 pts)\n"
     ]
    }
   ],
   "source": [
    "# [CIS 545 Test Cases] (5 pts)\n",
    "\n",
    "print('[CIS 545 Test Cases] (5 pts)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "4f547934421c69b19f5fb4f34715b1ab",
     "grade": false,
     "grade_id": "cell-5f0307b0d9eb48e1",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "#### Step 3.3.3: Extracting friend as list\n",
    "The `friends` column is a string with comma-separated `user_id`s as values. We will make use of `lambda` functions to extract the different `user_id`s from this comma separated string, and apply this function to each row.\n",
    "\n",
    "To do this, use `df.apply()` which can take a function as a parameter.  As an example of its use, we can say\n",
    "\n",
    "`df['col_2'] = df['col_1'].apply(lambda x: x+10)`\n",
    "\n",
    "This will create a column 'col_2' in df with values of df['col_1'] + 10.\n",
    "\n",
    "For the next step, make use of `lambda` functions to `split` the value of the `friends` column by `,` and apply this to create a new column called `list_friends` which contains a list of user_ids of friends."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "3d9822fcc3f8d018adb7862a78cbe139",
     "grade": false,
     "grade_id": "cell-e7bf0a891c952120",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "# TODO: friend lists\n",
    "\n",
    "user_200['list_friends'] = user_200['friends'].apply(lambda x: x.split(','))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>list_friends</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Chris</td>\n",
       "      <td>[0njfJmB-7n84DlIgUByCNw,  rFn3Xe3RqHxRSxWOU19G...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Bobby</td>\n",
       "      <td>[jYiZnueCr7gVq9T34xoa7g,  yFLXGdY6rpHt7hRiwEFM...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Hugo</td>\n",
       "      <td>[hkXekeW_Jj6mIy8r8N7r1Q,  dQDpV-VUtwYGqHznuRV-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>Michelle</td>\n",
       "      <td>[HDb4fBWIAQ-foS8qLJty9w,  x0hBZsmBTYxhjjx0MShz...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>Kenny</td>\n",
       "      <td>[Cit5yho-DqotA0BnXHErTQ,  bm2DqfP4P454FjEtCbZd...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>Tisha</td>\n",
       "      <td>[PZbPhdy0_08tHprIJiZ4uw,  i4dhajw93ZDmIa89n6-w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>John</td>\n",
       "      <td>[MkXARyNby-scUaINNA6aCg,  ITD5f15-4cySvVyXvBwK...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134</th>\n",
       "      <td>Karla Mabelle Angel</td>\n",
       "      <td>[gFbLKOxGxHvWKdPALz6QQw,  bz-V2TKFjOKjB36E3lna...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135</th>\n",
       "      <td>Elise</td>\n",
       "      <td>[979dGx748hoDdqluJBp8rg,  kG35y4osns_nRp9znfZv...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144</th>\n",
       "      <td>Alison</td>\n",
       "      <td>[c3Vekx8APWg3zJiBA3IcNQ,  Ky5umve0w3f6zRECybTd...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158</th>\n",
       "      <td>Nina</td>\n",
       "      <td>[-KGB8XVQeFnd0HbNszrxgg,  kMilmJJSwJymx4axp5_H...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159</th>\n",
       "      <td>Portia</td>\n",
       "      <td>[Lxr5NU3wpgQoEwLSmobq8Q,  wYukCt4cgSdnvvk5Dp9e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>174</th>\n",
       "      <td>Dwiana</td>\n",
       "      <td>[XzzqUtjzFiRtumbX6-pN8w,  g3Segt1gteqr4l7QRyOY...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175</th>\n",
       "      <td>Tony</td>\n",
       "      <td>[WKIW7tWyMq7_XN0V2ouo0A,  yss1qD4e_7sPWUbuc_0S...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    name                                       list_friends\n",
       "0                  Chris  [0njfJmB-7n84DlIgUByCNw,  rFn3Xe3RqHxRSxWOU19G...\n",
       "17                 Bobby  [jYiZnueCr7gVq9T34xoa7g,  yFLXGdY6rpHt7hRiwEFM...\n",
       "18                  Hugo  [hkXekeW_Jj6mIy8r8N7r1Q,  dQDpV-VUtwYGqHznuRV-...\n",
       "44              Michelle  [HDb4fBWIAQ-foS8qLJty9w,  x0hBZsmBTYxhjjx0MShz...\n",
       "74                 Kenny  [Cit5yho-DqotA0BnXHErTQ,  bm2DqfP4P454FjEtCbZd...\n",
       "90                 Tisha  [PZbPhdy0_08tHprIJiZ4uw,  i4dhajw93ZDmIa89n6-w...\n",
       "91                  John  [MkXARyNby-scUaINNA6aCg,  ITD5f15-4cySvVyXvBwK...\n",
       "134  Karla Mabelle Angel  [gFbLKOxGxHvWKdPALz6QQw,  bz-V2TKFjOKjB36E3lna...\n",
       "135                Elise  [979dGx748hoDdqluJBp8rg,  kG35y4osns_nRp9znfZv...\n",
       "144               Alison  [c3Vekx8APWg3zJiBA3IcNQ,  Ky5umve0w3f6zRECybTd...\n",
       "158                 Nina  [-KGB8XVQeFnd0HbNszrxgg,  kMilmJJSwJymx4axp5_H...\n",
       "159               Portia  [Lxr5NU3wpgQoEwLSmobq8Q,  wYukCt4cgSdnvvk5Dp9e...\n",
       "174               Dwiana  [XzzqUtjzFiRtumbX6-pN8w,  g3Segt1gteqr4l7QRyOY...\n",
       "175                 Tony  [WKIW7tWyMq7_XN0V2ouo0A,  yss1qD4e_7sPWUbuc_0S..."
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_200[['name','list_friends']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "73ffc9e2c33a232e81512378441c19ea",
     "grade": true,
     "grade_id": "cell-924eac91ebd6db5a",
     "locked": true,
     "points": 5,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CIS 545 Test Cases] (5 pts)\n"
     ]
    }
   ],
   "source": [
    "# [CIS 545 Test Cases] (5 pts)\n",
    "\n",
    "print('[CIS 545 Test Cases] (5 pts)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "f4fdc5d08821538c21cb9628564103e5",
     "grade": false,
     "grade_id": "cell-32c319ed5a364cce",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "#### Step 3.3.4: Obtaining Friend lists\n",
    "\n",
    "We now want to create a dataframe called `subset_users` of edges constructed from \n",
    "the `user_id` and `list_friends` columns. Start by extracting these columns from `user_200`.\n",
    "\n",
    "Then use the `.stack()` option to \"unnest\" the items in the list of friends, associating them with the corresponding value of the index.  \n",
    "\n",
    "First, use `set_index()` on `user_id` so the user ID is the index.  Then if we `.apply()` the `stack()` operation on the `list_friends` column we will get each element in the list associated with the appropriate `user_id`:\n",
    "\n",
    "```\n",
    "result_df = df.set_index(['col_1'])['col_2'].apply(pd.Series).stack()\n",
    "```\n",
    "\n",
    "Finally, performing `df.reset_index()` on `result_df` will give us `friend_data` in an edge table.  Rename the columns of `friend_data` to 'source', 'level_1', and 'target'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "0857a5a8bc8ec103db1e52a7efea6eee",
     "grade": false,
     "grade_id": "cell-b8b6b26cc7ff6140",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# TODO: projection to users and lists of friends, stacked\n",
    "\n",
    "subset_users = user_200.set_index(['user_id'])['list_friends'].apply(pd.Series).stack()\n",
    "friend_data = subset_users.reset_index()\n",
    "friend_data.columns = ['source','level_1','target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source</th>\n",
       "      <th>level_1</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>JJ-aSuM4pCFPdkfoZ34q0Q</td>\n",
       "      <td>0</td>\n",
       "      <td>0njfJmB-7n84DlIgUByCNw</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>JJ-aSuM4pCFPdkfoZ34q0Q</td>\n",
       "      <td>1</td>\n",
       "      <td>rFn3Xe3RqHxRSxWOU19Gpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>JJ-aSuM4pCFPdkfoZ34q0Q</td>\n",
       "      <td>2</td>\n",
       "      <td>HVUAmApa0fCbHHVJ0ALshw</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>JJ-aSuM4pCFPdkfoZ34q0Q</td>\n",
       "      <td>3</td>\n",
       "      <td>LBOTb6bJjCdFyDLNswUGmA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>JJ-aSuM4pCFPdkfoZ34q0Q</td>\n",
       "      <td>4</td>\n",
       "      <td>cy3d0moQOsrhWo6VAyA_kA</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   source  level_1                   target\n",
       "0  JJ-aSuM4pCFPdkfoZ34q0Q        0   0njfJmB-7n84DlIgUByCNw\n",
       "1  JJ-aSuM4pCFPdkfoZ34q0Q        1   rFn3Xe3RqHxRSxWOU19Gpg\n",
       "2  JJ-aSuM4pCFPdkfoZ34q0Q        2   HVUAmApa0fCbHHVJ0ALshw\n",
       "3  JJ-aSuM4pCFPdkfoZ34q0Q        3   LBOTb6bJjCdFyDLNswUGmA\n",
       "4  JJ-aSuM4pCFPdkfoZ34q0Q        4   cy3d0moQOsrhWo6VAyA_kA"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "friend_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "674c44de30f4b94e0fdd8a32c0a732d7",
     "grade": true,
     "grade_id": "cell-9e479267e9510f18",
     "locked": true,
     "points": 5,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CIS 545 Test Cases] (5 pts)\n"
     ]
    }
   ],
   "source": [
    "# [CIS 545 Test Cases] (5 pts)\n",
    "\n",
    "print('[CIS 545 Test Cases] (5 pts)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "97c22fe086e9011236d5fb5f1088d1f8",
     "grade": false,
     "grade_id": "cell-633f34432d775d4e",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "#### Step 4.3.5: Visualization using Networkx\n",
    "\n",
    "In this step we will use the `networkx` library to visualize.\n",
    "\n",
    "\n",
    "You can create the graph from the networkx function `from_pandas_edgelist` and the `friend_data`. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "8e6fd309722195a4ed532b47d2cedcfc",
     "grade": false,
     "grade_id": "cell-c93af9717022b7eb",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# TODO: networkx graph ready to draw\n",
    "# Worth 3 points\n",
    "\n",
    "import networkx as nx\n",
    "graph = nx.from_pandas_edgelist(friend_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAecAAAFCCAYAAADL3BUJAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4zLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvIxREBQAAIABJREFUeJzt3Xt4TVfeB/DvuZCcMAgSSVtSWioV2kEYY4YqYkqrQ9VbnQpa2ipVfZsK+gz6zrQi9EYkHWpGdExpp62pGW3oTS8qdqaXELTmfdFRiX1Ki8rFJef9Y00qIZJz3Wvtvb+f58lTue31U8n57rX2ujh8Pp8PREREpAyn7AKIiIioLoYzERGRYhjOREREimE4ExERKYbhTEREpBiGMxERkWIYzkRERIphOBMRESmG4UxERKQYhjMREZFiGM5ERESKYTgTEREphuFMRESkGIYzERGRYhjOREREimE4ExERKYbhTEREpBi37AKIiMJG14GcHGDjRqCsDCgvB1wuoG1boEcPoE8fYNIkIC5OdqVEDXL4fD6f7CKIiIKm68DDDwOvvAKcOePf98TFiRAfOzaytREFieFMROZUUACMHw94vcFfw+MB8vKACRPCVxdRGDCcichcNA245RbgyJHwXbNPH6CwMHzXIwoRw5mIzGPCBGDNmshcu3Vr4OjRyFybKEAMZyIyh+RkYO/eyLbRrBnwww+RbYPID1xKRUTqMyKYAeDUKaBNm8i3Q9QIhjMRqW3CBGOCucaxY8DPfmZce0T1YDgTkbo0LXLPmBtSWCinXaL/4DNnIlJXnz4ioGWIixNrqIkkYDgTkZp0HWjXTm4Nu3eL591EBuOwNhGpKSdHdgVARobsCsimGM5EpKaNG2VXAOzaJbsCsimGMxGpKZRtOcPl2DHZFZBNMZyJiIgUw3AmIjXFx8uuQByMQSQBw7khug5kZwNjxgCdOolfVLdbnA/rcgFRUWI2ac+ewPz5agzDEVnFzTfLrgDo1092BWRTXEp1IV0H5swB1q8XW/kFwuEQv8zPPgukpkamPiK70HUgIQGQ+RKVnQ08+qi89sm2GM41NA2YORPYti0813vwQWDp0vBci8iuBg0C3n9fTttuN3D4sNiMhMhgHNYGgMxMoG/f8AUzACxbJp6ZydrdiMgKsrPFIyQZRo5kMJM0DOdx48QLQCQGELxeEfp5eeG/NpEdpKaKG12jeTzi8RaRJPYO58xMYN26yLbh8wHTpjGgiYI1dSowa5Zx7TkcwFNPAb17G9cm0QXs+8xZ00Sv1qi/vssFbN/OX3iiYF1zDfDVV5FvJzMTyMqKfDtEDbBvz/mOO4ydBXrunLF3/0RW06VLZK/vcDCYSRn2DOeCAuD//s/4dt97j2uhiYLVsmXkrt2kCbB8OYOZlGHPcJ44UV7bc+fKa5vIzHr0AKKjw3/d668XKzWmTg3/tYmCZL9nzrLPiG3ZEvj+e3ntE5mVrgNJSUBlZXiu53YDTzzBx02kJPv1nBctktv+8eMc2iYKRnw8cNNN4tlwsBwO0fsePRr45BMGMynLfuGcny+7AmD1atkVEJnTnDnBH0bhcgHTpwNffw28+ipXTpDS7BfOR4/KrgDYuVN2BUTmlJoKLFkCxMQE9n0xMWIzk6VLuesXmYJbdgG2VFwsuwIi86qZuJWRAVRUNLwk0uEQPe0lSzjhi0zFfj1nFXzxBXcMIwrF1KnA1q3AqFHiGfKFQ90ej/j4qFHi6xjMZDL2m63tcgHV1bKrEC8eTz3FFw2iUHm9Yh7Hzp3Ad98BsbFA9+5iySSHsMmk7BfOHTsCBw7IrkKIiRF39ZyYQkREtdhvWFuF2do1KiqAhQtlV0FERIqxX88ZUGdoGxDPxb7+msNvRET0I/v1nAFg5kzZFZzncHDdMxER1WHPcH7qKaB5c9lVCBUVXPdMRER12DOcAeDdd2VXcN5338mugIiIFGLfcE5NBXJzZVchxMbKroCIzETXgexsYMwYoFMnsfLD7QacTvHmcIi5NS7X+Y9FRYnXmsGDgT17ZP8NqBH23iGsZo3xjBnA2bNyavB4xJpMIqLGaBrwyCPAhx82/rUXTno9fVq8vfsucO21IsBvuQV44QVOSFWQPWdrX6ioSJxO8957xrfN2dpE1BhdBwYMAL78MjLX//nPgWefFSOKpAT7DmvX1ru3uJvUdfFDahSHAxg+nMFMRPXTNGDECHEGfaSCGQC2bQP69AEyMyPXBgWEPef6FBUBCxYAb70FnDsXuXbMsEOYrgM5OcDf/w6UlgLl5eKmwu0W9TdrBnTrJn6xJ03ijQZRuGRmigM7jN6ToU8foLDQ2DbpIgznhtTs2atpwK5dQGWlCCWPR/y3SRPg8GHg0KGGT8apT0yMuifllJQAU6aIm5QzZwL//iuuAO6+W5ydy7AmCty4ccC6dfLaT0wUr20kDcM5HLxe4L77gDfeaLynreoRdiUlwOjRwFdfhfe6rVoBf/gDMHZseK9LZFWZmWImtmzt2gFlZbKrsC2GczgVFYm9sjdtEiFcUXH+cx6P6F0PHw7MmaPOUHZ+PjBtGnDqVGTbcbuBp58GHnwwsu0QmZmmiWFlVfTtC2zfLrsKW2I4R4JZjrDr2RP47DNj22zZEtiyhbNCierTqhVw/LjsKurKzwfS02VXYTsMZzvSNKB//+CeJ4fLrFnAokXy2idSzbJlYs8F1cTFiYmhZCiGs92o8jwLAIYOBTZvll0FkRqaNROrIVS0ezeQnCy7ClvhOmc7GTdOnWAGxPD2sGGyqyCSr6RE3WAGgIwM2RXYDsPZLjIz5S7NuJTNm4HZs2VXQSTXo4/KrqBhRs9NIQ5r24KmiVmXKv9Ta5o6M9iJjNaxI3DggOwqLs3pjOyGTHQR9pztYOZMtYMZAIYMkV0BkTwqD2kDxu9SRgxny9N1sW+u6o4fB5Yvl10FkRwxMbIrIMUwnK1uyhTZFfjvv/9bdgVEcnAmNF2Az5ytzuGQXUFguGSD7KikBEhJkV1FwxgVhmLP2cpKSmRXELj775ddAZHxunUz3400RRTD2crMGHQ7dsiugEiOtDTZFVyayyW7AtvhsLaVRUcDVVWyqwgcfyTJjnRdnASlIp5QZTj2nK3MjMFMZFfx8eKAHBXddZfsCmyHPWcrM+szLP5Ikl2pdmRkDV1X60Q9G2DP2cqcJvznbdpUdgVE8qSmihPbVNKrF4NZAhO+epPf2rSRXUHgrrpKdgVEci1aBNxxB5QYP3I4gOefl12FLTGcrWz8eNkVBO7222VXQCSVruu4w+fD8y1byg/o5cu5570kDGcry8yUXUHgpk+XXQGRFD6fD/n5+ejevTuSkpIw4fBhODQN6N9fTkG5ucDUqXLaJk4Is7z4eMDrlV2Ff/r0AQoLZVdBZLj9+/fj/vvvh67rWLVqFXr27Fn3C7xe4PrrgcOHI19MVBTw0UfsMUvGnrPVvfii7Ar8x4MvrKGkBBg+HLj8cqBJEzEx0eGo++Z0As2bAz16APPnm+cGMszOnTuHZ555BqmpqbjxxhuxY8eOi4MZEBOyvvkm8huV9OwJVFYymBXAnrMdJCQAR47IrqJh6elAfr7sKigYJSXAPfeIZUChHC3YtKmYGbxqlS32V9+5cycmT54Mj8eDlStXonPnzv594+zZwOLF4T/GMTMTyMoK7zUpaOw528HGjbIraNiVVzKYzaakBLj2WtELTkkRjyNCDYvTp4FPPhHXjY+37M9EVVUVfvvb3+LGG2/E5MmT8e677/ofzIAI0MJCYMSI8CyX7N9f3FgxmJXCnrNdpKerOcTdujVw9KjsKshf+fnAzJnA998b057F5iF8/PHHmDx5MpKTk5GTk4PLLrsstAt6vcDq1SJcNU2MkFVUNP59zZsDd9wBPPkk1zAriuFsJ8nJwN69sqs4LykJOHBAdhXkr7595RxM8pOfACdOGN9uGJ04cQJz5szBhg0bsHTpUtx2222Ra6x2YO/aJZ4hR0eLEY7UVGDiRAayCTCc7UaVgOYzZnORPW+hSRMx7G1C//jHPzB16lSkpaVh8eLFiI2NlV0SmQDD2Y4mTADWrJHTtscDfPABZ4OaSdu2ajx6iIoSvUCT0HUdDz30EDRNw4oVK3DjjTfKLolMhBPC7Cg/Xwx5tWhhbLtjxwLl5QxmM0lMVCOYAXHKWqtWsqtolM/nw4svvoju3bujffv2KC4uZjBTwNhztrsZM4BlyyLbRp8+3AbQjFR5BHKhxERjNuMIwoEDB3D//fejrKwMq1atQq9evWSXRCbFnrPdLV0qetHh3iLQ6QRGjhRHzRUWMpjNZsIENYMZAEpLlVsHfe7cOTz33HPo3bs3brjhBmiaxmCmkLDnTOd5vaKHu2oVqg8dggNAQCdCezwi5JcuVe7FkwKg6pnCF1JkUuGuXbswefJkREVFYeXKlejSpYvsksgC2HOm8+LigAUL8OXbbyMhLg6nNA0YMkQ853O7z2/D6HIBzZoBHTqIbRp37wZ8PvE8ecsWBrPZTZgguwL/rFkDFBVJa76qqgrz58/HoEGDcPfdd+O9995jMFPYsOdMF5k0aRI6duyIefPmyS6FjKbrQLt2sqvw35Ah4obQYNu2bcPkyZPRpUsXLF++HJdffrnhNZC1MZypjgMHDqBXr17417/+xfWYdvSb3wB/+YvsKgKj64ZtqnHy5EnMnTsXr7766o+biTgcAT38IfILh7WpjuzsbEyZMoXBbFd//avsCgK3erUhzWzatAkpKSkoLy9HSUkJxowZw2CmiHHLLoDUUVpainXr1mGvqrN0KbJ03Zy7cGmaf1+n6yLIi4uB48eBli3FkZWTJjXY8/Z6vZg5cyYKCwvxxz/+EYMHDw5P3UQNYDjTj5566imMHz8e8fHxskshGQzqgYbd5583/HlNAxYuBN58U7xfe5ex114T50nfdBMwZ47Ye/o/fD4f1q5di4yMDIwfPx7FxcWIiYmJwF+A6GJ85kwAgKNHj6Jz584oLi7GFVdcIbsckuGuu4C1a2VXEbiGTjbLywMyMsRJTQ291DkcYingkiXA1Kk4ePAg7r//fpSWlnIzEZKCz5wJAPDcc8/htttuYzDb2fHjsisIztmz9X+8JpjLyxsOZuDHpYC+jAy8O3YsevXqhQEDBnAzEZKGPWfCiRMn0KlTJ2zfvh1XX3217HJIFrP2nOvbzlPTgBtuEMEcoEqnE0defhlJkTzWkagR7DkTcnNzMWzYMAaz3fXoIbuC4CQmXvyxhQvFUHYQonw+JJltORlZDnvONldeXo5OnTrh7bffRkpKiuxySCZdF+c2m+0lYd484PHHz7+v60BSUmjHS0ZHA19/bdj6aaILsedscytXrkS/fv0YzATExwODBsmuInDTp9d9Pxyzzh0O885eJ0vgUiobq6qqwuLFi7FhwwbZpZAqsrLMcehFjZYtL+7dFheH1msGxJD4zp2hXYMoBOw529iaNWuQkpKC3jzOkWqkpgLjx8uuwn8rV178sXDNOv/uu/BchygIDGebOnv2LLKysvDYY4/JLoVUs2YNcNllsqtonMsF3H77xR9v2TI81+cWtiQRw9mm1q9fj8svvxy//OUvZZdCKvrmG9kVNO7ZZ+v/eI8eYkJXKDweoHv30K5BFALO1rah6upqdO/eHU8//TSGDRsmuxxSVX4+MHGi7CrqFxsLHDtW/+d0Hb4OHeCoqgr++pytTZKx52xDf/vb3+DxeJCWlia7FFLZhAnA0KGyq6jf5s2X/FTh/v14p2lTVAd7bYcDGD6cwUxSMZxtxufz4YknnsBjjz3G4+6ocZs3A127yq6irtxcoJ5JjCdPnsSMGTPw61//GudmzYIj2EMqPB5xCAaRRFxKZVWXOB7vvaQkVFZW4tZbb5VdIZnFnj3Az34GFBbKrcPpBHJygKlTL/rUxo0bMW3aNAwdOhQlJSVo3bo10KbN+b21/RUTIw6/4AoGkozPnK2moePxPB5UVVbiSM+e6JCXV+d4PKJGzZ4NZGfL2UGsVy/g+ecvCs3S0lI89NBD+Oyzz7BixQoMunATlSBPpSKSjcPaVpKXJzb737BBhPKFGzFUVCDK50P7Tz8VX5eXJ6NKMqusLGDHDmDECNGLNcJNN4lRoKKiOsFcXV2NlStX4rrrrvvxqNOLghkQQbt1KzBqlJjk5fHU/bzHIz4+apT4OgYzKYI9Z6uofTyev2qG8PiCRIHyesVjk4IC4MMPgdOnw3ftqCjgxRfrX8MMYO/evbjvvvtQWVmJlStXooe/B3bU1Lxzp9hgJDZWLJeaOJGTv0g5DGcrCOF4PMTEiB4Dn7FRKLxeYNEi4E9/uvQSp8ZcfbWY7HWJGeKnT5/GokWL8Nxzz2H+/Pl44IEH4HK5QiiaSF0MZysYPVoMZQfzT+lwiCG9V18Nf11kX14vsHw58NprwIED4saxulrs6uVyieHkuDjg+uvF3IdGeq/btm3DlClT0KlTJ+Tm5qJ9+/aG/VWIZGA4mx2PxyMLO378OObOnYvXX38dzz33HMaMGcMlgGQLnBBmdjwejyxqw4YNSElJwZkzZ1BSUoLbb7+dwUy2wXXOZsfj8chiDh8+jOnTp6OkpAR//vOfMXDgQNklERmOPWez4/F4ZBHV1dV4/vnncd111yElJQVffPEFg5lsiz1ns+PxeGQBu3fvxr333ovq6mq8//776Natm+ySyGx0Xewgt3EjUFYmJiG6XEDbtuKksj59gEmTTDO3hj1ns+PxeGRiVVVVWLBgAQYOHIg777wTH330EYOZGqfrYre6tDTx+udwAO3aAb/7HfD55yKcT5wQI4L79onVKJmZQHy8+FqnE2jRAhgyRGxPqyDO1jY7ztYmk/rwww9x77334pprrkFOTg6uuOIK2SWR6jRNbCP77rvhva7TCdx8M/DCC8q8DrLnbHbx8WKLw2BnsfJ4PDLY999/j/vuuw/jxo3DE088gQ0bNjCYqXEjR4qh6XAHMyDW4L/xhng9HTRI3ARIxp6zFXCHMApF7RPMdB04fFgMB549K0ZkHA7A7RY/K82bA9deG9TzO5/Ph9deew0zZszAyJEjkZWVhZbhmjNB1pWfD9xzD3DunHFtulzAsmVStzZmOFsF99amQOg6MHcu8PLLwMmTwV3D5QKGDQMWLGj0hLNDhw5h2rRp2LdvH1asWIFf/OIXwbVJ9jJuHLBunbz2Z80S29JKwGFtq5g6FViyBGebNkWj95cOB4PZrjQN6N9fTJ5ZtSr4YAZET2bTJtGL7t9fbNl50ZecQ05ODn7605+iZ8+e+OyzzxjM5B/ZwQyISWezZ0tpmj1nC6mqqsLtHTti1dVXI07TRAhXVJz/Ao9H7L89fDgwZw6Hsu0mM1O82ETSz38OPPsskJqKXbt2YcqUKXC73VixYgWSk5Mj2zZZR36+2G9dFbm5hndkGM4WsmzZMrz11lv4xz/+wePxqK60NGDLFkOa8gH4oF8/jNm3D7///e8xZcoUOI06/5msoXlz4NQp2VXUpWmGdmgYzhZx8uRJdO7cGQUFBbjuuutkl0MqMTCYa/gAVA0YgOitWw1tlyxg3ToxpK0atxs4c8aw5ng7axHPPPMMBg8ezGCmujIzDQ9mAHAAiP7gAzFhjCgQqs6DOXuWPWcKjNfrRdeuXaFpGjp16iS7HFKFponJWrJlZgJZWbKrIDPQdTFZUWX5+UB6esSbYThbwMMPP4wzZ84gJydHdimkkvj4emdQG87hAHbs4AREaty8eWILTpXFxgLHjkW8GYazyR08eBA9e/bE7t270U71O04yTkEB8Ktfya7ivP79gY8+kl0FqS45Gdi7V3YVjdu9W9QaQQxnk5s4cSLat2+P36l+t0nG6tgROHBAdhXnORzAkSNcKUANi44GqqpkV9G4IUMiPpeDE8JMbNeuXXjzzTeRkZEhuxRSia6rFcyAWF+/fLnsKkh1Bs6GDklRUcSbYDib2GOPPYbMzEzuT0x1rV4tu4L6bdwouwJSnVkGcn/4IeJNuCPeAkXExx9/jM8//xzr16+XXQqpprhYdgX1O3JEdgVE4WHATQR7zibk8/kwe/ZsLFiwANHR0bLLIdUcPy67AqLgNGkiuwL/uCPfr2U4m9CmTZtw7NgxpBuw1o5MSNXHHFxNQI1R9Wf3QgbsJ8FwNpnq6mrMmTMHTzzxBFwul+xySEU9esiuoH433yy7AlLd5ZfLrsA/Y8dGvAmGs8m89NJLaNasGW699VbZpZCqVDrNp7bp02VXQKq75RbZFfhn2rSIN8F1ziZy+vRpdO3aFX/6058wcOBA2eWQylTZHayGxwOUl8uuglRnhu0727cHvv464s2w52wiK1asQNeuXRnM1LilS2VXUNdvfiO7AjKD+HggKUl2FQ1btcqQZthzNokffvgBnTt3xptvvonrr79edjlkBjExQEWF7CoEXefuYOQf1baera1DB+DgQUOaYs85knQdeOQRMckhKkpsYejv209+AgweDOzZA0AcCXnjjTcymMl/eXmyKxAGDWIwk/+GDRPbz6rG4QBefdW45thzDrOSEuCuu4AvvgjbQnUfgN1OJ9plZKBtRgZf6Mh/ffuKE6FkcbuBTz7hiVQUGFWOO60tN9fQs6bZcw6XZctE7zglBfj887DuIOMA0K26Gm2zs8VkiREjxA8vUWMKC+VOsFm6lMFMgUtNBWbNkl3FeQYHM8Cec+gKCoBbbzX+JBWHQyxNUW3iD6lJxilVmZlAVpaxbZK1pKVF/PSnRuXnAxI2fGLPOViaBiQmiokLMo448/lEb71tW/aiqXH79xv7AsNgpnDYvFnu8HZurpRgBhjOwUlPFz8wZWWyKwGOHhW1ZGbKroRUl58vbuSioiLXhtMpXtAYzBQuhYWiB200CUPZtXFYO1C9ewP//KfsKurXp4/4QSZqzH/9F/Dyy+G95qBBQHY2nzFTZMyeDSxaFPl2OnQQs7Il/xyz5xyItDR1gxkQs3KTk2VXQWawfr3oRQ8ZEtp13G4x50LXgXfflf6CRhaWlSV+Zvv3j8z1r7hCDKMfPKjEzzF7zv7Kz1d3z+ILpaWJiWpE/vB6gdWrgQ8/FCMvp06dn0dx7pz4r88nJiG63WL2d48ewJIlvBkkObxeYO5c4C9/CW5bWKdTbCl71VXAr38tJtcqtkSV4ewvh0N2BYGRNMOQiMhQXi+QkwO8/rqY+FhZef6mEhA3lC1aiB73L34hOlmKBXF9GM7+uOwyoLRUdhWBiYkRPSAiIjIdPnNuzPz55gtmQAz1vPKK7CqIiCgI7Dk3pmlT4MwZ2VUE5yc/AU6ckF1FeOn6+SGsr78Wz0bPngWqq8VzUadTPIKIiRHPRq+7TsxinzTJFENZREQAw7lhH3wAmP14RqucBlRQANx3X2gnwgweDCxcKLYGJCJSGIe1G3LPPbIrCF12tuwKQqNpQJcuYie2UI9qe+cd0YseOzY8tRERRQh7zg1p0kQMmZpZ69ZiFzEzyssDpk0L6yEiP2rRAnj7bfaiiUhJ7Dk3xOzBDADHjsmuIDh5ecADD0QmmAHxLJ7bnhKRothzbojZ1jZfitmeO2uaOIfYqB/NO+4AXnrJmLaIiPzAnrMdrF4tu4LAjBhhXDADwLp1Yt9eIiJFMJztYOdO2RX4b9kyseOP0RYtAoqKjG+XiKgeHNZuiEWGtbe2aIGF/fohMTERCQkJSExMrPPnhIQENG/eXHaZQkwMUFEhp+3+/YGPPvLva3VdjEgUFwPHjwMtW4r9prmemojCgOHcEIuE89Hhw1E4bRpKS0tRVlaG0tLSi/7sdrsvCuwLQzwxMRFt2rSB0xmhAZeSEiAlJTLX9ldjz+c1TayVfvNN8X5l5fnPeTxiOP6mm4A5czgTnIiCxnBuiBXC2eMBHn8cePTRS36Jz+fDiRMn6gR2fSFeVlaGEydOID4+/pI98JqPtWvXDtHR0YHVmpoqf2h53jzx/6s+eXlARobo2Tf0a+NwiP/vS5ZIPaydiMyL4dyQ+Hg5zz/DKTpabHMZpqHWqqoqHDlypNEQLysrQ/PmzRsN8YSEBLRq1QoOh0ONdeU9egBffHHxx2uCOZDj6WJiGNBEFBSGc0MeeQR4+mnZVYRm9Gjg1VcNb7a6uhrHjh27ZHjX/lhVVRW6xcWh6NAhSB+raNFCPEOuTdOAG24I7tzYmBhg61YlDm8nIvNgODdE18XhCWblcgHbtysfDOXl5aj4n/9B60WL5IdzdPTFE9JGjwY2bAhueZfDAYwaJeUGiepRM5Fvxw7g00/FiXNnztQ9/9fhEAeoNGsmfv979ODhKWQ4hnNjrr0W2LNHdhXBWbQImDVLdhX+uesuYO1a2VUAUVF1J3npOpCUVPdjgQrzowUKgqYBCxYAb70lTjALhsMhDk958klO9qOI4zrnxuTny64gOImJ5glm4OKhZElOVVdj3rx5yM3Nxeuvv44Djz+O6lDvXx0O820EYyXTp4ue76ZNwQczIEZO3n6bh6eQIRjOjUlNBcaPl11F4N54Q3YFgWnZUnYFAIDTsbFwOp0oLi7G6tWrsWf9ejirqkK7aEWFuTaCsYqCAqB5c2D58vBf+5VXxLC3poX/2kTgsLb/kpOBvXtlV+Gf9HTz9fizs9U4hOL224GXXz7//i23AH//e8iX/eqaa/BBRsaPs9QTExMRFxcHt9sd8rXpApoGTJlS/6z7SDDj7xspj68M/tqzB7jsMjGBRGVJSeZ8oZg4UWzcEcqwY6gcjoufJYapR3+qSRNs27btx1nqpaWlOHbsGNq0aVPvZi8Xvu/xeMJSh+Xl5QEzZhi7JG/NGrGBjuw1+mQp7DkH6sorgYMHZVdRv3btgLIy2VUEb8gQ4J135LXftClw6FDdiVvZ2cD8+aFNCLvERjBnz56Fruv17th24Z+joqIuGdy134+NjRVrxu0oLw948MG6M6+NlJYmhtKJwoDhHIxhw4DNm2VXUVffvmLZlJlpmphsI0t9a8IVmK3t8/nw/fff1+l11w7v2u9XVlYiISGhwV54YmIi4uPjrTWkrmlAv37ygrlGfr4Y5iYKEcM5WGvWiLv0Eyfk1uFyAX/8o3VeEMaOFZNtjNbQZiGJ8sm+AAAOMklEQVQmWudcXl6OI0eONBrk3377LVq3bu3XkHpMTIwhtYekSxdg3z7ZVQBut1g3TRQihnOoXnkFmDBBzklK99wDvPCC8e1GWvPmwKlTxrUXFQU888ylt9m04A5hZ8+ehdfrbbAXXvPWtGlTv4bUW7duLWdIvaAA+NWvjG/3UhYu5PngFDKGc7hs2QKMGwccPRr5tkaMEBsqKPaCHzZGDm+7XOIM6cb2v7bp3to+nw/Hjx9vdDi9tLQU5eXlfg+pN2nSJHxFtmkDHDsWvuuFyumUP7xOpsdwDjevVwTn6tXB9bTq43CIZ8qjR4tZzXbYaSovD3jggci2ERcnNqbw9yaHp1I1qKKios6Q+qWC3Ov1IjY21q8h9WbNmjXcqGq95hq7d4vll0RBYjhHmtcrttF88UUxucgfTqcIjvHjxS5fdgjj+uTlid2dIrG8auBA4P33A/++oiIxbLlpkwjh2o8zas5zHj5cLAuz6shGiM6dOwev19tgL7zmz263u8Hw/uW99yLqwAH5e7JfaMAA8TiDKEgMZxm8XtGz3rkT+O47IDYW6N7dPr3iQBQVAXfeGb7JPg6HuOHJygrtOvw3jLj6zhmvHd4VBw/i5Q8/VHObwwv3aCcKEMOZzGHLFrG5xJdfBjdr2uUSw59WflZvN6rsKncpfGmlEFhooSNZ2tChYpe2mh6rpontGb/9Fjh9WnxN06ZAq1Ziz+MmTcREoYQE9mitqrhYdgVEEcNwJnOJi7topy2yKUVOMiOKBCUf1xARNUqRk8zq5eRLK4WGP0FEZE49eqgbgm3byq6ATE7Rn2wiokZMnKhuOJvxDHhSCmdrE5F5DR8OvPmm7CoupuucgEghYTgTkXlpmtg9T6WXsfh44MgR2VWQySk6JkRE5IfUVPVm7//5z7IrIAtgz5mIzK9vX2DHDtlVAB06AAcPyq6CLIA9ZyIyv8JC4MorZVdh2LndZH0MZyKyhv37gcREee3n5nJrWAobhjMRWcfhw0BSkvHt5uba6nhQijyGMxFZy4EDQFqaMW116CBmjDOYKcwYzkRkPQUFQH6+OLoxEpo2BTZvFpO/OJRNEcBwJiJrSk8XZyo/+aQI03CIjgZycoCqKnFSGlGEcCkVEdnDnj3A5MnAp5+K0PaXwwEkJwN//av4L5EBGM5EZD+1zwXftQs4cQL47jvxOacTaN0aSEkBlixhIJMUDGciIiLF8JkzERGRYhjOREREimE4ExERKYbhTEREpBiGMxERkWIYzkRERIphOBMRESmG4UxERKQYhjMREZFiGM5ERESKYTgTEREphuFMRESkGLfsAoiUoOvinN6//x345hvg+HHg3DnxVsPpBNxuICYGaN5cvF17LdCnDzBpEhAXJ69+IrIUnkpF9qZpwMyZwCefAKH8KjgcQJcuwNKlQFpa+OojIltiOJN9ZWYCixeHFsr1ad0aeOklhjQRBY3hTPaUlgZs2RLZNjp3BtauBVJTI9sOEVkOJ4SR/RgRzACwbx/Qty+Qlxf5tojIUhjOZC+ZmcYEcw2fD3jgAQY0EQWEw9pkH5omZlbL4HAAO3YAvXvLaZ+ITIXhTPbRti1w9Ki89jt3Br76Sl77RGQaHNYme1i2TG4wA+IZtJFD6kRkWuw5kz14PEBlpewqgCuvBPbvl10FESmOPWeyvpISNYIZAA4cALxe2VUQkeIYzmR9M2bIrqCu5ctlV0BEiuOwNllfbCzw/feyqzive3eguFh2FUSkMPacyfoqKmRXUNfBg7IrICLFMZzJ+mqfLKWCM2dkV0BEimM4k/U1aSK7grpUq4eIlMNwJutLSJBdQV1JSbIrICLFMZzJ+nr2lF1BXaNHy66AiBTHcCbr69NH7G2timnTZFdARIrjUiqyPl0H2rcHTp+WXQnQvz/w0UeyqyAixbHnTNYXHw+MGCG7CuHZZ2VXoAZdB7KzgTFjgKuuEturulxihMPhAJxOMXEuOhpo1w746U+BefO4uxrZBnvOZA+aBtxwA1BeLq+GzEwgK0te+zLpOpCTA6xdK9Z5h7K8zekEOnYEfvMbYPp0IC4ufHUSKYLhTPaRlwc8/DBQVWV82337Atu3G9+uTLoOzJkDrF8PnDoVuXZSU8WWqKmpkWuDyGAMZ7KXvDyx1/bZs8a1abeTqDQNmDkT2LbN2HbT04H8fGPbJIoQhjPZT1ERMGoUcOhQ5Nvq2hXYsyfy7agiM1M8S5YlKUmc/EVkcpwQRvbTuzfw738DDz4Y2XbS0+0VzOPGyQ1mQDzPbtNGbg1EYcCeM9lbUREwciRQWhq+a/bpI56B9u4dvmuqTnaP+UIJCeH9N6WG6TqwejWwY4c4P/3UKXGGenQ00KwZ0K2b+L2YNIkT+PzEcCYCRJjOnQucOBHc98fEAHfeCTz5pP1efDRNvPCqxo6T8Iyk68Dvfge88gpw5Ij/33f11eL3LS0tcrVZAMOZqLY9e4CMDHHesq6LJT+1l/3UrMONiRHLeUaPFjt+2S2Qa4uOljMD3h92Xr4WKZoGLFgAbNoU2nUSEoA33uAs+0tgOBNR8MaOFT0nlWmavR4xRFJmJrB4MRDO2Bg5Evjb38J3PYtgOBNRcFQdzr4Qt0wNjxtuALZujcy1XS5g1SpgwoTIXN+EGM5EFJzERKCsTHYV/tF1ez96CIWmAUOGBD8fIxBDhwKbN0e+HRPgUioiClxBgXmCGRBbh1LgHnxQjI4YEcwAsGULH0H8B3vORBS4du1Eb9QsunQBvvxSdhXmoWlio55vvpHTPnd7YzgTUYB0XYSzmbhcxm7ZamZ5eeJAkepquXXYfCIfh7XtoqQEGD4cuPxyoGlTwO0Wp/u4XOK/brd483iAli2B7t15RB/Vb9Ei2RUELpRTsOxElWAGgFtukV2BVOw5W5muA5Mni/WIobw4ud1iQ4eRI7nDD4kbvMOHZVcROL7UNUzTxO+5Sv+fNm8Wk8RsiOFsRZomNsbQtMhcf/BgYOFCbh5gVzExQEWF7CoCx5e6hsXHqzdSFh8f2O5jFsJhbauZMEHMroxUMAPAO++INm69NXJtkLpUGPKk8Fq6VL1gBsTon4p1GYDhbCXJycCaNca198YbYivLZcuMa5Pk83hkV0DhNnu27AouzYxzHMKA4WwVV14J7N0rp+0ZM4CoKG4eYBc9e8qugMKppETtxxRr18quQAqGsxX07SvOsZXp9Glg2DCgQ4fIDqmTfEuXyq4gcNHRsitQ10MPya6gYd9+K7sCKRjOZpeZKc5QVcW//y2eR+flya6EIqVbN/OFnRn2AJelqEh2BQ2z6fp0hrOZaZpaB9zX9sADDGgrM9tzwOefl12Buk6elF0B1YNLqczMDOtNbb7Lj6W1agUcPy67isY1b84AaojTqf4yM9XriwD2nM2qoED9YAaA226TXQFFypYtsivwz/LlsitQm8MhuwKqB8PZrNLTZVfgn6+/Ns+LOAUmNRWYNUt2FQ3r29c8vyuyqL40LipKdgVScFjbjMx28ICNd/mxhbQ0NW/AWrcGjh6VXYX6uncHdu2SXcWl/fKXwAcfyK7CcOw5m5HZJuPYeJcfW9i8WczgVklMDIPZX6NGya6gYX/4g+wKpGDP2Yzi4sy39i8jA1i8WHYVFEnJyfI2wqmtRQtzTFRTha4DCQlqTrqy8WQ+9pzNyIw9AiO3FSU59uwRQ9wyXXklgzlQ8fFAv36yq6ifjSfzMZzNSMU73MZwWNseCgqA/HxxTrjR0tOB/fuNb9cKnn1WvVnbNp/Mx3AmY5jxhoKCk54udnUy6tSymlPY8vONac+KUlOBRx+VXcV57doB27fLrkIqPnM2I9XucP3FHzX7KSoC5swB3n47/NceMAD461/FHAwKj3HjgHXr5NaQmGiOPRwijOFsRgxnMhuvF1i9Wgx7f/IJUF4e3HU6dgRefBHo3z+s5VEts2fLWxHSt6/te8w1GM5mFB9vvme4UVFAZaXsKkgVNWGtacAXXwBlZcCpU+IGzukUs3R79xYnYCUny67WfoqKgJtuMnZVSGYmkJVlXHuKYzib0SOPAE8/LbuKwAwYAGzdKrsKIgpETg7w8MORPRnqqqvEUDr34K+D4WxGZtshDAB272YPiMisXnkFuPtu4IcfwnfNrl3FyMjQoeG7poUwnM2qSxdg3z7ZVfiHQ9pE1uD1iufRL70kOgmB9KijooCBA4EhQ4CJEzmRrxEMZ7PSNPMcIJ+TA0ybJrsKIoqEPXvEDoC7dokNkhwOsX1q375iX2wGcVAYzmY2ZAjwzjuyq2hYbCxw7JjsKoiITIXhbHZuN3DunOwqLk3TONGDiChA3CHM7D75RHYFl5aby2AmIgoCw9nsVD3wPjcXmDpVdhVERKbEYW2rUOXAe49HHIzOHjMRUdAYzlYybJg4+F6Wnj2Bf/5TXvtERBbBYW0rKSgQW+AZrWlTcSIQg5mIKCzYc7aioiLgjjuA//3fyLbjcgG//73YKJ+IiMKGPWcr6t0b+Ne/xBB3167hv37r1sDLL4vdgRjMRERhx56zHdScALRzp+hN79wpjuzzZ3200yl2++nYERg9Wuz0xd1+iIgiiuFMRESkGA5rExERKYbhTEREpBiGMxERkWIYzkRERIphOBMRESmG4UxERKQYhjMREZFiGM5ERESKYTgTEREphuFMRESkGIYzERGRYhjOREREimE4ExERKYbhTEREpBiGMxERkWIYzkRERIphOBMRESmG4UxERKQYhjMREZFiGM5ERESKYTgTEREphuFMRESkGIYzERGRYhjOREREimE4ExERKYbhTEREpBiGMxERkWIYzkRERIphOBMRESmG4UxERKQYhjMREZFi/h/tstrwVleQXAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "nx.draw(graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "684a03a2c5b1b5fad2d8fd424170398d",
     "grade": true,
     "grade_id": "cell-1ac608c1997b2f46",
     "locked": true,
     "points": 5,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CIS 545 Test Cases] (5 pts)\n"
     ]
    }
   ],
   "source": [
    "# [CIS 545 Test Cases] (5 pts)\n",
    "\n",
    "print('[CIS 545 Test Cases] (5 pts)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "c4087542be58241c3585039473e2cff9",
     "grade": true,
     "grade_id": "cell-6c94292fe1666538",
     "locked": true,
     "points": 5,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
