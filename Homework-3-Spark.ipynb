{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CIS 545"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "67048efff7e997465111973f64845f93",
     "grade": false,
     "grade_id": "note-hw3-1",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "# Homework 3, Part I: Spark\n",
    "\n",
    "## Spark Setup\n",
    "\n",
    "Please make sure the Jupyter menu above shows \"PySpark\" at the upper right, and not \"Python 3\".  If it says \"Python 3\" then go to the *Kernel* menu and choose *PySpark*.\n",
    "\n",
    "\n",
    "## Step 1\n",
    "\n",
    "### Breadth-first Search over the NotreDame Graph\n",
    "\n",
    "In the previous assignment, you had built several primitives, including breadth-first search, to compute over the Yelp social graph.  Some of your computations were limited by the amount of computational resources available.  We will now try to explore the broader graph."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "03f2b53d6b0053e0771a3ab12bc48086",
     "grade": false,
     "grade_id": "note-readme",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### Read **prior** to starting assignment\n",
    "\n",
    "You will see some of the following in the cells below, and you should treat them as follows:\n",
    "- `[hw3-cellname]` This is a cell label. You should not delete these.\n",
    "- For the CIS 545 test case cells with score additions, those are just for padding, they have no purpose but to let the autograder run more smoothly! The `score` variable **is not** what you will get in an autograder score\n",
    "- try/catch: This is for the name of your dataset. Please read the instructions carefully."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "b7e38aaabb0097a4ffad30dac6a2208d",
     "grade": false,
     "grade_id": "note-hint",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "Hints: For early testing of your solutions to Step 2, you should probably just load a subset of thisl file into `graph_sdf` to validate your solutions.  Later go back, add the contents of the other files to graph_sdf, and re-run your solutions.  Make sure you rerun your code with the full `graph_sdf` before submitting. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "764571687c2c2e933aea025349948d0a",
     "grade": false,
     "grade_id": "note-sparkinit",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### Google Code Spark Initialization\n",
    "\n",
    "These codes are a setup for your reference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.6/site-packages (0.24.2)\n",
      "Requirement already satisfied: numpy>=1.12.0 in /opt/conda/lib/python3.6/site-packages (from pandas) (1.16.2)\n",
      "Requirement already satisfied: pytz>=2011k in /opt/conda/lib/python3.6/site-packages (from pandas) (2018.9)\n",
      "Requirement already satisfied: python-dateutil>=2.5.0 in /opt/conda/lib/python3.6/site-packages (from pandas) (2.8.0)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.6/site-packages (from python-dateutil>=2.5.0->pandas) (1.11.0)\n",
      "\u001b[31mpyspark 2.3.2 requires py4j==0.10.7, which is not installed.\u001b[0m\n",
      "\u001b[33mYou are using pip version 10.0.1, however version 19.0.3 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "04eb9376942c5a50aabee82a7010ffb5",
     "grade": false,
     "grade_id": "hw3-import",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# [hw3-import]\n",
    "# Run this cell to import the various PySpark and OS operations\n",
    "\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import * \n",
    "import pyspark.sql.functions as F\n",
    "import pyspark.sql.functions as f\n",
    "\n",
    "import pandas as pd\n",
    "import logging\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [hw3-sparkcheck]\n",
    "# Just a check to make sure you have the pyspark kernel.\n",
    "\n",
    "try:\n",
    "    if(spark == None): \n",
    "        raise ValueError(\"PySpark Kernel not ready! \")\n",
    "        # The following sentence only run on local machine. \n",
    "#         spark = SparkSession.builder.appName('Graphs-HW3').getOrCreate()\n",
    "except NameError:\n",
    "    raise ValueError(\"PySpark Kernel not ready! \")\n",
    "    # The following sentence only run on local machine. \n",
    "#     spark = SparkSession.builder.appName('Graphs-HW3').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "ff4a3246c1286325ecb0f14f737bf8d8",
     "grade": false,
     "grade_id": "first-pad",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# This is a padding cell for Spark operation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "45933ca03c80bb30a82ff36744579d2f",
     "grade": false,
     "grade_id": "hw3-test1-pad1",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CIS 545 Homework 3] Test Case Padding\n"
     ]
    }
   ],
   "source": [
    "# [hw3-pad]\n",
    "# This cell is for grading purposes only!\n",
    "\n",
    "score = 0 # for manual grading\n",
    "\n",
    "print(\"[CIS 545 Homework 3] Test Case Padding\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "2c57baccae041bb7fb57de6c98afa265",
     "grade": false,
     "grade_id": "step1-1",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### Step 1.1. Loading the Data\n",
    "\n",
    "Let’s read our graph data from NotreDame, which forms a directed graph.  Here, the set of nodes is also not specified; the assumption is that the only nodes that matter are linked to other nodes, and thus their IDs will appear in the set of edges.  For example, to load the file `input.txt` into a Spark DataFrame, you can use lines like the following. Note that we aren't running just `.txt` files in this assignment!\n",
    "\n",
    "```\n",
    "# Read lines from the text file, from Google Storage bucket my-bucket\n",
    "input_sdf = spark.read.format(\"txt\").load(\"gs://my-bucket/mypath/input.txt\")\n",
    "```\n",
    "\n",
    "We’ll use the suffix `_sdf` to represent “Spark DataFrame,” much as we used `_sdf` to denote a Spark DataFrame in Homework 2. You will load a `ndgraph_sdf` table from the `web-NotreDame.csv` file. \n",
    "\n",
    "Downloading these files may take a while, so don’t worry. See if you can further add a function call (e.g., to selectExpr()) to rename the edges to `from_node` and `to_node`, to convert these to integers, respectively. Remove duplicates to create `graph_sdf` with all data.\n",
    "\n",
    "Please name the tables as we have described in this assignment. This helps with our grading and test cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "541953268f9e56a88fa9730518aeb85e",
     "grade": false,
     "grade_id": "hw3-filepath",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# [hw3-filepath]\n",
    "# We'll use the variable dataset to store the filepath for your csv\n",
    "\n",
    "try:\n",
    "    # This is how you should load your data\n",
    "    dataset = 'web-NotreDame.csv'\n",
    "except:\n",
    "    # Note that this is not going to be a valid link until we grade!\n",
    "    dataset = 'https://s3.amazonaws.com/cis545-hw3data/web-NotreDame.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "9a3fa70ea71fdeb87eb60d393a2fbcfc",
     "grade": false,
     "grade_id": "note-dataset",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "Above, note that we stored `dataset` as the name of your CSV. You should use the variable dataset below when importing, in lieu of a filepath directly. If you need to change it, you may change the allocation of the variable below.\n",
    "\n",
    "Note that you should not change the S3 link, as doing so (or not using the variable `dataset` below) will lead to issues autograding your homework."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "bdab95b5d9db9b1cca0bb42f1a36094c",
     "grade": false,
     "grade_id": "hw3-importdata",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# [hw3-importdata]\n",
    "# Import your dataset\n",
    "\n",
    "# YOUR CODE HERE\n",
    "ndgraph_sdf = spark.read.format(\"csv\").load(\"gs://bucket-name_rj16/notebooks/web-NotreDame.csv\", header=True)\n",
    "ndgraph_sdf.createOrReplaceTempView('ndgraph')\n",
    "\n",
    "ndgraph_sdf = spark.sql('select from as from_node, to as to_node from ndgraph')\n",
    "ndgraph_sdf = ndgraph_sdf.withColumn(\"from_node\", ndgraph_sdf[\"from_node\"].cast(IntegerType()))\n",
    "ndgraph_sdf = ndgraph_sdf.withColumn(\"to_node\", ndgraph_sdf[\"to_node\"].cast(IntegerType()))\n",
    "ndgraph_sdf.createOrReplaceTempView('ndgraph')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "9f82ddc58332c2aef6667cc4fd0935e4",
     "grade": false,
     "grade_id": "not-dontedit",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "The following cells are used for grading and data entry purposes, if necessary. Please do not edit the cells (although it is allowed to be edited). If you must, add cells **below** them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [hw3-dataload]\n",
    "# Do not edit!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "56ca103cf4c49fdc54add68c8605e0f4",
     "grade": false,
     "grade_id": "note-edit",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "You may edit cells below **here**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "6cbb248a8b012969d5a91c471046ac22",
     "grade": false,
     "grade_id": "hw3-printschema",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- from_node: integer (nullable = true)\n",
      " |-- to_node: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# [hw3-printschema]\n",
    "# View the schema presented \n",
    "\n",
    "ndgraph_sdf.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "c18d00c44272fcc4b77d2dfc24a6155e",
     "grade": false,
     "grade_id": "hw3-showndgraph",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------+\n",
      "|from_node|to_node|\n",
      "+---------+-------+\n",
      "|        0|      0|\n",
      "|        0|      1|\n",
      "|        0|      2|\n",
      "|        0|      3|\n",
      "|        0|      4|\n",
      "|        0|      5|\n",
      "|        0|      6|\n",
      "|        0|      7|\n",
      "|        0|      8|\n",
      "|        0|      9|\n",
      "|        0|     10|\n",
      "|        0|     11|\n",
      "|        0|     12|\n",
      "|        0|     13|\n",
      "|        0|     14|\n",
      "|        0|     15|\n",
      "|        0|     16|\n",
      "|        1|      0|\n",
      "|        1|      7|\n",
      "|        1|     17|\n",
      "+---------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# [hw3-showndgraph]\n",
    "# View the output of ndgraph. You should see your correct columns\n",
    "\n",
    "ndgraph_sdf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "eafdcd794f88dff6d9f2887e305a0d29",
     "grade": true,
     "grade_id": "test-colcount",
     "locked": true,
     "points": 5,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# [test-colcount]\n",
    "# [CIS 545 Homework 3] Test Case (test case pad below!)\n",
    "\n",
    "assert (len(ndgraph_sdf.columns) == 2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "5c5352b6adf6e45712d5d9ee97f7a276",
     "grade": false,
     "grade_id": "test-pad1",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CIS 545 Homework 3] Test Case - 2 points\n"
     ]
    }
   ],
   "source": [
    "# [CIS 545 Test Cases] (2 pts)\n",
    "print(\"[CIS 545 Homework 3] Test Case - 2 points\")\n",
    "score = score + 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "e1cee96260a13c9b74de03989ee31b70",
     "grade": true,
     "grade_id": "test-colset",
     "locked": true,
     "points": 5,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# [test-colset]\n",
    "# [CIS 545 Homework 3] Test Case\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "a838a4fa343a6b64432a8a07585b79dc",
     "grade": false,
     "grade_id": "test-pad2",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CIS 545 Homework 3] Test Case - 5 points\n"
     ]
    }
   ],
   "source": [
    "# [CIS 545 Test Cases] (5 pts)\n",
    "print(\"[CIS 545 Homework 3] Test Case - 5 points\")\n",
    "score = score + 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "903440059e61fea822cfdbd154fcb89c",
     "grade": false,
     "grade_id": "hw3-creatgraph",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "You will want to create a `graph_sdf` and save it in the Spark TempView so that you can call your transitive closure function on it here.\n",
    "\n",
    "Hints: For early testing of your solutions to Step 2, you should probably just load a subset of this file into `graph_sdf` to validate your solutions.  Later go back, add the contents of the other files to graph_sdf, and re-run your solutions.  Make sure you rerun your code with the full `graph_sdf` before submitting. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "39cd4891720cc0a4bcc4d139a7522a65",
     "grade": false,
     "grade_id": "hw3-pysparkF",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# [hw3-pysparkF]\n",
    "# Import PySpark types and functions\n",
    "\n",
    "from pyspark.sql.types import *\n",
    "import pyspark.sql.functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "915be2c41d788d54707ab664151feb73",
     "grade": false,
     "grade_id": "hw3-graph",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# [hw3-graph]\n",
    "# Create graph_sdf in ndgraph_sdf and create a SQL table if you decide to use it later.\n",
    "\n",
    "# YOUR CODE HERE\n",
    "graph_sdf = ndgraph_sdf.distinct()\n",
    "#graph_sdf = ndgraph_sdf[ndgraph_sdf['from_node']<150]\n",
    "graph_sdf.createOrReplaceTempView('graph')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1497134"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph_sdf.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "a4a974d29cfe44d8b15d7e51181c4bf3",
     "grade": false,
     "grade_id": "hw3-pad1",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CIS 545 Homework 3] Padding\n"
     ]
    }
   ],
   "source": [
    "# [hw3-padding]\n",
    "# Just a padding cell\n",
    "print(\"[CIS 545 Homework 3] Padding\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "98477de99fa2bdef6a194c0e95fc6132",
     "grade": false,
     "grade_id": "hw3-cache",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[from_node: int, to_node: int]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# [hw3-cache]\n",
    "# You should now cache for efficiency\n",
    "graph_sdf.cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "ab5d8a4bb5298fd05616f91b7318657f",
     "grade": false,
     "grade_id": "step1-2",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### Step 1.2. Transitive Closure of the Graph\n",
    "\n",
    "Now we would like to do the following: given a set of nodes, compute the set of all nodes reachable from these nodes.  This can be obtained via a type of transitive closure computation. Google it if you don't know Transitive Closure. \n",
    "\n",
    "Define a function `transitive_closure(graph_sdf, origins_sdf, depth)` that returns a Spark DataFrame.\n",
    "\n",
    "The result should be the set of all nodes from the `input graph_sdf` that are reachable via graph edges from the set of `origins_sdf`, in at most depth iterations (hops).  Both origins_sdf and the returned result should be DataFrames with a single attribute called node.  \n",
    "\n",
    "You should treat the edges in the `graph_sdf` as directed edges!  You should iterate until you have either hit the maximum depth or the set of newly discovered (frontier) nodes is empty."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "758c55bbe89c4134a39e46298ba1036b",
     "grade": false,
     "grade_id": "note-transitive",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "Hints: this resembles your BFS algorithm from HW2, but you should take advantage of the opportunities to optimize.  Both the graph and the various node sets can easily have duplicates; you should make heavy use of duplicate removal (and repartition and cache) since you are only computing the set of reachable nodes. Also, to quickly check whether a DataFrame is empty, you can use something similar to the following. \n",
    "\n",
    "(You are not needed to update the following code block. Please start your code in the block containing `transitive_closure`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "9fe12371883dadf2ef17d141486a5bf1",
     "grade": false,
     "grade_id": "hw3-sdfempty",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# [hw3-sdfempty]\n",
    "# This code block may be used in transitive_closure\n",
    "\n",
    "def sdf_is_empty(sdf):\n",
    "    try:\n",
    "        sdf.take(1)\n",
    "        return False\n",
    "    except:\n",
    "        return True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "82fca8f228efd60e044253a9a554d04e",
     "grade": false,
     "grade_id": "note-transitivecode",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "Please insert your code for `transitive_closure` below. You may find repartitioning or caching valuable in speeding up your queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "4c422d69c0f51949f13616597dcc1693",
     "grade": false,
     "grade_id": "hw3-transitive",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# [hw3-transitive]\n",
    "# Write your transitive closure function here. Take note of the types of the inputs!\n",
    "\n",
    "def transitive_closure(graph, origin, depth):\n",
    "    G = graph\n",
    "    schema = StructType([\n",
    "        StructField(\"node\", IntegerType(), True),\n",
    "        StructField(\"distance\", IntegerType(), False)])\n",
    "    origin_nodes = []\n",
    "    for element in origin.select('node').collect():        \n",
    "        origin_nodes.append((element.node, 0))\n",
    "    #print(my_list_of_maps)\n",
    "    frontier_sdf = spark.createDataFrame(origin_nodes, schema)\n",
    "    if depth == 0:\n",
    "        return frontier_sdf\n",
    "    visited_sdf = spark.createDataFrame([], schema)\n",
    "    G.createOrReplaceTempView('graph')\n",
    "    #G_sdf = spark.sql(\n",
    "    #    'SELECT from_node, to_node FROM graph UNION SELECT to_node, from_node FROM graph')\n",
    "    return_sdf = frontier_sdf\n",
    "    for i in range(0, depth):\n",
    "        #print(return_sdf.show(20))\n",
    "        visited_sdf = visited_sdf.union(frontier_sdf)\n",
    "        edge_sdf = frontier_sdf.join(G, frontier_sdf.node == G.from_node)\n",
    "        next_sdf = edge_sdf.select(edge_sdf.to_node.alias('node')).withColumn(\"distance\", F.lit(i + 1))\n",
    "        next_sdf = next_sdf.join(visited_sdf, visited_sdf.node == next_sdf.node, \"leftanti\")\n",
    "        return_sdf = return_sdf.union(next_sdf).distinct()\n",
    "        #print(next_frontier_sdf.show(5))\n",
    "        frontier_sdf = next_sdf.distinct()\n",
    "        #print(next_frontier_sdf.distinct())\n",
    "        #print(sdf_is_empty(frontier_sdf))\n",
    "        #print(frontier_sdf.select('node').collect())\n",
    "        if frontier_sdf.select('node').collect()==[] :\n",
    "            return return_sdf\n",
    "    return return_sdf\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "bd10668c179a5db4373f7f7fb21687d3",
     "grade": false,
     "grade_id": "step1-3",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### Step 1.3 Testing transitive closure\n",
    "\n",
    "Compute a Spark DataFrame called `nodes_sdf` as follows:\n",
    "* Select `from_node` id 113, 114, 115 and 116\n",
    "* Rename the column to `node`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "dcadc95c3bf87aa315a9bf4effa65391",
     "grade": false,
     "grade_id": "hw3-nodes",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# [hw3-nodes]\n",
    "# Create nodes_sdf as described above\n",
    "\n",
    "nodes_sdf = spark.sql(\"SELECT from_node AS node FROM graph WHERE from_node>=113 AND from_node<=116\")\n",
    "nodes_sdf = nodes_sdf.distinct()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "1aecb13cc5894011fd8d7fd84eb91208",
     "grade": false,
     "grade_id": "hw3-nodeshow",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+\n",
      "|node|\n",
      "+----+\n",
      "| 115|\n",
      "| 114|\n",
      "| 113|\n",
      "| 116|\n",
      "+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# [hw3-nodeshow]\n",
    "# View nodes_sdf\n",
    "\n",
    "nodes_sdf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "115\n",
      "114\n",
      "113\n",
      "116\n"
     ]
    }
   ],
   "source": [
    "for i in nodes_sdf.select('node').collect():\n",
    "    print(i.node)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "c78618650d0f58afcaa1c5182987a2fd",
     "grade": true,
     "grade_id": "test-nodes",
     "locked": true,
     "points": 5,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# [test-nodes]\n",
    "# Testing nodes setup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "6a88cf931259d2ba3942e445af6038f0",
     "grade": false,
     "grade_id": "test-pad3",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CIS 545 Homework 3] Test Case - 5 points\n"
     ]
    }
   ],
   "source": [
    "# [CIS 545 Test Cases] (5 pts)\n",
    "print(\"[CIS 545 Homework 3] Test Case - 5 points\")\n",
    "score = score + 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "509038f3259a53a604c64686a469f057",
     "grade": false,
     "grade_id": "note-reachable",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "Compute a Spark DataFrame called `reachable_sdf` with the results of `transitive_closure(graph_sdf, nodes_sdf, 3)` over the NotreDame dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "e80927c2779eada79b2f533d6cc50aa5",
     "grade": false,
     "grade_id": "hw3-reachable",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# [hw3-reachable]\n",
    "# Create reachable_sdf using transitive closure\n",
    "reachable_sdf = transitive_closure(graph_sdf, nodes_sdf, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "faeaa4ac21f55f98eb7e9a1053866b90",
     "grade": false,
     "grade_id": "test-getresult",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# [test-getresult]\n",
    "# This is a no-points test case, making sure we get a result\n",
    "assert (reachable_sdf.take(1)[0].node != None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "d96d3299de6a2ddc0c132a6f6b24483d",
     "grade": false,
     "grade_id": "hw3-pad2",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CIS 545 Homework 3] Padding\n"
     ]
    }
   ],
   "source": [
    "# [hw3-padding]\n",
    "# Just a padding cell\n",
    "\n",
    "print(\"[CIS 545 Homework 3] Padding\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "426"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reachable_sdf.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "cd87b39ce6e7edc8cb8ccd7324d53e69",
     "grade": false,
     "grade_id": "note-showreach",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "Run the code cell to call `show()` and `count` on `reachable_sdf`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "0735182b1a2fb969cfd1582674369a18",
     "grade": false,
     "grade_id": "hw3-showreach",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------+\n",
      "| node|distance|\n",
      "+-----+--------+\n",
      "| 4383|       2|\n",
      "| 3146|       2|\n",
      "|11759|       3|\n",
      "| 1124|       1|\n",
      "| 3359|       2|\n",
      "|   16|       3|\n",
      "|11762|       3|\n",
      "|14424|       3|\n",
      "|12183|       3|\n",
      "| 4366|       2|\n",
      "| 3150|       2|\n",
      "| 3297|       2|\n",
      "|12311|       3|\n",
      "| 1141|       1|\n",
      "| 4363|       2|\n",
      "| 3291|       2|\n",
      "|12333|       3|\n",
      "| 4386|       2|\n",
      "| 4368|       2|\n",
      "| 3244|       2|\n",
      "+-----+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# [hw3-showreach]\n",
    "# Show reachacble_sdf\n",
    "\n",
    "reachable_sdf.show(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "fe413a1650acb9cb0cc7084c67848a68",
     "grade": false,
     "grade_id": "hw3-showcount",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "426"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# [hw3-count]\n",
    "# Show the count of reachable_sdf\n",
    "\n",
    "reachable_count = reachable_sdf.count()\n",
    "reachable_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "5be5086c945e5646bb8f9e314d596438",
     "grade": true,
     "grade_id": "test-reachable1",
     "locked": true,
     "points": 5,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# [test-reachable1]\n",
    "# Tests on reachable_sdf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "402cf5bf00de19b85dbfde32f6a59dcf",
     "grade": false,
     "grade_id": "test-pad4",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CIS 545 Homework 3] Test Case - 5 points\n"
     ]
    }
   ],
   "source": [
    "# [CIS 545 Test Cases] (5 pts)\n",
    "print(\"[CIS 545 Homework 3] Test Case - 5 points\")\n",
    "score = score + 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "f84406b305b5e3c0f8132ce05bd97339",
     "grade": true,
     "grade_id": "test-reachable2",
     "locked": true,
     "points": 5,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# [test-reachable1]\n",
    "# Tests on reachable_sdf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "fc6cbb5de40f5c2c4630dd7582a4b12b",
     "grade": false,
     "grade_id": "test-pad5",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CIS 545 Homework 3] Test Case - 5 points\n"
     ]
    }
   ],
   "source": [
    "# [CIS 545 Test Cases] (5 pts)\n",
    "print(\"[CIS 545 Homework 3] Test Case - 5 points\")\n",
    "score = score + 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}