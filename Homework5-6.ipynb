{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rQFR6wRlOAI2"
   },
   "source": [
    "Before you turn this problem in, make sure everything runs as expected. First, **restart the kernel** (in the menubar, select Kernel$\\rightarrow$Restart) and then **run all cells** (in the menubar, select Cell$\\rightarrow$Run All).\n",
    "\n",
    "Make sure you fill in any place that says `YOUR CODE HERE` or \"YOUR ANSWER HERE\", as well as your name and collaborators below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WH7-HS0HOAI5"
   },
   "outputs": [],
   "source": [
    "COLLABORATORS = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "V7WmJ1o3OAJA"
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "deletable": false,
    "editable": false,
    "id": "PUujfuXdOAJB",
    "nbgrader": {
     "checksum": "c0307ee75db0cf4d3c2fc0e28909dac4",
     "grade": false,
     "grade_id": "title",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "# CIS 545 Homework 5 and 6: Amazon Review Analysis and Classification\n",
    "\n",
    "Your main training set for this assignment is the text from 100,000 reviews from Amazon.com, their timestamps, and their star ratings. The high level goal of this homework is to use the textual and temporal data to predict the star ratings.\n",
    "\n",
    "**Adventurers beware!** Analyzing this data in `sklearn` will likely kill your kernel. So instead we will use the package [gensim](https://radimrehurek.com/gensim/) for analysis. gensim specializes in efficient implementations of common modeling techniques for big text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 683
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 11862,
     "status": "ok",
     "timestamp": 1556892090991,
     "user": {
      "displayName": "Ruo Jia",
      "photoUrl": "",
      "userId": "09326440466598086645"
     },
     "user_tz": 240
    },
    "id": "JPlSrTq4OAJC",
    "outputId": "ae2d3cc3-1a4a-4bf0-d2ff-296af3bb0b2a"
   },
   "outputs": [],
   "source": [
    "# install stuff\n",
    "!pip install gensim\n",
    "!pip install paramiko"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4lyByQfCOAJG"
   },
   "outputs": [],
   "source": [
    "# import stuff\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from gensim import corpora\n",
    "from gensim.models import LsiModel, KeyedVectors\n",
    "import sklearn.model_selection as ms\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from datetime import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 119
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 35154,
     "status": "ok",
     "timestamp": 1556892114308,
     "user": {
      "displayName": "Ruo Jia",
      "photoUrl": "",
      "userId": "09326440466598086645"
     },
     "user_tz": 240
    },
    "id": "KEu9lq9wC-sz",
    "outputId": "25fc648e-e172-4301-b849-9d2af7d5d2a6"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uK3O7Oc8OAJJ"
   },
   "outputs": [],
   "source": [
    "# read stuff\n",
    "reviews_dict = corpora.Dictionary.load(\"reviews.dict\")\n",
    "reviews_bow = corpora.MmCorpus('train_reviews.mm')\n",
    "reviews_times  = np.load('train_times.npy')\n",
    "reviews_times.shape = (len(reviews_bow),1)\n",
    "reviews_wc = corpora.MmCorpus('reviews_wc.mm')\n",
    "reviews_sppmi_300 = np.load('reviews_sppmi_300.npy')\n",
    "reviews_wv = KeyedVectors.load(\"word_vectors.wv\", mmap='r')\n",
    "y = np.vstack((np.repeat(1, 4000), np.repeat(2, 4000), np.repeat(3, 4000), np.repeat(4, 4000), np.repeat(5, 4000)))\n",
    "y = np.repeat(y, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "deletable": false,
    "editable": false,
    "id": "ke4Y1J72OAJM",
    "nbgrader": {
     "checksum": "a23a9a0c711f4448f64597a9ff3a5053",
     "grade": false,
     "grade_id": "dictionary_spec",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "## Step 0: Format Exploring\n",
    "\n",
    "We will start with exploring the format of all of the data files that we imported above. \n",
    "\n",
    "### Step 0.1: gensim dictionary (lexicon)\n",
    "\n",
    "Most data science over text has some form of vocabulary. Simply put, you need to decide which words your model will care about. Very rare words, misspellings, numbers, and urls are good candidates for exclusion, especially since if the model needs any form of normalization, the time complexity of such computations is at least linear in the size of the vocabulary, if not worse.\n",
    "\n",
    "A lexicon associates each word in the vocabulary with an index. Since words are repeated, the model can save space by using the index for every repetition and only linking the index with the string form once. A `gensim` dictionary is special in that it is very fast and allows bidirectional lookups, namely, word to index and index to word.\n",
    "\n",
    "After reviewing the [documentation](https://radimrehurek.com/gensim/corpora/dictionary.html), rewrite the right hand side of each line in the cell below with the answers to these questions.\n",
    "\n",
    "1. What is the index of \"good\"? Look it up and store it in a variable named `good`. To clarify, if you find that 42 is the index of \"good\", change the line below so that it sets `good` equal to 42. Of course, you can do this with `good = 42` and earn full points, but it is a litte better to reuse the command with which you found the index. For example, if the `gensim` dictionary worked like a list of strings, you could do it with  \n",
    "`good = reviews_dict.iloc('good')`.\n",
    "2. What word belongs to index 195? Look it up and store it in a variable named `oneninefive`.\n",
    "3. What happens when you evaluate `reviews_dict[i]` for some variable `i`? If this returns the word associated with that index, set `idx2word` to `True`. Otherwise, set it to `False`. For example, if `reviews_dict['good']` equals `good`, `idx2word` should be `False`, but if `reviews_dict[195]` equals `oneninefive`, `idx2word` should be `True`.\n",
    "\n",
    "Hint: `token2id('good')` and `id2token(195)` didn't work for me either. Keep trying!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 36023,
     "status": "ok",
     "timestamp": 1556892115219,
     "user": {
      "displayName": "Ruo Jia",
      "photoUrl": "",
      "userId": "09326440466598086645"
     },
     "user_tz": 240
    },
    "id": "DMi3BBEw0oYi",
    "outputId": "220b26c2-24f5-41de-f6f4-ae86ef007b66"
   },
   "outputs": [],
   "source": [
    "reviews_dict.token2id['good']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "deletable": false,
    "executionInfo": {
     "elapsed": 36000,
     "status": "ok",
     "timestamp": 1556892115219,
     "user": {
      "displayName": "Ruo Jia",
      "photoUrl": "",
      "userId": "09326440466598086645"
     },
     "user_tz": 240
    },
    "id": "B2wRSl1-OAJN",
    "nbgrader": {
     "checksum": "73ea04c9b49f909d2df7f3b377f9a75d",
     "grade": false,
     "grade_id": "dictionary_answer",
     "locked": false,
     "schema_version": 1,
     "solution": true
    },
    "outputId": "706ab82d-9053-476d-c788-e0e57722c146"
   },
   "outputs": [],
   "source": [
    "# dictionary_answer\n",
    "good = 19\n",
    "oneninefive = \"brass\"\n",
    "idx2word = True\n",
    "reviews_dict[195]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "deletable": false,
    "editable": false,
    "executionInfo": {
     "elapsed": 35976,
     "status": "ok",
     "timestamp": 1556892115221,
     "user": {
      "displayName": "Ruo Jia",
      "photoUrl": "",
      "userId": "09326440466598086645"
     },
     "user_tz": 240
    },
    "id": "QbbRipUbOAJS",
    "nbgrader": {
     "checksum": "b2bf4521d366ef89b1baec5386fa14c3",
     "grade": true,
     "grade_id": "dictionary_test_1",
     "locked": true,
     "points": 5,
     "schema_version": 1,
     "solution": false
    },
    "outputId": "c4635e2e-dc25-4ed6-9c61-43f3f03ab60b"
   },
   "outputs": [],
   "source": [
    "# dictionary_test_1 (5 points)\n",
    "print(\"The index of good is\", good)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "deletable": false,
    "editable": false,
    "executionInfo": {
     "elapsed": 35955,
     "status": "ok",
     "timestamp": 1556892115222,
     "user": {
      "displayName": "Ruo Jia",
      "photoUrl": "",
      "userId": "09326440466598086645"
     },
     "user_tz": 240
    },
    "id": "i_tk0cjYOAJX",
    "nbgrader": {
     "checksum": "e90358716fc6d2c8131a110b88f763ef",
     "grade": true,
     "grade_id": "dictionary_test_2",
     "locked": true,
     "points": 3,
     "schema_version": 1,
     "solution": false
    },
    "outputId": "fc76bc9b-2989-4017-925e-1dc4104be9ac"
   },
   "outputs": [],
   "source": [
    "# dictionary_test_2 (3 points)\n",
    "print(\"Word 195 is\", oneninefive)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "deletable": false,
    "editable": false,
    "executionInfo": {
     "elapsed": 35936,
     "status": "ok",
     "timestamp": 1556892115223,
     "user": {
      "displayName": "Ruo Jia",
      "photoUrl": "",
      "userId": "09326440466598086645"
     },
     "user_tz": 240
    },
    "id": "soLjjm4JOAJc",
    "nbgrader": {
     "checksum": "aeef76084a9408be0cd5d81e754a0da8",
     "grade": true,
     "grade_id": "dictionary_test_3",
     "locked": true,
     "points": 2,
     "schema_version": 1,
     "solution": false
    },
    "outputId": "7e9aaef9-5dc4-46ff-ea06-64acd0a1b0e5"
   },
   "outputs": [],
   "source": [
    "# dictionary_test_3 (2 points)\n",
    "print(\"It is\", idx2word, \"that brackets can look up a word by its index.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "deletable": false,
    "editable": false,
    "id": "jI8G2wMJOAJi",
    "nbgrader": {
     "checksum": "31ccdc3f20979a61e331e48f3b31519e",
     "grade": false,
     "grade_id": "readable_spec",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### Step 0.2: Efficiently storing text\n",
    "\n",
    "`gensim` represents everything in a **sparse** way. Namely, the representation of a review will be a variable-size list that contains counts of the words that _are present_ in the review. A **dense** representation, on the other hand, such as a matrix, would, in addition to the present words, contain zero counts for all of the words that are not in that particular review. For some examples, see [this tutorial](https://radimrehurek.com/gensim/tut1.html).\n",
    "\n",
    "You may run the cell below as it is to see what is actually stored in a gensim [corpus](https://radimrehurek.com/gensim/corpora/mmcorpus.html). In each review, `gensim` stores a tuple of size 2 for each distinct word in the review. The first number in the tuple is the index of the word in the dictionary and the second number in the tuple is the count of the times that word appeared in that review.\n",
    "\n",
    "Now, we would like you to add a bit of code to the cell below that \"translates\" the first two amazon reviews into something more readable. The reviews are already represented as bags of words, so recall that you cannot recover the order of the words in the reviews. Rather, the words are in alphabetical order. But, we would like you to spell out the repeats of each word. So, if the original review were \"to be or not to be\", `reviews_bow` would have something like:\n",
    "\n",
    "`[(0, 2.0), (1, 1.0), (2, 1.0), (3, 2.0)]`\n",
    "\n",
    "and we would like you to make the string\n",
    "\n",
    "`\"be be not or to to\"`\n",
    "\n",
    "When you are done, `readable_reviews[0]` should be the first review and `readable_reviews[1]` should be the second.\n",
    "\n",
    "Hint: In a normal array, you would be able to access the first two reviews with the indices 0 and 1, but this is not a normal array. Therefore, we have started an iteration through the documents and put a break the loop when `i` reaches a value strictly greater than 1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71
    },
    "colab_type": "code",
    "deletable": false,
    "executionInfo": {
     "elapsed": 36069,
     "status": "ok",
     "timestamp": 1556892115390,
     "user": {
      "displayName": "Ruo Jia",
      "photoUrl": "",
      "userId": "09326440466598086645"
     },
     "user_tz": 240
    },
    "id": "cZW_dRukOAJj",
    "nbgrader": {
     "checksum": "d32969a58d55b387599152ad39b8faea",
     "grade": false,
     "grade_id": "readable_answer",
     "locked": false,
     "schema_version": 1,
     "solution": true
    },
    "outputId": "37734504-a010-41bf-dcef-02ed1ae22b02"
   },
   "outputs": [],
   "source": [
    "# readable_answer\n",
    "i = 0\n",
    "readable_reviews = []\n",
    "for doc in reviews_bow:\n",
    "    review = ''\n",
    "    if i > 1: break\n",
    "    print(doc)\n",
    "    i += 1\n",
    "    for (s, j) in doc:\n",
    "      for k in range(int(j)):\n",
    "        review = review + reviews_dict[s]+' '\n",
    "    readable_reviews.append(review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 36042,
     "status": "ok",
     "timestamp": 1556892115391,
     "user": {
      "displayName": "Ruo Jia",
      "photoUrl": "",
      "userId": "09326440466598086645"
     },
     "user_tz": 240
    },
    "id": "Pi6nwAuEhaCt",
    "outputId": "297264b9-67f0-41f2-a167-4d5775932274"
   },
   "outputs": [],
   "source": [
    "reviews_bow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 36022,
     "status": "ok",
     "timestamp": 1556892115393,
     "user": {
      "displayName": "Ruo Jia",
      "photoUrl": "",
      "userId": "09326440466598086645"
     },
     "user_tz": 240
    },
    "id": "bxjjZeyQTKbu",
    "outputId": "d4198f00-262e-4b1c-ce7b-446a0c4a5617"
   },
   "outputs": [],
   "source": [
    "reviews_dict[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "deletable": false,
    "editable": false,
    "executionInfo": {
     "elapsed": 36001,
     "status": "ok",
     "timestamp": 1556892115394,
     "user": {
      "displayName": "Ruo Jia",
      "photoUrl": "",
      "userId": "09326440466598086645"
     },
     "user_tz": 240
    },
    "id": "Gw0Cyd2MOAJn",
    "nbgrader": {
     "checksum": "c364b5251be4c668282edd417b322521",
     "grade": true,
     "grade_id": "readable_test_1",
     "locked": true,
     "points": 5,
     "schema_version": 1,
     "solution": false
    },
    "outputId": "5085ad36-893b-486e-a2d7-4401da2b451b"
   },
   "outputs": [],
   "source": [
    "# readable_test_1 (5 points)\n",
    "print(readable_reviews[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "deletable": false,
    "editable": false,
    "executionInfo": {
     "elapsed": 35986,
     "status": "ok",
     "timestamp": 1556892115398,
     "user": {
      "displayName": "Ruo Jia",
      "photoUrl": "",
      "userId": "09326440466598086645"
     },
     "user_tz": 240
    },
    "id": "9LseEF89OAJr",
    "nbgrader": {
     "checksum": "01cb0f0e85a086aced96d7423cf42b7f",
     "grade": true,
     "grade_id": "readable_test_2",
     "locked": true,
     "points": 5,
     "schema_version": 1,
     "solution": false
    },
    "outputId": "8cf18d17-e9eb-4d06-b064-b05aa4499da9"
   },
   "outputs": [],
   "source": [
    "# readable_test_2 (5 points)\n",
    "print(readable_reviews[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "deletable": false,
    "editable": false,
    "id": "giV84RveOAJu",
    "nbgrader": {
     "checksum": "9fc59ba35eee3b95d947459aa67a9cd7",
     "grade": false,
     "grade_id": "datetime_spec",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### Step 0.3: Parsing review times\n",
    "\n",
    "It might be useful in predicting the scores of the reviews to know when the reviews were written. In this dataset, the day of the review was recorded as the number of seconds that passed between midnight on January 1, 1970 (the beginning of time for many computer systems) and the time the review was created. This may be efficient because it is one integer, but it is not very convenient. So we are going to convert these int objects to [datetime](https://docs.python.org/3/library/datetime.html) objects:\n",
    "\n",
    "1. Do not change `review_times` in any way. Work with other variables instead.\n",
    "1. Make a new variable named `converted_times`.\n",
    "2. Set that variable equal to a pandas `Series` object made from `review_times` but the entries should be of type `datetime` or `Timestamp`.\n",
    "\n",
    "3. Make a new variable named `forty_days_before_review_times_0`. \n",
    "4. Set that variable equal to 40 days before the time of the first review, using the `timedelta` function.\n",
    "\n",
    "Hint: You might find `datetime.fromtimestamp` to be useful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "deletable": false,
    "id": "UIuFbBpROAJw",
    "nbgrader": {
     "checksum": "aef6457f54419450489903b519f90775",
     "grade": false,
     "grade_id": "datetime_answer",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# datetime_answer \n",
    "# YOUR CODE HERE\n",
    "reviews_times_c = reviews_times.copy()\n",
    "converted_times = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5uX6kq55VdKl"
   },
   "outputs": [],
   "source": [
    "for i in reviews_times_c:\n",
    "  converted_times.append(datetime.fromtimestamp(i[0]))\n",
    "converted_times = pd.Series(converted_times)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "deletable": false,
    "editable": false,
    "executionInfo": {
     "elapsed": 36123,
     "status": "ok",
     "timestamp": 1556892115561,
     "user": {
      "displayName": "Ruo Jia",
      "photoUrl": "",
      "userId": "09326440466598086645"
     },
     "user_tz": 240
    },
    "id": "uJmQZF5EOAJ0",
    "nbgrader": {
     "checksum": "aaa5bab2c56eafcde1696645a4886f74",
     "grade": true,
     "grade_id": "datetime_test_1",
     "locked": true,
     "points": 2,
     "schema_version": 1,
     "solution": false
    },
    "outputId": "ae127dd8-7268-43a8-c2cd-58519dbd98d9"
   },
   "outputs": [],
   "source": [
    "# datetime_test_1 (2 points)\n",
    "print(\"converted_times is a\", type(converted_times))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "deletable": false,
    "editable": false,
    "executionInfo": {
     "elapsed": 36107,
     "status": "ok",
     "timestamp": 1556892115562,
     "user": {
      "displayName": "Ruo Jia",
      "photoUrl": "",
      "userId": "09326440466598086645"
     },
     "user_tz": 240
    },
    "id": "E_RYxmvIOAJ4",
    "nbgrader": {
     "checksum": "1e22c3e79ac7ccb172b20c9dcbf4b141",
     "grade": true,
     "grade_id": "datetime_test_2",
     "locked": true,
     "points": 5,
     "schema_version": 1,
     "solution": false
    },
    "outputId": "8c74b359-4cd7-418c-c4cd-88433af1200e"
   },
   "outputs": [],
   "source": [
    "# datetime_test_2 (5 points)\n",
    "print(\"converted_times[0] is a\", type(converted_times[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6iMFWVg4aJdu"
   },
   "outputs": [],
   "source": [
    "from datetime import timedelta\n",
    "forty_days_before_review_times_0 = converted_times[0]+timedelta(days=-40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "deletable": false,
    "editable": false,
    "executionInfo": {
     "elapsed": 36087,
     "status": "ok",
     "timestamp": 1556892115569,
     "user": {
      "displayName": "Ruo Jia",
      "photoUrl": "",
      "userId": "09326440466598086645"
     },
     "user_tz": 240
    },
    "id": "j6ivecyJOAJ8",
    "nbgrader": {
     "checksum": "fbdb726339ad346b8f5eeb0a1597fb83",
     "grade": true,
     "grade_id": "datetime_test_3",
     "locked": true,
     "points": 3,
     "schema_version": 1,
     "solution": false
    },
    "outputId": "c3741980-4735-43e1-ccb9-a468e9a937ce"
   },
   "outputs": [],
   "source": [
    "# datetime_test_3 (3 points)\n",
    "display(converted_times[0])\n",
    "display(forty_days_before_review_times_0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "deletable": false,
    "editable": false,
    "id": "OuHGGVRTOAKE",
    "nbgrader": {
     "checksum": "14cf022d9a18ce174aeb705830ae0130",
     "grade": false,
     "grade_id": "word_vectors_spec",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "## Step 1: Word vectors\n",
    "\n",
    "We saw in class that performing several optimizations on a word-context shifted positive pointwise mutual information ($SPPMI$) matrix, including decomposing with SVD, is mathematically equivalent to a neural network that learns word embeddings using skip gram with negative sampling ($SGNS$).  We are going to implement each of these here and compare against a pure bag of words ($BOW$) model as a baseline.\n",
    "\n",
    "To begin, let's make a corpus out of our favorite toy dataset: the 5 computer science and 4 math article titles. After lower casing, tokenizing, and stop wording, the corpus looks like `titles` in the cell below. Then, we create a dictionary and a sparse document-term matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 170
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 36068,
     "status": "ok",
     "timestamp": 1556892115569,
     "user": {
      "displayName": "Ruo Jia",
      "photoUrl": "",
      "userId": "09326440466598086645"
     },
     "user_tz": 240
    },
    "id": "nXurZ3AMOAKF",
    "outputId": "d4c9f3cc-185a-4c99-b077-6459af107db9"
   },
   "outputs": [],
   "source": [
    "titles = [['human', 'interface', 'computer'],\n",
    "          ['survey', 'user', 'computer', 'system', 'response', 'time'],\n",
    "          ['eps', 'user', 'interface', 'system'],\n",
    "          ['system', 'human', 'system', 'eps'],\n",
    "          ['user', 'response', 'time'],\n",
    "          ['trees'],\n",
    "          ['graph', 'trees'],\n",
    "          ['graph', 'minors', 'trees'],\n",
    "          ['graph', 'minors', 'survey']]\n",
    "\n",
    "titles_dict = corpora.Dictionary(titles)\n",
    "titles_bow = [titles_dict.doc2bow(title) for title in titles]\n",
    "display(titles_bow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9hg3St2Chxku"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "deletable": false,
    "editable": false,
    "id": "2g-SiXjfOAKI",
    "nbgrader": {
     "checksum": "8bb87b98a3665ecc73a96075e6f93740",
     "grade": false,
     "grade_id": "densify_spec",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### Step 1.1: Sparse to dense\n",
    "\n",
    "To get the term-document matrix that we have seen in lecture, we need to convert this matrix to its dense form. Write a function `densify` that takes as input:\n",
    "\n",
    "1. a sparse matrix in the format of `titles_bow` above\n",
    "3. an integer number of columns\n",
    "\n",
    "and returns a NumPy array. Note that `titles_bow` is a document-term matrix, not a term-document matrix, so we transpose it in the test cell to show the matrix from lecture (with the rows and columns slightly reordered)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "deletable": false,
    "id": "8G2IH8GsOAKJ",
    "nbgrader": {
     "checksum": "7db7411994cdd5df9d865e0c8dedd05b",
     "grade": false,
     "grade_id": "densify_answer",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# densify_answer\n",
    "def densify(sparse, columns):\n",
    "  num_row = len(sparse)\n",
    "  td = np.zeros((num_row, columns))\n",
    "  k=0\n",
    "  for item in sparse:\n",
    "    for (i,j) in item:\n",
    "      td[k, i] = j\n",
    "    k=k+1\n",
    "  return td\n",
    "      \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 221
    },
    "colab_type": "code",
    "deletable": false,
    "editable": false,
    "executionInfo": {
     "elapsed": 36610,
     "status": "ok",
     "timestamp": 1556892116137,
     "user": {
      "displayName": "Ruo Jia",
      "photoUrl": "",
      "userId": "09326440466598086645"
     },
     "user_tz": 240
    },
    "id": "X4rUKsXOOAKQ",
    "nbgrader": {
     "checksum": "b7036664411b854abd9a31bb174b69ac",
     "grade": true,
     "grade_id": "densify_test_1",
     "locked": true,
     "points": 5,
     "schema_version": 1,
     "solution": false
    },
    "outputId": "1d8d4a3b-9582-44d0-ab52-b76481b38634"
   },
   "outputs": [],
   "source": [
    "# densify_test_1 (5 points)\n",
    "td = densify(titles_bow, len(titles_dict)).transpose()\n",
    "print(td)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "deletable": false,
    "editable": false,
    "executionInfo": {
     "elapsed": 36595,
     "status": "ok",
     "timestamp": 1556892116139,
     "user": {
      "displayName": "Ruo Jia",
      "photoUrl": "",
      "userId": "09326440466598086645"
     },
     "user_tz": 240
    },
    "id": "5MsUyIa-OAKZ",
    "nbgrader": {
     "checksum": "f3618616ea065367a28f3b0c031128e8",
     "grade": true,
     "grade_id": "densify_test_2",
     "locked": true,
     "points": 5,
     "schema_version": 1,
     "solution": false
    },
    "outputId": "184ee2dd-68c5-4a53-9315-01cd41177dfd"
   },
   "outputs": [],
   "source": [
    "# densify_test_2 (5 points)\n",
    "print(td.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "deletable": false,
    "editable": false,
    "id": "utDVvvZVOAKe",
    "nbgrader": {
     "checksum": "c70e3bebad46adfd7a6d89edc4c2cc40",
     "grade": false,
     "grade_id": "count_words_spec",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### Step 1.2: Counting words\n",
    "\n",
    "Write a function `count_words` that takes as input:\n",
    "\n",
    "1. a dictionary in the format of `titles_dict` above\n",
    "2. a bag of words corpus in the format of `titles_bow` above\n",
    "\n",
    "and returns a list called `counts` that has the total occurrences of each word in the corpus, in the order of the original word indices. Note that a gensim dictionary only counts a word once per document, so we can't directly use `titles_dict`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "deletable": false,
    "id": "b5ebtsX8OAKn",
    "nbgrader": {
     "checksum": "f19fb05fa9bcdba3f8c093d1305ea708",
     "grade": false,
     "grade_id": "count_words_answer",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# count_words_answer\n",
    "def count_words(gsdict, gsbow):\n",
    "  num_row = len(gsbow)\n",
    "  num_col = len(gsdict)\n",
    "  td = np.zeros((num_col)).astype(int)\n",
    "  for doc in gsbow:\n",
    "    for (i,j) in doc:\n",
    "      td[i] +=j\n",
    "  return td"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "deletable": false,
    "editable": false,
    "executionInfo": {
     "elapsed": 36581,
     "status": "ok",
     "timestamp": 1556892116144,
     "user": {
      "displayName": "Ruo Jia",
      "photoUrl": "",
      "userId": "09326440466598086645"
     },
     "user_tz": 240
    },
    "id": "CptknKM5OAKu",
    "nbgrader": {
     "checksum": "50b681fb8eacbc97e794266dca24c0a6",
     "grade": true,
     "grade_id": "count_words_test_1",
     "locked": true,
     "points": 5,
     "schema_version": 1,
     "solution": false
    },
    "outputId": "504155d2-d400-48e1-c10a-ae4dcd50038c"
   },
   "outputs": [],
   "source": [
    "# count_words_test_1 (5 points)\n",
    "titles_counts = count_words(titles_dict, titles_bow)\n",
    "display(len(titles_counts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "deletable": false,
    "editable": false,
    "executionInfo": {
     "elapsed": 36564,
     "status": "ok",
     "timestamp": 1556892116147,
     "user": {
      "displayName": "Ruo Jia",
      "photoUrl": "",
      "userId": "09326440466598086645"
     },
     "user_tz": 240
    },
    "id": "vSLUl6q3OAK2",
    "nbgrader": {
     "checksum": "f6fe3318b20ec1d5c26be0ca6e8ce81f",
     "grade": true,
     "grade_id": "count_words_test_2",
     "locked": true,
     "points": 5,
     "schema_version": 1,
     "solution": false
    },
    "outputId": "1c35afb8-3bde-4da9-ce08-48dbfe3c5014"
   },
   "outputs": [],
   "source": [
    "# count_words_test_2 (5 points)\n",
    "display(titles_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "deletable": false,
    "editable": false,
    "id": "YQuYebc4OAK6",
    "nbgrader": {
     "checksum": "2bdafd450ea1e9c0fe1e6f93c68cb494",
     "grade": false,
     "grade_id": "reviews_counts",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "The cell below makes the counts for the 100,000 Amazon reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RaOp1CMAOAK7"
   },
   "outputs": [],
   "source": [
    "reviews_counts = count_words(reviews_dict, reviews_bow)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "deletable": false,
    "editable": false,
    "id": "KFV46VmNOALC",
    "nbgrader": {
     "checksum": "5e4fb1e8ccfe658e40d229424dda6757",
     "grade": false,
     "grade_id": "word_context_spec",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### Step 1.3: The word-context matrix\n",
    "\n",
    "Now, write a function `word_context` that takes as input:\n",
    "\n",
    "1. a dictionary in the format of `titles_dict` above\n",
    "2. a corpus in the format of `titles` above\n",
    "3. a window size (integer)\n",
    "\n",
    "and creates a **sparse** word-context matrix. Recall that the word-context matrix is a square matrix and the number of rows/columns is the number of words in the vocabulary/dictionary. A nonzero term in this matrix represents the number of times word $i$ appears within (and including) the window size distance of word $j$. The word-context matrix for this toy corpus is in the lecture slides, so you may use that to check your work.\n",
    "\n",
    "_Note: the only time a word should be in the context of itself is when it occurs with a second instance of that word, for example in the fourth \"document\" of the titles corpus._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "deletable": false,
    "id": "HOmJy0YAOALE",
    "nbgrader": {
     "checksum": "8eaf69fa98dd1aa8fed69222bad93ce2",
     "grade": false,
     "grade_id": "word_context_answer",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# word_context_answer\n",
    "def word_context(gsdict, gscorpus, window):\n",
    "  size = len(gsdict)\n",
    "  wc = np.zeros((size,size)).astype(int)\n",
    "  count = wc.copy()\n",
    "  new_wc = wc.copy()\n",
    "  for corpus_list in gscorpus:\n",
    "    for idx in range(len(corpus_list)):\n",
    "      low, high = idx -window, idx+window\n",
    "      if (idx-window)< 0:\n",
    "        low = 0\n",
    "      if (idx+window+1> len(corpus_list)):\n",
    "        high = len(corpus_list) -1\n",
    "      for idx2 in range(low, high+1):\n",
    "        i = gsdict.token2id[corpus_list[idx2]]\n",
    "        j = gsdict.token2id[corpus_list[idx]]\n",
    "        if (idx2 != idx) and (count[i,j]==0) :\n",
    "          wc[i, j] +=1\n",
    "#         elif (idx2 != idx) & (count[i,j]!=0):\n",
    "#           new_wc[i,j] +=1\n",
    "#       for i in range(size):\n",
    "#         for j in range(size):\n",
    "#           if wc[i,j]!=0:\n",
    "#             count[i,j] = 1\n",
    "#           if new_wc[i,j]>wc[i,j]:\n",
    "#             wc[i,j] = new_wc[i,j]\n",
    "#       new_wc =  np.zeros((size,size)).astype(int)\n",
    "      #print(wc[5,2]), print(new_wc[5,5])\n",
    "  wc_sparse = []\n",
    "  for i in range(size):\n",
    "    row_sparse = []\n",
    "    for j in range(size):\n",
    "      if wc[i,j]!=0:\n",
    "        row_sparse.append((j, wc[i,j]))\n",
    "    wc_sparse.append(row_sparse)\n",
    "  return wc_sparse\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 221
    },
    "colab_type": "code",
    "deletable": false,
    "editable": false,
    "executionInfo": {
     "elapsed": 46360,
     "status": "ok",
     "timestamp": 1556892125964,
     "user": {
      "displayName": "Ruo Jia",
      "photoUrl": "",
      "userId": "09326440466598086645"
     },
     "user_tz": 240
    },
    "id": "9TFUiGy3OALJ",
    "nbgrader": {
     "checksum": "bd5ed02097252722c11df1fc685e0978",
     "grade": true,
     "grade_id": "word_context_test_1",
     "locked": true,
     "points": 5,
     "schema_version": 1,
     "solution": false
    },
    "outputId": "3ab75437-7ef9-4325-c5e0-50ce0d763a3e"
   },
   "outputs": [],
   "source": [
    "# word_context_test_1 (5 points)\n",
    "titles_wc = word_context(titles_dict, titles, 2)\n",
    "display(titles_wc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 46344,
     "status": "ok",
     "timestamp": 1556892125966,
     "user": {
      "displayName": "Ruo Jia",
      "photoUrl": "",
      "userId": "09326440466598086645"
     },
     "user_tz": 240
    },
    "id": "SKspb6DFzXL8",
    "outputId": "1242361d-14bd-4161-a226-6fa10832cd08"
   },
   "outputs": [],
   "source": [
    "titles_dict[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6SXGrIIS4KVU"
   },
   "outputs": [],
   "source": [
    "titles = [['human', 'interface', 'computer'],\n",
    "          ['survey', 'user', 'computer', 'system', 'response', 'time'],\n",
    "          ['eps', 'user', 'interface', 'system'],\n",
    "          ['system', 'human', 'system', 'eps'],\n",
    "          ['user', 'response', 'time'],\n",
    "          ['trees'],\n",
    "          ['graph', 'trees'],\n",
    "          ['graph', 'minors', 'trees'],\n",
    "          ['graph', 'minors', 'survey']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 221
    },
    "colab_type": "code",
    "deletable": false,
    "editable": false,
    "executionInfo": {
     "elapsed": 46328,
     "status": "ok",
     "timestamp": 1556892125970,
     "user": {
      "displayName": "Ruo Jia",
      "photoUrl": "",
      "userId": "09326440466598086645"
     },
     "user_tz": 240
    },
    "id": "7_68E0egOALN",
    "nbgrader": {
     "checksum": "8bac96367ebf9cfdbcc2afa60d886871",
     "grade": true,
     "grade_id": "word_context_test_2",
     "locked": true,
     "points": 5,
     "schema_version": 1,
     "solution": false
    },
    "outputId": "519cf36d-a3b4-40ec-9a66-bfe13a432d5d"
   },
   "outputs": [],
   "source": [
    "# word_context_test_2 (5 points)\n",
    "display(densify(titles_wc, len(titles_dict))) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "deletable": false,
    "editable": false,
    "id": "5ZWgdQJPOALQ",
    "nbgrader": {
     "checksum": "50515f2d347b02886c1c84becb5f5e89",
     "grade": false,
     "grade_id": "sppmi_spec",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### Step 1.4: Enhancing the word-context matrix\n",
    "\n",
    "In lecture, we saw a number of possible enhancements for word-context matrices. Write a function `sppmi` that takes as input:\n",
    "\n",
    "1. a sparse word-context matrix in the format of `titles_wc` above\n",
    "3. a counts dictionary\n",
    "4. a float `logk`\n",
    "\n",
    "and returns a new sparse word-context matrix with the values in the matrix replaced by shifted positive pointwise mutual informations ($SPPMI$). The formula is:\n",
    "\n",
    "$$SPPMI = \\max(\\log (\\frac{\\#(w,c) |D|}{\\#(w)\\#(c)}) - \\log(k), 0)$$\n",
    "\n",
    "where $\\#(w,c)$ is the count of word $w$ in context $c$ (the original count from the last section), $\\#(w)$ is the count of word $w$, $\\#(c)$ is the count of word $c$ (both come from `titles_counts`), $|D|$ is the length of the corpus, and $k$ is a free hyperparameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "deletable": false,
    "id": "9eNwWzu1U-_d",
    "nbgrader": {
     "checksum": "7428dbc006f33f9b300da79342cde4b1",
     "grade": false,
     "grade_id": "sppmi_answer",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# sppmi_answer\n",
    "def sppmi(gswc, counts, logk):\n",
    "  D = sum(counts)\n",
    "  size = len(counts)\n",
    "  sppmi = np.zeros((size, size))\n",
    "  row = 0\n",
    "  sparse_mi = []\n",
    "  for doc in gswc:\n",
    "    row_mi = []\n",
    "    for (i,j) in doc:\n",
    "      sppmi = max(np.log(j*D/counts[i]/counts[row]) - logk, 0)\n",
    "      if sppmi !=0:\n",
    "        row_mi.append((i,sppmi))\n",
    "    row = row + 1\n",
    "    sparse_mi.append(row_mi)\n",
    "  return sparse_mi\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "deletable": false,
    "editable": false,
    "id": "-wM4bRMeOALW",
    "nbgrader": {
     "checksum": "02b45efb445c4160b708bfbbac3b2157",
     "grade": true,
     "grade_id": "sppmi_test_1",
     "locked": true,
     "points": 5,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# sppmi_test_1 (5 points)\n",
    "titles_sppmi = sppmi(titles_wc, titles_counts, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 221
    },
    "colab_type": "code",
    "deletable": false,
    "editable": false,
    "executionInfo": {
     "elapsed": 46299,
     "status": "ok",
     "timestamp": 1556892125981,
     "user": {
      "displayName": "Ruo Jia",
      "photoUrl": "",
      "userId": "09326440466598086645"
     },
     "user_tz": 240
    },
    "id": "5Q70Zcb_OALa",
    "nbgrader": {
     "checksum": "a3dd9c1002950de76e4fb622be7dbdcd",
     "grade": true,
     "grade_id": "sppmi_test_2",
     "locked": true,
     "points": 5,
     "schema_version": 1,
     "solution": false
    },
    "outputId": "4036aad2-78c7-4704-f306-4464d9d4eef5"
   },
   "outputs": [],
   "source": [
    "# sppmi_test_1 (5 points)\n",
    "titles_dense_sppmi = densify(titles_sppmi, len(titles_dict))\n",
    "display(titles_dense_sppmi.round(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "deletable": false,
    "editable": false,
    "id": "Om9g0fHOOALe",
    "nbgrader": {
     "checksum": "6a70313249b7c2055b596e4c4d919818",
     "grade": false,
     "grade_id": "reviews_sppmi",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "The cell below converts the counts to $SPPMI$s in the word-context matrix for the 100,000 Amazon reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Iu-ohKv1OALf"
   },
   "outputs": [],
   "source": [
    "reviews_sppmi = sppmi(reviews_wc, reviews_counts, np.log(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "deletable": false,
    "editable": false,
    "id": "xIZUQ5YMOALj",
    "nbgrader": {
     "checksum": "75d2c0acac0039bbab6914deda0e59af",
     "grade": false,
     "grade_id": "reconstruction_spec",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### Step 1.5: Sparse SVD/PCA/LSA/LSI\n",
    "\n",
    "One of the greatest benefits of gensim is that there is no need to densify the matrix before decomposing. Indeed, they post some impressive numbers about their SVD speed [here](https://radimrehurek.com/gensim/models/lsimodel.html). But for now, we will test it just on our toy dataset. In the cell below, write a function called `reconstruction` that takes as input:\n",
    "\n",
    "1. a sparse matrix\n",
    "2. a gensim dictionary\n",
    "2. a cutoff for PCA\n",
    "\n",
    "The function should compute the norm of the difference of the reconstructed matrix and the original, and return that dividing by the norm of the original.\n",
    "\n",
    "Hint: The right singular vectors ($V$ or `model[sparse]`) already contain the singular values ($S$ or `model.projection.s`) so don't include them again!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "deletable": false,
    "id": "D1FyfW7pOALm",
    "nbgrader": {
     "checksum": "c1f3939222b67f7f87703d74283fec27",
     "grade": false,
     "grade_id": "reconstruction_answer",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# reconstruction_answer\n",
    "def reconstruction(sparse, gsdict, cutoff):\n",
    "  model = LsiModel(sparse, id2word = gsdict, num_topics = cutoff)\n",
    "  V = model[sparse]\n",
    "  U = model.projection.u\n",
    "  dense_V = densify(V, cutoff).transpose()\n",
    "  dense = densify(sparse, len(gsdict)).transpose()\n",
    "  #print(dense.shape)\n",
    "  #recon = U[:,0:cutoff-1].dot(dense_V[0:cutoff-1, :])\n",
    "  recon = U.dot(dense_V)\n",
    "  norm_re = np.linalg.norm(recon - dense)\n",
    "  norm_or = np.linalg.norm(dense)\n",
    "  return norm_re/norm_or\n",
    "  \n",
    "\n",
    "cutoff = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "deletable": false,
    "editable": false,
    "executionInfo": {
     "elapsed": 69832,
     "status": "ok",
     "timestamp": 1556892149553,
     "user": {
      "displayName": "Ruo Jia",
      "photoUrl": "",
      "userId": "09326440466598086645"
     },
     "user_tz": 240
    },
    "id": "oXb64Yf4OALt",
    "nbgrader": {
     "checksum": "1112c8fb4b2e6a0b4205691e5c55ba65",
     "grade": true,
     "grade_id": "reconstruction_test_1",
     "locked": true,
     "points": 5,
     "schema_version": 1,
     "solution": false
    },
    "outputId": "6a2f97c3-d55f-41a1-8bf4-0350f608226b"
   },
   "outputs": [],
   "source": [
    "# reconstruction_test_1 (5 points)\n",
    "\n",
    "error = reconstruction(titles_bow, titles_dict, cutoff)\n",
    "print(\"The reconstruction error with\", cutoff, \"components on the the toy dataset is\", error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "deletable": false,
    "editable": false,
    "executionInfo": {
     "elapsed": 69813,
     "status": "ok",
     "timestamp": 1556892149555,
     "user": {
      "displayName": "Ruo Jia",
      "photoUrl": "",
      "userId": "09326440466598086645"
     },
     "user_tz": 240
    },
    "id": "hsf6bDd1OALw",
    "nbgrader": {
     "checksum": "11729a2ee9ba74ddbe8f0429ed95f586",
     "grade": true,
     "grade_id": "reconstruction_test_2",
     "locked": true,
     "points": 5,
     "schema_version": 1,
     "solution": false
    },
    "outputId": "787c5124-02b5-435e-d057-4ca15fa7d43f"
   },
   "outputs": [],
   "source": [
    "# reconstruction_test_2 (5 points)\n",
    "cutoff = 9\n",
    "error = reconstruction(titles_bow, titles_dict, cutoff)\n",
    "print(\"The reconstruction error with\", cutoff, \"components on the the toy dataset is\", error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "deletable": false,
    "editable": false,
    "id": "S_ZAH23oOALz",
    "nbgrader": {
     "checksum": "e8c9251026ed314e8d2a64ab0b8091f8",
     "grade": false,
     "grade_id": "vec2doc_spec",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### Step 1.6: Assembling the dense representation\n",
    "\n",
    "The last step in this process is to combine our two datasets, namely the document-term matrix and the word-context matrix. The appeal is that rather than reducing dimensionality by choosing a very small vocabulary, we can select relevant features from a vector space that contains all of the words. However, this requires a heavy assumption: a document representation is the sum of the representations of its words. This does not allow any non-compositionality of language (which we know there is some) and it makes each word equally important (which arguably is false as well).\n",
    "\n",
    "In the cell below, write a function `vec2doc` that takes as input:\n",
    "\n",
    "1. a bag of words corpus in the format of `titles_bow` above\n",
    "2. a list of word vectors in proper order (by dictionary index) like `reviews_wv` above\n",
    "\n",
    "and returns a dense matrix. This matrix contains one vector per document which was computed by summing all of the vectors corresponding to the words in the document (including repeats)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "deletable": false,
    "id": "hfMPVMOcOAL0",
    "nbgrader": {
     "checksum": "15a754f1efecf926e907031cebfbc320",
     "grade": false,
     "grade_id": "vec2doc_answer",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# vec2doc_answer\n",
    "def vec2doc(gsbow, vectors):\n",
    "  return_vecs = []\n",
    "  for doc in gsbow:\n",
    "    sum = np.zeros(300)\n",
    "    for (i,j) in doc:\n",
    "      sum = sum + j * vectors[i]\n",
    "    return_vecs.append(sum)\n",
    "  return np.array(return_vecs)\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "deletable": false,
    "editable": false,
    "id": "AdcgLyDFOAL3",
    "nbgrader": {
     "checksum": "98e0a39fef48b1d3f839221018dc1591",
     "grade": true,
     "grade_id": "vec2doc_test_1",
     "locked": true,
     "points": 5,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# vec2doc_test_1 (5 points)\n",
    "vecs_sppmi = vec2doc(reviews_bow, reviews_sppmi_300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "deletable": false,
    "editable": false,
    "executionInfo": {
     "elapsed": 79433,
     "status": "ok",
     "timestamp": 1556892159220,
     "user": {
      "displayName": "Ruo Jia",
      "photoUrl": "",
      "userId": "09326440466598086645"
     },
     "user_tz": 240
    },
    "id": "ISeBpakoOAL5",
    "nbgrader": {
     "checksum": "1b422a5e8098c575116d5ad28596bfb2",
     "grade": true,
     "grade_id": "vec2doc_test_2",
     "locked": true,
     "points": 5,
     "schema_version": 1,
     "solution": false
    },
    "outputId": "eb42e54e-1d5f-4834-efc5-37e40cde3897"
   },
   "outputs": [],
   "source": [
    "# vec2doc_test_2 (5 points)\n",
    "display(vecs_sppmi.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "deletable": false,
    "editable": false,
    "id": "aQ22dsr7OAL8",
    "nbgrader": {
     "checksum": "c35e1c149734511299758f933b295b12",
     "grade": false,
     "grade_id": "use_dict_spec",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "## Step 2: Feature selection and hyperparameter tuning\n",
    "\n",
    "By this point, you have already written most of the code to import and use the word embeddings made by `word2vec`. Once both are loaded and the hyperparameters are set, you are ready to compare them against the bag of words baseline on the classification task.\n",
    "\n",
    "### Step 2.1: Assembling the other dense representation\n",
    "\n",
    "The only small structural difference between the word vectors that you computed and the `word2vec` embeddings is that your word vectors use a vocabulary/dictionary for indexing, whereas `word2vec` vectors are accessed using a Python dictionary. So, to access the vector for the word \"good\" you would need to use the integer `good` in `vecs_sppmi[good]` and the string 'good' in `reviews_wv['good']`.\n",
    "\n",
    "Since string lookups can be inefficient and so you may maximally reuse your code, write a function `use_dict` that takes as input:\n",
    "\n",
    "1. a dictionary in the format of `titles_dict` above\n",
    "2. `KeyedVector word2vec` embeddings, as in `reviews_wv`\n",
    "\n",
    "and returns a list of vectors indexed using the input dictionary.\n",
    "\n",
    "In the unlikely case that you are missing a vector, just set the value at that index to the integer `0`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 79412,
     "status": "ok",
     "timestamp": 1556892159223,
     "user": {
      "displayName": "Ruo Jia",
      "photoUrl": "",
      "userId": "09326440466598086645"
     },
     "user_tz": 240
    },
    "id": "LI4QPYhleV5z",
    "outputId": "c2f16096-503d-4f17-b738-c2cf3be03efd"
   },
   "outputs": [],
   "source": [
    "len(reviews_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "deletable": false,
    "id": "s26JW1yIOAL8",
    "nbgrader": {
     "checksum": "af6c283259a55ba1e9ec06dbd791175f",
     "grade": false,
     "grade_id": "use_dict_answer",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# use_dict_answer\n",
    "def use_dict(gsdict, vectors):\n",
    "  vecs = []\n",
    "  for i in range(len(gsdict)):\n",
    "    word = gsdict[i]\n",
    "    if word not in vectors.vocab:\n",
    "      vecs.append(np.zeros(300))\n",
    "    else:\n",
    "      vecs.append(vectors[word])\n",
    "  return np.array(vecs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "deletable": false,
    "editable": false,
    "id": "tQb8_vHkOAL_",
    "nbgrader": {
     "checksum": "20857020c959d177902bc5d63a18926f",
     "grade": true,
     "grade_id": "use_dict_test",
     "locked": true,
     "points": 5,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# use_dict_test (5 points)\n",
    "reviews_sgns_300 = use_dict(reviews_dict, reviews_wv)\n",
    "vecs_sgns = vec2doc(reviews_bow, reviews_sgns_300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "deletable": false,
    "editable": false,
    "id": "u0D5p-gBOAMB",
    "nbgrader": {
     "checksum": "0fd0d8ec73dd139cae3a2f660c4a71de",
     "grade": false,
     "grade_id": "pca_bow_spec",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### Step 2.2: PCA on pure BOW\n",
    "\n",
    "While the $SPPMI$ and $SGNS$ models are already dimensionality reduced, it would be a good idea to also run PCA on the $BOW$ baseline. Train a gensim `LsiModel` on `reviews_bow` using `reviews_dict` as the dictionary and 1000 components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "deletable": false,
    "id": "-rXaAyPa5Zqp",
    "nbgrader": {
     "checksum": "96932268e3cb047c3b15377b81ec59bd",
     "grade": false,
     "grade_id": "pca_bow_answer",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# pca_bow_answer\n",
    "max_cutoff = 1000\n",
    "model =  LsiModel(reviews_bow, id2word = reviews_dict, num_topics = max_cutoff)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "deletable": false,
    "editable": false,
    "id": "2EsMep1jOAME",
    "nbgrader": {
     "checksum": "4c78bee8ef6c21d2cdc2946c9ef10b26",
     "grade": false,
     "grade_id": "pca_bow_var_plot_spec",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "Once it is finished, use the code below to plot the explained variance versus number of components. You just need to pass in a list of singular values (no `diag` necessary)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 295
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 349840,
     "status": "ok",
     "timestamp": 1556892429675,
     "user": {
      "displayName": "Ruo Jia",
      "photoUrl": "",
      "userId": "09326440466598086645"
     },
     "user_tz": 240
    },
    "id": "UC9vD7tG7XxH",
    "outputId": "9f410bfd-2b30-41a0-d2dd-b5005beb16fb"
   },
   "outputs": [],
   "source": [
    "def plot_variance_vs_features(singular_values, cutoff):\n",
    "    evr = np.array([singular_values[i]**2 / sum(singular_values**2) for i in range(cutoff)])\n",
    "    var = np.cumsum(evr*100)\n",
    "    plt.ylabel('% Variance Explained')\n",
    "    plt.xlabel('# of Features')\n",
    "    plt.title('PCA Analysis')\n",
    "    plt.style.context('seaborn-whitegrid')\n",
    "    plt.plot(var)\n",
    "    \n",
    "plot_variance_vs_features(model.projection.s, max_cutoff)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "deletable": false,
    "editable": false,
    "id": "1Bo8_yQdOAMK",
    "nbgrader": {
     "checksum": "97cb6c09ccedbcef1593ce66f433d8ba",
     "grade": false,
     "grade_id": "pca_bow_var_plot_analysis",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "The good news is this curve is very steep in the beginning, which shows that a lot of information is conveyed in the first components. However, there is no plateau that we can use to choose a cutoff!\n",
    "\n",
    "**So, instead, we will break off a validaton set and use classifier performance to tune this hyperparameter.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VF12IS8EOAML"
   },
   "outputs": [],
   "source": [
    "def evaluate_model(X, review_times, y):\n",
    "    X = np.hstack((X, review_times))\n",
    "    X_train, X_test, y_train, y_test = ms.train_test_split(X, y, test_size=0.2, random_state = 195)\n",
    "    rfor = RandomForestClassifier(n_estimators=51, random_state=195)\n",
    "    rfor.fit(X_train, y_train)\n",
    "    return rfor.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "deletable": false,
    "editable": false,
    "id": "pmIIfGvkOAMO",
    "nbgrader": {
     "checksum": "04e21c4ce2cf078ca2bee2f5575553b0",
     "grade": false,
     "grade_id": "baseline_tuning_spec",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### Step 2.3: Choosing the number of components via the downstream task\n",
    "\n",
    "In some ways, this method of choosing the number of components is even better than the plateau method, because we are optimizing directly on the machine learning task rather than something intrinsic to the dataset. In the cell below, call `evaluate model` on models between 10 and 30 PCA components. For each model, you will need to train an `LsiModel`, compute the $V$ matrix (right singular vectors), call `densify` on that, and pass the dense matrix to evaluate model. Store all of your accuracies in a list named results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "deletable": false,
    "id": "uhKPMGF1OAMO",
    "nbgrader": {
     "checksum": "37d29ce8a8461c0ab03fb7994b4d33f0",
     "grade": false,
     "grade_id": "baseline_tuning_answer",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# # baseline_tuning_answer\n",
    "accuracy = []\n",
    "for i in range(10,31):\n",
    "  model =  LsiModel(reviews_bow, id2word = reviews_dict, num_topics = i)\n",
    "  V = model[reviews_bow]\n",
    "  dense_V = densify(V, i)\n",
    "  acc = evaluate_model(dense_V, reviews_times, y)\n",
    "  accuracy.append(acc)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NXPB8hduCOO1"
   },
   "outputs": [],
   "source": [
    "#accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 370
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 265,
     "status": "ok",
     "timestamp": 1556895779449,
     "user": {
      "displayName": "Ruo Jia",
      "photoUrl": "",
      "userId": "09326440466598086645"
     },
     "user_tz": 240
    },
    "id": "d-ZgrSBBGLkM",
    "outputId": "264552eb-0350-40dd-9f15-cf023a7764c9"
   },
   "outputs": [],
   "source": [
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7h9xkzvfFVP2"
   },
   "outputs": [],
   "source": [
    "# dense_V.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "p0BzmKXSEYcu"
   },
   "outputs": [],
   "source": [
    "# U = model.projection.u\n",
    "# U.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "deletable": false,
    "editable": false,
    "id": "Ne3Wx_BQOAMR",
    "nbgrader": {
     "checksum": "def3777cddbd039a8e5966aadbeac413",
     "grade": true,
     "grade_id": "baseline_tuning_test",
     "locked": true,
     "points": 5,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# baseline_tuning_test (5 points)\n",
    "# display(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "deletable": false,
    "editable": false,
    "executionInfo": {
     "elapsed": 265,
     "status": "ok",
     "timestamp": 1556895779464,
     "user": {
      "displayName": "Ruo Jia",
      "photoUrl": "",
      "userId": "09326440466598086645"
     },
     "user_tz": 240
    },
    "id": "R_rQTSamOAMY",
    "nbgrader": {
     "checksum": "83b6d38076f108ee9b323b6c99243407",
     "grade": true,
     "grade_id": "final_sppmi_test",
     "locked": true,
     "points": 5,
     "schema_version": 1,
     "solution": false
    },
    "outputId": "89a415b2-5c8a-4c7a-8d5d-2354cdc7d031"
   },
   "outputs": [],
   "source": [
    "# final_sppmi_test (5 points)\n",
    "final_sppmi = evaluate_model(vecs_sppmi, reviews_times, y)\n",
    "print(final_sppmi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "deletable": false,
    "editable": false,
    "executionInfo": {
     "elapsed": 267,
     "status": "ok",
     "timestamp": 1556895779468,
     "user": {
      "displayName": "Ruo Jia",
      "photoUrl": "",
      "userId": "09326440466598086645"
     },
     "user_tz": 240
    },
    "id": "sHjASduwOAMg",
    "nbgrader": {
     "checksum": "f6dc570e5130d32f2126e616cbe5b9e1",
     "grade": true,
     "grade_id": "final_sgns_test",
     "locked": true,
     "points": 5,
     "schema_version": 1,
     "solution": false
    },
    "outputId": "b5948c51-8c73-4ae5-e988-3940c18aef0c"
   },
   "outputs": [],
   "source": [
    "# final_sgns_test (5 points)\n",
    "final_sgns = evaluate_model(vecs_sgns, reviews_times, y)\n",
    "print(final_sgns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vSOoRxN2JSpH"
   },
   "source": [
    "## Step 3: In-class Kaggle competition\n",
    "\n",
    "To conclude this homework and this class, we are opening up to a more flexible system. We would like for you to use everything you have learned in CIS 545 to make the best star rating predictor possible. For grading purposes, we are going to check (manually) whether you thought about and worked on each of the subsections that follow. These sections will be pass/fail and the questions under each of the headlines are meant to help you brainstorm, not impose requirements.\n",
    "\n",
    "**Beyond that, a small amount of extra credit will be awarded to the students with the best performing models. We should be able to maintain an (anonymous) leaderboard on Piazza. Scores on this leaderboard will correspond to performance on our test set, which has not been released to you. Therefore, our TAs will run your models on our test data and report the results to ensure fairness.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "deletable": false,
    "editable": false,
    "id": "-TxhH4UIOAMn",
    "nbgrader": {
     "checksum": "d3408498aa1c5caade08bc9d90fbc2b1",
     "grade": false,
     "grade_id": "scaling_spec",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### Step 3.1: Scaling\n",
    "\n",
    "What is the range of values for each feature? Are they comparable? Do some dominate others? Try minmax, standard, log, exponential, binary, or thresholding. How are predicted 1 star rating and predicted 5 star ratings different? How are actual 1 star ratings and actual 5 star ratings different? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JXvC2ElY7zbC"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler,StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 271,
     "status": "ok",
     "timestamp": 1556895779478,
     "user": {
      "displayName": "Ruo Jia",
      "photoUrl": "",
      "userId": "09326440466598086645"
     },
     "user_tz": 240
    },
    "id": "NTef-G3l8dT_",
    "outputId": "6862e15a-6a3a-4306-9130-9219f9f9fee9"
   },
   "outputs": [],
   "source": [
    "#range of feature value of second feature\n",
    "max(vecs_sgns[:,1]) - min(vecs_sgns[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 272,
     "status": "ok",
     "timestamp": 1556895779483,
     "user": {
      "displayName": "Ruo Jia",
      "photoUrl": "",
      "userId": "09326440466598086645"
     },
     "user_tz": 240
    },
    "id": "ziW9HuRw3kGY",
    "outputId": "4c73f486-65ec-40dc-e407-980434b7ff28"
   },
   "outputs": [],
   "source": [
    "#did a few trials on difference in feature values across ratings\n",
    "np.mean(vecs_sgns[0+20000:2000+20000, 0] - vecs_sgns[2000:4000, 0]) # difference of first feature in 1 star rating(same star rating)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 277,
     "status": "ok",
     "timestamp": 1556895779490,
     "user": {
      "displayName": "Ruo Jia",
      "photoUrl": "",
      "userId": "09326440466598086645"
     },
     "user_tz": 240
    },
    "id": "TED3NmUj37Bd",
    "outputId": "908a9c16-c2bb-4987-cefc-620d2e0bf89d"
   },
   "outputs": [],
   "source": [
    "np.mean(vecs_sgns[0:4000, 0] - vecs_sgns[4000:8000, 0]) # difference of first feature between 1 star and 2 star rating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 281,
     "status": "ok",
     "timestamp": 1556895779497,
     "user": {
      "displayName": "Ruo Jia",
      "photoUrl": "",
      "userId": "09326440466598086645"
     },
     "user_tz": 240
    },
    "id": "0KKMcAbQ4AKB",
    "outputId": "25987d86-15a6-450e-9c5e-ecc2f794a1b8"
   },
   "outputs": [],
   "source": [
    "np.mean(vecs_sgns[0:4000, 0] - vecs_sgns[16000:20000, 0]) # difference of first feature between 1 star and 5 star rating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "669up3jtPgHj"
   },
   "outputs": [],
   "source": [
    "#I also did trials on other features. \n",
    "#The results show that feature difference within the same rating would be smaller than feature difference between different star ratings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TISj_Nf_tKPc"
   },
   "outputs": [],
   "source": [
    "#try minmax scaler\n",
    "def minmax(col):\n",
    "  scaler = MinMaxScaler()\n",
    "  new_col = scaler.fit_transform(col)\n",
    "  return new_col.reshape(-1)\n",
    "#try standard scaler\n",
    "def standard(col):\n",
    "  scaler = StandardScaler()\n",
    "  new_col = scaler.fit_transform(col)\n",
    "  return new_col.reshape(-1)\n",
    "#save copies of scaled variables\n",
    "vecs_sgns_c = vecs_sgns.copy()\n",
    "vecs_sppmi_c = vecs_sppmi.copy()\n",
    "vecs_sgns_c2 = vecs_sgns.copy()\n",
    "vecs_sppmi_c2 = vecs_sppmi.copy()\n",
    "for i in range(vecs_sppmi_c.shape[1]):\n",
    "  vecs_sgns_c[:,i] = minmax(vecs_sgns[:,i].reshape(-1,1))  #apply scaler to each feature\n",
    "  vecs_sppmi_c[:,i] = minmax(vecs_sppmi[:,i].reshape(-1,1)) #apply scaler to each feature\n",
    "  vecs_sgns_c2[:,i] = standard(vecs_sgns[:,i].reshape(-1,1)) #apply scaler to each feature\n",
    "  vecs_sppmi_c2[:,i] = standard(vecs_sppmi[:,i].reshape(-1,1)) #apply scaler to each feature\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YdnUAU6D80sc"
   },
   "outputs": [],
   "source": [
    "#minmax scaler results\n",
    "final_sgns_scale = evaluate_model(vecs_sgns_c, reviews_times, y)\n",
    "final_sppmi_scale = evaluate_model(vecs_sppmi_c, reviews_times, y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9UQx5uCo1H8b"
   },
   "outputs": [],
   "source": [
    "#standard scaler results\n",
    "final_sgns_scale2 = evaluate_model(vecs_sgns_c2, reviews_times, y)\n",
    "final_sppmi_scale2 = evaluate_model(vecs_sppmi_c2, reviews_times, y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 50
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 288,
     "status": "ok",
     "timestamp": 1556895779519,
     "user": {
      "displayName": "Ruo Jia",
      "photoUrl": "",
      "userId": "09326440466598086645"
     },
     "user_tz": 240
    },
    "id": "VN0Y4dKk9rNo",
    "outputId": "583d8e58-674c-44ac-ad50-e7ab59aea9ba"
   },
   "outputs": [],
   "source": [
    "print(final_sgns_scale)\n",
    "print(final_sppmi_scale)\n",
    "#results using minmax scaler show that sppmi matrix works better on random forest model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 50
    },
    "colab_type": "code",
    "deletable": false,
    "executionInfo": {
     "elapsed": 287,
     "status": "ok",
     "timestamp": 1556895779522,
     "user": {
      "displayName": "Ruo Jia",
      "photoUrl": "",
      "userId": "09326440466598086645"
     },
     "user_tz": 240
    },
    "id": "JWQae-ruOAMp",
    "nbgrader": {
     "checksum": "c54727f2f28132c47b590ba75b6a3698",
     "grade": true,
     "grade_id": "scaling",
     "locked": false,
     "points": 15,
     "schema_version": 1,
     "solution": true
    },
    "outputId": "3c287378-0fc0-4cc7-ce38-e2edfdddbc59"
   },
   "outputs": [],
   "source": [
    "print(final_sgns_scale2)\n",
    "print(final_sppmi_scale2)\n",
    "#results using standard scaler also show that sppmi matrix works better on random forest model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "deletable": false,
    "editable": false,
    "id": "w7bH89F3OAMs",
    "nbgrader": {
     "checksum": "9fc0a48c94876daac660f4fe9b186af0",
     "grade": false,
     "grade_id": "clustering_spec",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### Step 3.2: Clustering\n",
    "\n",
    "What is the distribution of ratings? How else might this dataset be artificially balanced? What patterns can you find? Are there detectable clusters? Are some ratings easier to classify than others?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 269
    },
    "colab_type": "code",
    "deletable": false,
    "executionInfo": {
     "elapsed": 38287,
     "status": "ok",
     "timestamp": 1556895943049,
     "user": {
      "displayName": "Ruo Jia",
      "photoUrl": "",
      "userId": "09326440466598086645"
     },
     "user_tz": 240
    },
    "id": "_S2m-ZZ1OAMt",
    "nbgrader": {
     "checksum": "8ed12980942f15e0f230455f51b47565",
     "grade": true,
     "grade_id": "clustering",
     "locked": false,
     "points": 15,
     "schema_version": 1,
     "solution": true
    },
    "outputId": "4002f17f-b41a-4d89-c1ec-b4f14a0e2d97"
   },
   "outputs": [],
   "source": [
    "#kmeans, set n_clusters =5, corresponding to star 1 - 5 rating\n",
    "from sklearn.cluster import KMeans\n",
    "kmeans = KMeans(n_clusters=5)\n",
    "pca_sppmi = PCA(n_components=2).fit_transform(vecs_sppmi)\n",
    "pca_sgns = PCA(n_components=2).fit_transform(vecs_sgns)\n",
    "kmeans.fit(pca_sppmi)\n",
    "y_kmeans_sppmi = kmeans.predict(pca_sppmi)\n",
    "plt.scatter(pca_sppmi[:, 0], pca_sppmi[:, 1], c=y_kmeans_sppmi, s=50, cmap='viridis')\n",
    "centers = kmeans.cluster_centers_\n",
    "plt.scatter(centers[:, 0], centers[:, 1], c='black', s=200, alpha=0.5);\n",
    "##################\n",
    "plt.figure()\n",
    "kmeans.fit(pca_sgns)\n",
    "y_kmeans_sgns = kmeans.predict(pca_sgns)\n",
    "plt.scatter(pca_sgns[:, 0], pca_sgns[:, 1], c=y_kmeans_sgns, s=50, cmap='viridis')\n",
    "centers = kmeans.cluster_centers_\n",
    "plt.scatter(centers[:, 0], centers[:, 1], c='black', s=200, alpha=0.5);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#By doing PCA on vecs_sppmi and vec_sgns and pick top 2 PCs, we can see that there are clear clusters.\n",
    "#However, it doesn't mean that in each cluster, the ratings are the same. So I did some checks on the majority \n",
    "#cluster of each rating. Cluster numbers are in range(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 33564,
     "status": "ok",
     "timestamp": 1556895943051,
     "user": {
      "displayName": "Ruo Jia",
      "photoUrl": "",
      "userId": "09326440466598086645"
     },
     "user_tz": 240
    },
    "id": "4ZQvFqcn6mha",
    "outputId": "c34959b4-1503-44d8-f431-53cb7dc5b9c5"
   },
   "outputs": [],
   "source": [
    "np.argmax([list(y_kmeans_sppmi[0:4000]).count(i) for i in range(5)]) # majority for rate 1 is in this cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 32576,
     "status": "ok",
     "timestamp": 1556895943053,
     "user": {
      "displayName": "Ruo Jia",
      "photoUrl": "",
      "userId": "09326440466598086645"
     },
     "user_tz": 240
    },
    "id": "aDpe_9p_BKIy",
    "outputId": "50609f06-11fd-464f-81d0-1cd40e73f012"
   },
   "outputs": [],
   "source": [
    "np.argmax([list(y_kmeans_sppmi[0+4000:4000+4000]).count(i) for i in range(5)])# majority for rate 2 is also in this cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 31643,
     "status": "ok",
     "timestamp": 1556895943054,
     "user": {
      "displayName": "Ruo Jia",
      "photoUrl": "",
      "userId": "09326440466598086645"
     },
     "user_tz": 240
    },
    "id": "vFMd-JPTFTUN",
    "outputId": "2d88e38c-7561-4e9e-d72d-890647ae6341"
   },
   "outputs": [],
   "source": [
    "np.argmax([list(y_kmeans_sppmi[0+16000:4000+16000]).count(i) for i in range(5)]) #so does rate 5, so there is no detectable clusters when using vecs_sppmi or vecs_sgns directly sa features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NgW3NfMLP7gH"
   },
   "outputs": [],
   "source": [
    "# By doing kmeans only using embedding features, I observed that reviews of all star ratings will tend to \n",
    "# cluster at the same cluster. So there is no clear detectable clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fmMXDeHYG2Wu"
   },
   "outputs": [],
   "source": [
    "def evaluate_kmeans_model(X, review_times, y):\n",
    "    X = np.hstack((X, review_times))\n",
    "    kmeans = KMeans(n_clusters=5)\n",
    "    X_train, X_test, y_train, y_test = ms.train_test_split(X, y, test_size=0.2, random_state = 195)\n",
    "    kmeans.fit(X_train, y_train)\n",
    "    return kmeans.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SorArJBaHKJ_"
   },
   "outputs": [],
   "source": [
    "#By purely conducted k-means, the accuracy score is really bad.\n",
    "kmeans_score = evaluate_kmeans_model(pca_sppmi, reviews_times, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 47826,
     "status": "ok",
     "timestamp": 1556895963235,
     "user": {
      "displayName": "Ruo Jia",
      "photoUrl": "",
      "userId": "09326440466598086645"
     },
     "user_tz": 240
    },
    "id": "yYV22TVrIL5b",
    "outputId": "8754e70d-cafd-4e4a-f779-044776e4c833"
   },
   "outputs": [],
   "source": [
    "kmeans_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "deletable": false,
    "editable": false,
    "id": "5Rm8ACA0OAMw",
    "nbgrader": {
     "checksum": "74b55f7e2f5783bc5edf0acb9d5e01cd",
     "grade": false,
     "grade_id": "regression_spec",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### Step 3.3: Regression\n",
    "\n",
    "In the baseline models, star ratings were treated as categorical variables. So the models had no sense that a rating of 4 stars is in between ratings of 3 stars and 5 stars. What are the advantages and disadvantages of that approach? If you reformulate the problem as regression, how do you measure success? Is that way of measuring fair to the classifiers?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "deletable": false,
    "id": "RcxiT1d-OAMx",
    "nbgrader": {
     "checksum": "9c225f9b0c5fee5831baadad3b20c727",
     "grade": true,
     "grade_id": "regression",
     "locked": false,
     "points": 15,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# linear regression model with regularization\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn import linear_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "m1S-tth4GsMk"
   },
   "outputs": [],
   "source": [
    "def evaluate_reg_model(X, review_times, y):\n",
    "    X = np.hstack((X, review_times))\n",
    "    X_train, X_test, y_train, y_test = ms.train_test_split(X, y, test_size=0.2, random_state = 195)\n",
    "    reg = linear_model.Ridge(alpha=0.1).fit(X_train, y_train)  #adding regularization term to reduce over-fitting\n",
    "    return reg.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "u3te3epdLU6E"
   },
   "outputs": [],
   "source": [
    "reg_score = evaluate_reg_model(vecs_sppmi, reviews_times, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 42532,
     "status": "ok",
     "timestamp": 1556895964039,
     "user": {
      "displayName": "Ruo Jia",
      "photoUrl": "",
      "userId": "09326440466598086645"
     },
     "user_tz": 240
    },
    "id": "MqqmXDqoEVQa",
    "outputId": "ddc81c00-cffb-48eb-9488-56a6ad6425a6"
   },
   "outputs": [],
   "source": [
    "reg_score # accuracy score using vecs_sppmi is 0.3638"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 87
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 42616,
     "status": "ok",
     "timestamp": 1556895964825,
     "user": {
      "displayName": "Ruo Jia",
      "photoUrl": "",
      "userId": "09326440466598086645"
     },
     "user_tz": 240
    },
    "id": "tpV_jWYNQ31A",
    "outputId": "9db0a162-a249-4324-8af7-5bd3b2b9286d"
   },
   "outputs": [],
   "source": [
    "reg_score = evaluate_reg_model(vecs_sgns, reviews_times, y)\n",
    "print(reg_score) # accuracy score using vecs_sgns is 0.3476"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bXbKGGcBLa83"
   },
   "outputs": [],
   "source": [
    "# linear regression with penality still doesn't have a good performance on categorical variable classification.\n",
    "# The advantage of using regression is to break the hard wall among different ratings. \n",
    "# But since the predicted labels are still categorical and deterministic, using regression here actually diminish the actual difference between each ratings.\n",
    "# If I can reformulae the problem as regression, I will label y as a 5-column matrix with each column as the probability in each star rating. \n",
    "# Linear regression would perform much better in continuous data, and log loss would be a good metric to judge the performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "deletable": false,
    "editable": false,
    "id": "borR3BTdOAM2",
    "nbgrader": {
     "checksum": "a38d1b324967f26d5407776096a9ca52",
     "grade": false,
     "grade_id": "time_series_spec",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### Step 3.4: Time Series\n",
    "\n",
    "Since real products were rated over time, these star ratings were likely subject to trends. Can these trends be leveraged? What curve would explain the trends without requiring too many parameters? Can you use an autoregressor? Can you use a moving average? Are the data stationary?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-UAU-tqACOQw"
   },
   "outputs": [],
   "source": [
    "import statsmodels\n",
    "from statsmodels.tsa.stattools  import adfuller"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Kc5C1aGSu8zD"
   },
   "outputs": [],
   "source": [
    "#formulate Series data\n",
    "times = pd.DataFrame(converted_times)\n",
    "rating = pd.DataFrame(y)\n",
    "data = pd.concat([times, rating], axis = 1)\n",
    "data.columns = ['times', 'rating']\n",
    "data = data.set_index('times').sort_index()\n",
    "data_series= pd.Series(data.iloc[:, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1911
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 38480,
     "status": "ok",
     "timestamp": 1556895965440,
     "user": {
      "displayName": "Ruo Jia",
      "photoUrl": "",
      "userId": "09326440466598086645"
     },
     "user_tz": 240
    },
    "id": "NTvQ2r9iCORF",
    "outputId": "91c3cf4b-1c9f-44ae-adb4-7d3a1023a4b5",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ye9x6TVeCORP"
   },
   "outputs": [],
   "source": [
    "# calculate rolling mean and standard deviation\n",
    "rolmean = data_series.rolling(window = 12).mean()\n",
    "rolstd = data_series.rolling(window = 12).std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1IP8gP2dCORT"
   },
   "outputs": [],
   "source": [
    "# rolstd = pd.Series(np.nan_to_num(rolstd))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 406
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 37118,
     "status": "ok",
     "timestamp": 1556895966836,
     "user": {
      "displayName": "Ruo Jia",
      "photoUrl": "",
      "userId": "09326440466598086645"
     },
     "user_tz": 240
    },
    "id": "SFwkWsNsCORV",
    "outputId": "dbc2b86a-f899-4a9f-f5aa-9684ab9c5eb8"
   },
   "outputs": [],
   "source": [
    "#plot original data with rolling mean and standard deviation\n",
    "plt.plot(data_series, label ='original')\n",
    "plt.plot(rolmean, color='red', label ='rolling means')\n",
    "plt.plot(rolstd, color = 'black', label = 'rolling std')\n",
    "plt.legend(loc='best')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ho-NU5XrCORX"
   },
   "outputs": [],
   "source": [
    "data_log = np.log(data_series) # take log of the original data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 151
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 67978,
     "status": "ok",
     "timestamp": 1556895999425,
     "user": {
      "displayName": "Ruo Jia",
      "photoUrl": "",
      "userId": "09326440466598086645"
     },
     "user_tz": 240
    },
    "id": "1nI6RDi4CORY",
    "outputId": "15de3f19-d0f5-4543-b531-bb157cee6360"
   },
   "outputs": [],
   "source": [
    "#using dickey-fuller test to examine whether the data is stationary or not. \n",
    "#the resulting p value is zero, which means that the data is stationary.\n",
    "adfuller(data_series)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 151
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 97782,
     "status": "ok",
     "timestamp": 1556896030123,
     "user": {
      "displayName": "Ruo Jia",
      "photoUrl": "",
      "userId": "09326440466598086645"
     },
     "user_tz": 240
    },
    "id": "CpU5oQlvCORa",
    "outputId": "64072667-3d92-46d0-aab9-6a628675b036"
   },
   "outputs": [],
   "source": [
    "#try the test on log data, which also give zero p value, meaning the log data is stationary\n",
    "adfuller(data_log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "deletable": false,
    "id": "djdc-KsOOAM4",
    "nbgrader": {
     "checksum": "58872db77812cb2b8395c5d162bba31f",
     "grade": true,
     "grade_id": "time_series",
     "locked": false,
     "points": 15,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# def evaluate_model(review_times, y):\n",
    "#     #X = np.hstack((X, review_times))\n",
    "    \n",
    "#     #X_train, X_test, y_train, y_test = ms.train_test_split(X, y, test_size=0.2, random_state = 195)\n",
    "#     #rfor = RandomForestClassifier(n_estimators=51, random_state=195)\n",
    "#     #rfor.fit(X_train, y_train)\n",
    "#     return rfor.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "deletable": false,
    "editable": false,
    "id": "l-8yOKexOAM9",
    "nbgrader": {
     "checksum": "1f9b1dfa3c0546987f0bc762831c10a4",
     "grade": false,
     "grade_id": "neural_spec",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### Step 3.5: Neural networks\n",
    "\n",
    "The baseline models were are random forests. What other algorithms could be used? In a neural network, how many layers and nodes? Any convolutions or recurrences? How do you keep the model from overfitting? What regularization is appropriate? Is the task too hard or too easy for a neural network?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SqSMF2kP8Vs2"
   },
   "outputs": [],
   "source": [
    "# To conclude on neural network, it seems that neural network is not very suitable for this task. Since the only\n",
    "# features we used here are embedding features and time feature, adding these to neural network may over-complicate \n",
    "# the problem. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 67
    },
    "colab_type": "code",
    "deletable": false,
    "executionInfo": {
     "elapsed": 111449,
     "status": "ok",
     "timestamp": 1556896046747,
     "user": {
      "displayName": "Ruo Jia",
      "photoUrl": "",
      "userId": "09326440466598086645"
     },
     "user_tz": 240
    },
    "id": "ZjGRi7BHOAM-",
    "nbgrader": {
     "checksum": "e5846c031eecd85f866011905c4682f9",
     "grade": true,
     "grade_id": "neural",
     "locked": false,
     "points": 15,
     "schema_version": 1,
     "solution": true
    },
    "outputId": "29ce7d2e-4222-472a-a5af-e44ff030215f"
   },
   "outputs": [],
   "source": [
    "# use sklearn to train a multi-layer percetron as neural-net baseline\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "def nn_model(X, review_times, y):\n",
    "    X = np.hstack((X, review_times))\n",
    "    X_train, X_test, y_train, y_test = ms.train_test_split(X, y, test_size=0.2, random_state = 195)\n",
    "    nn = MLPClassifier(hidden_layer_sizes=(100, ), activation='relu', solver='adam', \n",
    "                       alpha=0.0001, batch_size='auto', learning_rate='constant', learning_rate_init=0.01, \n",
    "                       power_t=0.5, max_iter=200, shuffle=True, random_state=None, tol=0.0001, \n",
    "                       verbose=False, warm_start=False, momentum=0.9, nesterovs_momentum=True, \n",
    "                       early_stopping=False, validation_fraction=0.1, beta_1=0.9, beta_2=0.999, \n",
    "                       epsilon=1e-08, n_iter_no_change=10) # using the default multi-layer perceptron model\n",
    "    nn.fit(X_train, y_train)\n",
    "    print('\\nTRAIN SCORE', nn.score(X_train, y_train))\n",
    "    print('TEST SCORE', nn.score(X_test, y_test))\n",
    "    return None\n",
    "\n",
    "nn_model(vecs_sppmi, reviews_times, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 507
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 115396,
     "status": "ok",
     "timestamp": 1556896052820,
     "user": {
      "displayName": "Ruo Jia",
      "photoUrl": "",
      "userId": "09326440466598086645"
     },
     "user_tz": 240
    },
    "id": "uU5K0qrlCORu",
    "outputId": "7b21aba6-d13b-4a38-cddc-f911b0b0e2a3"
   },
   "outputs": [],
   "source": [
    "!pip install keras\n",
    "!pip install tensorflow\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 454
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 508518,
     "status": "ok",
     "timestamp": 1556899744252,
     "user": {
      "displayName": "Ruo Jia",
      "photoUrl": "",
      "userId": "09326440466598086645"
     },
     "user_tz": 240
    },
    "id": "Gthcnc8VCORw",
    "outputId": "b2755671-a56a-42f1-edd6-a264fb7c17cf"
   },
   "outputs": [],
   "source": [
    "#using keras to build my own neural-network, using minmax scaled data\n",
    "import tensorflow\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Conv1D, Flatten, Embedding,GlobalMaxPooling1D\n",
    "#create model\n",
    "def nn_model(X, review_times, y):\n",
    "    #X = np.hstack((X, review_times)) #it seems review_times doesn't help in this neural network\n",
    "    X_train, X_test, y_train, y_test = ms.train_test_split(X, y, test_size=0.2, random_state = 195)\n",
    "    embedding_dim = 50\n",
    "    vocab_size = len(X_train)\n",
    "    maxlen = 300\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(vocab_size, embedding_dim, input_length=maxlen)) #embedding layer\n",
    "    model.add(Conv1D(64, 5, activation='relu')) #convolution layer\n",
    "    model.add(GlobalMaxPooling1D()) # pooling\n",
    "    model.add(Dense(10, activation='relu')) #fully connected layer\n",
    "    model.add(Dense(1, activation='softmax')) #fully connected layer\n",
    "    model.compile(optimizer='adam', \n",
    "                  loss='binary_crossentropy',\n",
    "                  metrics=['accuracy']) \n",
    "    model.summary()\n",
    "    history = model.fit(X_train, y_train,\n",
    "                    epochs=3,\n",
    "                    validation_data=(X_test, y_test),\n",
    "                    batch_size=100)\n",
    "    loss, accuracy = model.evaluate(X_train, y_train, verbose=False)\n",
    "    print(\"Training Accuracy: {:.4f}\".format(accuracy))\n",
    "    loss, accuracy = model.evaluate(X_test, y_test, verbose=False)\n",
    "    print(\"Testing Accuracy:  {:.4f}\".format(accuracy))\n",
    "    return None\n",
    "  \n",
    "nn_model(vecs_sppmi_c, reviews_times, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "deletable": false,
    "editable": false,
    "id": "MCODNhy1OANB",
    "nbgrader": {
     "checksum": "5089e89df3a1b054209f4cd0650424c2",
     "grade": false,
     "grade_id": "other_spec",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### Step 3.6: Other enhancements\n",
    "\n",
    "You may use any other techniques from this class and beyond. Feel free to explore!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YD60JH8rHttB"
   },
   "outputs": [],
   "source": [
    "#In this section, I'm basically exploring more methods such as logistic regression and boosted tree. I'm assuming essemble method would have a better performace on embedding \n",
    "#features than random forest does. And boosted tree seems to work well when the feature dimension is relatively large. I'm also trying mixed model, meaning to combine the results from\n",
    "#logistic regression and boosted tree together and see whether the mixed results would have a higher accuracy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "deletable": false,
    "id": "v3Bltqr6OANC",
    "nbgrader": {
     "checksum": "abf0e1a7488d25fac513191b3ade3352",
     "grade": true,
     "grade_id": "other",
     "locked": false,
     "points": 15,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "i4YfbqQeLggB"
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression \n",
    "from sklearn.ensemble import GradientBoostingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "f3BPnLH6Lkkn"
   },
   "outputs": [],
   "source": [
    "def evaluate_log_model(X, review_times, y):\n",
    "    #X = np.hstack((X, review_times)) # reviews_times doesn't help the model.\n",
    "    X_train, X_test, y_train, y_test = ms.train_test_split(X, y, test_size=0.2, random_state = 195)\n",
    "\n",
    "    model = LogisticRegression(C=1e-2, random_state=17, solver='lbfgs', \n",
    "                           max_iter=300,\n",
    "                          n_jobs=4)  # logistic regression model with regularization. Adding regularization term is to reduce over-fitting\n",
    "    log = model.fit(X_train, y_train)  \n",
    "    return log.score(X_test, y_test), log.predict(X_test), y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 70
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 817972,
     "status": "ok",
     "timestamp": 1556896763053,
     "user": {
      "displayName": "Ruo Jia",
      "photoUrl": "",
      "userId": "09326440466598086645"
     },
     "user_tz": 240
    },
    "id": "egrPkYggMGXj",
    "outputId": "5451859e-b5d0-43fa-bdbd-7ebc82cc615f"
   },
   "outputs": [],
   "source": [
    "log_score_sppmi, y_pred, y_truth = evaluate_log_model(vecs_sppmi, reviews_times, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 817237,
     "status": "ok",
     "timestamp": 1556896763055,
     "user": {
      "displayName": "Ruo Jia",
      "photoUrl": "",
      "userId": "09326440466598086645"
     },
     "user_tz": 240
    },
    "id": "6NxVOyloNITW",
    "outputId": "a2fa54fa-3d94-4ce6-dc9a-b3285e47ce49"
   },
   "outputs": [],
   "source": [
    "log_score_sppmi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 70
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 903912,
     "status": "ok",
     "timestamp": 1556896850583,
     "user": {
      "displayName": "Ruo Jia",
      "photoUrl": "",
      "userId": "09326440466598086645"
     },
     "user_tz": 240
    },
    "id": "EuRJmRNg-YjY",
    "outputId": "7d0ed9c6-a086-4a37-cf12-e193cd30c754"
   },
   "outputs": [],
   "source": [
    "log_score_sgn, y_pred, y_truth = evaluate_log_model(vecs_sgns, reviews_times, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 903184,
     "status": "ok",
     "timestamp": 1556896850585,
     "user": {
      "displayName": "Ruo Jia",
      "photoUrl": "",
      "userId": "09326440466598086645"
     },
     "user_tz": 240
    },
    "id": "59KI4NUUL86o",
    "outputId": "2fe032b7-b455-4a56-b927-ed154bbd6cfc"
   },
   "outputs": [],
   "source": [
    "log_score_sgn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "F2rTYrihMl3P"
   },
   "outputs": [],
   "source": [
    "#This model takes a long time to run\n",
    "def evaluate_boosted_model(X, review_times, y):\n",
    "    #X = np.hstack((X, review_times))\n",
    "    X_train, X_test, y_train, y_test = ms.train_test_split(X, y, test_size=0.2, random_state = 195)\n",
    "\n",
    "    model = GradientBoostingClassifier(loss='deviance', learning_rate=0.1, n_estimators=100, \n",
    "                               subsample=1.0, criterion='friedman_mse', min_samples_split=2,\n",
    "                               min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_depth=3, \n",
    "                               min_impurity_decrease=0.0, tol=0.01) #train a boosted tree model\n",
    "    boosted = model.fit(X_train, y_train) \n",
    "    return boosted.score(X_test, y_test), boosted.predict(X_test) , y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hHQEv6Q5NPLj"
   },
   "outputs": [],
   "source": [
    "boosted_score, y_pred_boost, y_truth =  evaluate_boosted_model(vecs_sppmi, reviews_times, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 3037350,
     "status": "ok",
     "timestamp": 1556898989826,
     "user": {
      "displayName": "Ruo Jia",
      "photoUrl": "",
      "userId": "09326440466598086645"
     },
     "user_tz": 240
    },
    "id": "E0ZLDaQ2NckB",
    "outputId": "20113917-9919-4db4-b33a-a4c6c1b7ff83"
   },
   "outputs": [],
   "source": [
    "boosted_score  #accuracy score of gradient boosting using vecs_sppmi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uRaTg0c04dFk"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OFTzNhNm-o0Q"
   },
   "outputs": [],
   "source": [
    "y_emsem =[] # combine the results from the above two methods. \n",
    "for i in range(len(y_truth)):\n",
    "    y_emsem.append(int(0.8*y_pred[i]+y_pred_boost[i]*0.2))\n",
    "\n",
    "#The mixture doesn't really work, which makes sense because combining categorical labels doesn't really have a clear meaning. \n",
    "#Mixture model might have a better performance when the predicted results are continunous or probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 467,
     "status": "ok",
     "timestamp": 1556899222020,
     "user": {
      "displayName": "Ruo Jia",
      "photoUrl": "",
      "userId": "09326440466598086645"
     },
     "user_tz": 240
    },
    "id": "Zpz9BuEU_Rmw",
    "outputId": "8e921a30-4761-4244-bc97-9a90ba36d5ee"
   },
   "outputs": [],
   "source": [
    "accuracy_score(y_emsem, y_truth) #accuracy score of mixed y label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 70
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 46670,
     "status": "ok",
     "timestamp": 1556899271164,
     "user": {
      "displayName": "Ruo Jia",
      "photoUrl": "",
      "userId": "09326440466598086645"
     },
     "user_tz": 240
    },
    "id": "_LvTkdxyJYXs",
    "outputId": "e7306833-4f1e-4c08-dfc7-229673e7dcb7"
   },
   "outputs": [],
   "source": [
    "# trying dimension reduction results using logistic regression. Doesn't improve the performance\n",
    "accuracy = []\n",
    "model =  LsiModel(reviews_bow, id2word = reviews_dict, num_topics = 100)\n",
    "V = model[reviews_bow]\n",
    "dense_V = densify(V, 100)\n",
    "#acc = evaluate_model(dense_V, reviews_times, y)\n",
    "log_score2, y_pred2, y_truth = evaluate_log_model(dense_V, reviews_times, y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 44395,
     "status": "ok",
     "timestamp": 1556899271181,
     "user": {
      "displayName": "Ruo Jia",
      "photoUrl": "",
      "userId": "09326440466598086645"
     },
     "user_tz": 240
    },
    "id": "dqUnNUvtKNsm",
    "outputId": "41256c3b-db20-462b-e965-98ef863f365b"
   },
   "outputs": [],
   "source": [
    "log_score2 # accuracy score of dimension reducted feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iln158ONJwll"
   },
   "outputs": [],
   "source": [
    "#To conclude, among random forest, neural network, k-means, logistic regression and gradient boosted tree, \n",
    "#we see that random forest provides a relatively good baseline, while\n",
    "# k-means and neural networks seem not to be suitable for this task. \n",
    "#It might also due to the neural network structure. \n",
    "#Logistic regression and gradient boosting improved the performance a lot. \n",
    "#Logistic regression seems to be the best model."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Homework5_6_v2.ipynb",
   "provenance": [],
   "toc_visible": true,
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
